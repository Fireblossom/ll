language	Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Homepage_URL
Akan Corpus	citing_context	Akan	https://doi.org/10.48550/arXiv.2502.10973 (2025), https://doi.org/10.1016/j.dib.2025.111880 (2025)	https://doi.org/10.18653/v1/P19-1050 (2018)	The Akan dataset is used to assess inter-annotator agreement for emotion recognition, ensuring reliable annotations in the Akan language. It is also utilized to explore linguistic aspects of Akan, particularly in the context of financial inclusion, using an audio dataset. These applications focus on enhancing the reliability and understanding of Akan language data in specific social and emotional contexts.	
Akan Corpus	citing_context	Akan language dataset	https://doi.org/10.48550/arXiv.2502.10973 (2025), https://www.semanticscholar.org/paper/d8e6c45b1504089cdde501164d0bb557ac544dfb (2025)	https://doi.org/10.48550/arXiv.2503.19642 (2025)	The Akan language dataset is used to study cultural variation in emotion perception within a broader set of 15 African languages and to evaluate the performance of a base model in Akan compared to an English model, focusing on model generalization capabilities. This dataset enables researchers to explore linguistic and cultural nuances in emotion perception and machine learning model effectiveness across languages.	
Akan Corpus	citing_context	Akan UGSpeechData	https://www.semanticscholar.org/paper/d8e6c45b1504089cdde501164d0bb557ac544dfb (2025)		The Akan UGSpeechData dataset is used to train an Akan base model for standard speech recognition, utilizing 100 hours of transcribed Akan speech comprising 18,787 samples. This dataset enables researchers to develop and refine speech recognition systems specifically tailored for the Akan language, enhancing accuracy and performance in Akan speech processing applications.	
Akan Corpus	citing_context	EmoryNLP	https://doi.org/10.48550/arXiv.2502.10973 (2025)	https://www.semanticscholar.org/paper/ba4c923b43360325cba984549aa3c3224863d1f6 (2018)	The EmoryNLP dataset is used for Akan language research, specifically in dialogue modeling and emotion detection. It focuses on daily conversation scenarios to understand natural language interactions and analyzes emotional expressions in multi-party and text-based conversations. This dataset enables researchers to develop models that can accurately capture and interpret the nuances of Akan language dialogues and emotions.	
Akan Corpus	cited_context	JW300	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.18653/v1/P19-1310 (2019)	The JW300 dataset is primarily used to enhance and expand vocabulary in low-resource languages, particularly in the Akan language, by increasing token counts to over 10,000. It serves as a parallel corpus for training and evaluating machine translation and language modeling tasks. Researchers use it to compare the quality and dialect mix between Twi and Yoruba corpora, and to study the Akuampem dialect of Akan. The dataset provides clean, wide-coverage data, crucial for improving translation quality and addressing low-resource language challenges.	
Akan Corpus	cited_context	Twi Bible	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.1023/A:1001798929185 (1999)	The Twi Bible dataset is used as a base text for linguistic studies in the Akan language, particularly in low-resourced and endangered language contexts. Researchers leverage its widespread availability to conduct linguistic analyses, focusing on the preservation and documentation of the Akan language. The dataset's extensive use in these contexts enables detailed linguistic research and supports efforts to maintain linguistic diversity.	
Akan Corpus	citing_context	wage Data from Akan	https://www.semanticscholar.org/paper/e3b567a6f6121fc1e9325d570ac10159ef3bd06b (2025)		The 'wage Data from Akan' dataset is used to develop and evaluate Akan speech models, particularly for financial applications. It focuses on enhancing language resources and improving speech recognition systems by incorporating specific financial vocabulary and context. The dataset addresses the impact of recording conditions, dialect variations, and acoustic environments on model performance, thereby improving the robustness and accuracy of Akan speech models in real-world scenarios.	
Akan Corpus	cited_context	wordsim-353	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.1109/ICDARW.2019.40090 (2019)	The wordsim-353 dataset is used to evaluate word similarity in African languages, specifically Yoruba and Twi, by translating and adapting the original English dataset. Researchers employ this adapted dataset to assess the performance of computational models in capturing semantic similarities between words in these languages. This enables the development and refinement of natural language processing tools tailored for African linguistic contexts.	
Bulgarian Corpus	cited_context | citing_context	400 movie reviews (200 positive + 200 negative)	https://www.semanticscholar.org/paper/dfb479d4421930ee24bffe2f048ddcd2d46c14a5 (2015)	https://doi.org/10.1007/978-3-319-09879-1_28 (2014)	The 400 movie reviews dataset (200 positive + 200 negative) is used for sentiment analysis experiments, particularly to evaluate the performance of a Naïve Bayes classifier. Researchers employ a small manually annotated sentiment lexicon and various preprocessing techniques to enhance classification accuracy. This dataset enables the testing and validation of sentiment analysis models, focusing on the effectiveness of the classifier and preprocessing methods in distinguishing between positive and negative sentiments.; The 400 movie reviews dataset (200 positive + 200 negative) is used for sentiment analysis research, particularly to evaluate the performance of a Naïve Bayes classifier. This involves using a manually annotated sentiment lexicon and various preprocessing techniques. The dataset enables researchers to test and refine sentiment classification models, focusing on the accuracy and effectiveness of these methods in distinguishing between positive and negative sentiments in text.	
Bulgarian Corpus	cited_context	992 Amazon reviews	https://doi.org/10.1007/978-3-319-24069-5_17 (2015)	https://doi.org/10.1145/2507157.2507163 (2013)	The 992 Amazon reviews dataset, translated to Bulgarian using Google Translate, is used to study linguistic characteristics and sentiment analysis in Bulgarian translations. Researchers analyze the translated reviews to understand how sentiment and linguistic features are preserved or altered in the translation process. This dataset enables the exploration of cross-lingual sentiment analysis and the impact of machine translation on text characteristics.	
Bulgarian Corpus	citing_context	ARC-Challenge	https://doi.org/10.48550/arXiv.2407.08699 (2024)	https://www.semanticscholar.org/paper/88bb0a28bb58d847183ec505dda89b63771bb495 (2018)	The ARC-Challenge dataset is used to evaluate question answering systems on complex, challenging problems, specifically focusing on the Bulgarian language subset. Researchers employ this dataset to assess the performance and capabilities of these systems in handling linguistically intricate questions. This enables the development and refinement of algorithms tailored for advanced natural language processing tasks in Bulgarian.	
Bulgarian Corpus	citing_context	BSNLP 2017 Shared Task dataset	https://doi.org/10.18653/v1/W19-3712 (2019)	https://doi.org/10.18653/v1/W17-1412 (2017)	The BSNLP 2017 Shared Task dataset is used for evaluating and improving cross-lingual named entity recognition, normalization, and matching in Slavic languages, including Bulgarian, Czech, Russian, and Polish. It serves as a source for transfer learning in named entity recognition, providing annotated linguistic data that facilitates cross-lingual research and model development.	
Bulgarian Corpus	citing_context	Bulgarian Language Dataset	https://doi.org/10.26615/978-954-452-072-4_113 (2021)	https://doi.org/10.18653/v1/K19-1047 (2019)	The Bulgarian Language Dataset is used to develop coding schemes and detect media frames in Bulgarian news articles. Researchers employ automatic detection methods and large-scale analysis to identify and categorize media frames, enabling a deeper understanding of how news is presented and perceived in Bulgaria.	
Bulgarian Corpus	citing_context	Bulgarian MARCELL corpus	https://www.semanticscholar.org/paper/5f651b5b2c5ef9a5d144f068abf07611925e41cb (2020)	https://www.semanticscholar.org/paper/c5f5bb3131f5f1082bd82cce8a0ac08dea1e9366 (2016)	The Bulgarian MARCELL corpus is used in natural language processing (NLP) research, specifically for processing Bulgarian text using the NLP-Cube toolkit. It supports tasks such as tokenization, morphological analysis, part-of-speech (POS) tagging, and parsing. This dataset enables researchers to develop and evaluate NLP models tailored for the Bulgarian language, enhancing the accuracy and efficiency of linguistic analyses.	
Bulgarian Corpus	citing_context	Bulgarian MRC dataset	https://doi.org/10.26615/978-954-452-056-4_053 (2019)	https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776 (2017)	The Bulgarian MRC dataset is used to evaluate machine reading comprehension systems in Bulgarian, specifically focusing on their ability to answer questions based on given texts. This dataset enables researchers to assess the performance and accuracy of these systems in understanding and processing Bulgarian language content.	
Bulgarian Corpus	cited_context | citing_context	Bulgarian National Corpus	https://doi.org/10.26615/978-954-452-072-4_113 (2021), https://doi.org/10.26615/978-954-452-056-4_029 (2019), https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	https://www.semanticscholar.org/paper/cb53f9558bd13c853026f97dce3bbe3d989ca97d (2018), https://doi.org/10.18653/v1/D18-1269 (2018)	The Bulgarian National Corpus is used to develop a specialized corpus of 1.3k arguments annotated with fallacies, focusing on propaganda techniques and argumentation analysis. This dataset enables researchers to analyze and identify fallacious reasoning and propaganda methods, enhancing understanding of persuasive discourse and misinformation. The corpus's annotated nature supports detailed linguistic and rhetorical analysis, facilitating the development of tools and models for detecting and analyzing argumentative structures and fallacies.; The Bulgarian National Corpus is used to evaluate cross-lingual sentence representations and train models for Bulgarian language processing. It focuses on entailment tasks, cross-lingual transfer performance, and text classification. The dataset's linguistic features enable researchers to assess model performance and improve natural language processing techniques specifically for Bulgarian.	
Bulgarian Corpus	citing_context	Bulgarian speech data	https://doi.org/10.47810/clib.24.14 (2024)	https://doi.org/10.21437/interspeech.2022-143 (2021)	The Bulgarian speech data dataset is used to fine-tune Wav2Vec2 XLS-R models for Bulgarian speech recognition, specifically enhancing the model's performance on Bulgarian language tasks. This involves using the dataset to improve the accuracy and robustness of speech recognition systems tailored for the Bulgarian language.	
Bulgarian Corpus	citing_context	Bulgarian Treebank	https://doi.org/10.18653/v1/W19-3712 (2019)	https://www.semanticscholar.org/paper/a65e6a974212ed28133c2fe1e3b97a18dbc40cb6 (2009)	The Bulgarian Treebank is utilized primarily for transfer learning in named entity recognition (NER), providing annotated linguistic data for Bulgarian and other Slavic languages. It supports cross-lingual NER, normalization, and matching tasks, enabling researchers to leverage its annotated data for improving NER models across languages like Czech, Russian, and Polish. This dataset facilitates the development of multilingual NER systems by offering richly annotated resources.	
Bulgarian Corpus	citing_context	Europarl	https://doi.org/10.1007/978-94-024-0881-2_17 (2020)	https://www.semanticscholar.org/paper/694b3c58712deefb59502847ba1b52b192c413e5 (2005)	The Europarl dataset is used as a parallel corpus for statistical machine translation, enabling the development and evaluation of translation models across multiple languages. Researchers employ this dataset to train and test translation algorithms, focusing on improving the accuracy and fluency of automated translations. The dataset's multilingual nature supports cross-lingual research, facilitating comparisons and enhancements in translation performance.	
Bulgarian Corpus	citing_context	EXAMS	https://www.semanticscholar.org/paper/6a547cdba66a13aeedd7e5eb107754ee41a93db1 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.438 (2020)	The EXAMS dataset is used to generate model passages for black-box, zero-resource evaluation of hallucination in cross-lingual and multilingual question answering systems. It also serves to derive prompts for text generation tasks, specifically targeting Bulgarian matriculation exams across various school subjects. This dataset enables researchers to assess model performance and reliability in educational contexts, leveraging its focus on Bulgarian language content.	
Bulgarian Corpus	citing_context	GPT-WEB-BG 22	https://doi.org/10.26615/978-954-452-092-2_122 (2023)	https://doi.org/10.26615/978-954-452-092-2_077 (2023)	The GPT-WEB-BG 22 dataset is used to train transformer-based language models for the Bulgarian language, specifically focusing on content from major online media providers and social media. This dataset enhances language generation capabilities by providing diverse message types, enabling researchers to improve the quality and context relevance of generated text.	
Bulgarian Corpus	citing_context	HellaSwag	https://doi.org/10.48550/arXiv.2506.23394 (2025)	https://www.semanticscholar.org/paper/92e121c6e114fe3cfb89370df03847c66a9b4e28 (2019)	The HellaSwag dataset is used to evaluate models' capabilities in Bulgarian, focusing on common-sense reasoning, sentence completion, and complex reasoning tasks. It assesses tool-use capability, function-calling accuracy, and coreference resolution through adversarial examples, enhancing semantic understanding and problem-solving skills. This dataset enables researchers to rigorously test and improve models' linguistic and cognitive abilities in challenging scenarios.	
Bulgarian Corpus	citing_context	Kajgana forum	https://www.semanticscholar.org/paper/dfb479d4421930ee24bffe2f048ddcd2d46c14a5 (2015)	https://www.semanticscholar.org/paper/e9883ff80d17847c78769b8e41e4537e35680ebf (2014)	The Kajgana forum dataset is used for opinion mining research, focusing on classifying forum posts as positive, negative, or objective. Researchers employ SVM and Naïve Bayes classifiers, utilizing features such as bag of words, negation rules, and stemming. This dataset enables the development and evaluation of sentiment analysis techniques, specifically tailored for forum content.	
Bulgarian Corpus	citing_context	MaLA-500	https://doi.org/10.48550/arXiv.2506.23394 (2025)	https://doi.org/10.48550/arXiv.2405.12413 (2024)	The MaLA-500 dataset is used to extend language coverage to over 500 languages through continual pre-training, specifically focusing on enhancing performance for related language families. This approach involves leveraging the dataset to improve model performance across a broad range of languages, enabling more inclusive and effective multilingual natural language processing research.	
Bulgarian Corpus	cited_context | citing_context	mC4	https://doi.org/10.48550/arXiv.2407.08699 (2024), https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020), https://doi.org/10.18653/v1/N19-1423 (2019)	The mC4 dataset is used to evaluate the fertility of models trained on a mix of Bulgarian and multilingual data. This involves assessing model performance on specific research aspects, such as the effectiveness of multilingual training in enhancing Bulgarian language processing. The dataset's multilingual nature is a key characteristic, enabling researchers to explore how mixed-language training impacts model capabilities in Bulgarian.; The mC4 dataset is used to supplement training data for multilingual models, enhancing their performance by including data from 103 languages. Researchers modify the data pipeline by turning off the line length filter, allowing for a more diverse and extensive dataset. This approach supports the development of robust multilingual models capable of handling a wide range of languages.	
Bulgarian Corpus	citing_context	Media Frames Corpus	https://doi.org/10.26615/978-954-452-072-4_113 (2021)	https://doi.org/10.3115/v1/P15-2072 (2015)	The Media Frames Corpus is used to analyze linguistic patterns in the Bulgarian language, focusing on identifying media bias and framing in news articles. Researchers employ the dataset to examine how certain topics are presented and the linguistic techniques used to influence public perception. This enables detailed content analysis and the exploration of media influence on societal views.	
Bulgarian Corpus	citing_context	MGSM-DE	https://doi.org/10.48550/arXiv.2407.08699 (2024)	https://doi.org/10.18653/v1/D19-1382 (2019)	The MGSM-DE dataset is used to analyze paraphrasing tasks and evaluate performance on math-related tasks in Bulgarian. It measures cross-lingual paraphrase identification accuracy and assesses the effectiveness of models on Bulgarian sentences, enabling researchers to understand linguistic nuances and model capabilities in specific contexts.	
Bulgarian Corpus	citing_context	MTE digital lexical resources	https://doi.org/10.55630/dipp.2011.1.16 (2011)	https://www.semanticscholar.org/paper/80d8a20897a577f8acb9020aa9189fef437e8f5d (2000)	The MTE digital lexical resources dataset is used to provide well-structured and lemmatized language-specific resources for linguistic analysis. It supports the study of language-specific phenomena and methodological approaches in multilingual translation evaluation, enabling researchers to focus on detailed linguistic aspects and enhance the accuracy of translation evaluations.	
Bulgarian Corpus	citing_context	MULTEXT-East corpus	https://doi.org/10.55630/dipp.2011.1.16 (2011)	https://www.semanticscholar.org/paper/ba97deb7a45dc9746711b3d29592dc7ed03e0b68 (1998)	The MULTEXT-East corpus is used to study morpho-syntactic features and sentence-level structures in the Bulgarian translation of George Orwell's '1984'. It supports the alignment of Bulgarian and English sentences, focusing on translation equivalence and linguistic structure. The dataset facilitates research on parallel translations in Central and Eastern European languages, analyzing paragraph and sentence counts, and both undisambiguated and disambiguated lexical information.	
Bulgarian Corpus	citing_context	multilingual (mostly Bulgarian) dataset	https://www.semanticscholar.org/paper/ebb147c750bbd0bacfb6ecfdc42906ee6cc87f08 (2022)	https://www.semanticscholar.org/paper/069cff8081de01bd9210b00677b77f9d0b2c869c (2021)	The multilingual (mostly Bulgarian) dataset is primarily used for fake content detection and language-specific studies. It consists of 3350 annotated tweets, enabling researchers to identify and analyze false information on Bulgarian social media. The dataset supports manual annotation for language-specific tasks, particularly language identification, enhancing the accuracy of linguistic analyses and content verification efforts.	
Bulgarian Corpus	citing_context	ParlaMint	https://doi.org/10.3233/IDA-227347 (2022)	https://doi.org/10.1007/s10579-021-09574-0 (2022)	The ParlaMint dataset is used to create unified corpora of parliamentary debates from 17 European parliaments, enhancing accessibility and facilitating comparative linguistic and political research. It supports methodologies that involve analyzing and comparing legislative discourse across different countries, enabling researchers to explore variations in political language and communication strategies. The dataset's comprehensive coverage and standardized format are crucial for these comparative studies.	
Bulgarian Corpus	cited_context	PAWS-X	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	https://doi.org/10.18653/v1/D19-1382 (2019)	The PAWS-X dataset is primarily used to evaluate cross-lingual capabilities of machine learning models, particularly in tasks such as question answering, paraphrase identification, named entity recognition, and natural language inference. It focuses on model performance across multiple languages, including Bulgarian, by assessing the ability to handle complex linguistic structures and generate accurate outputs like answers, entity tags, and labels. This dataset enables researchers to test and improve the cross-lingual robustness and generalization of models.	
Bulgarian Corpus	citing_context	PubMed	https://doi.org/10.48550/arXiv.2407.08699 (2024)	https://www.semanticscholar.org/paper/efac04450c531b3769451a886ed9a42fce4754d9 (2012)	The PubMed dataset is used to create an English validation dataset, specifically for evaluating model performance. It focuses on 7,000 samples from a Bulgarian language dataset. This approach helps in assessing the effectiveness of models in cross-lingual settings, ensuring they perform well on Bulgarian data despite being trained primarily on English.	
Bulgarian Corpus	citing_context	RACE	https://doi.org/10.26615/978-954-452-056-4_053 (2019)	https://doi.org/10.18653/v1/D17-1082 (2017)	The RACE dataset is used to fine-tune multilingual BERT models for multiple-choice reading comprehension tasks, specifically focusing on zero-shot transfer from English to Bulgarian. This enables researchers to evaluate the model's ability to generalize to unseen languages, enhancing cross-lingual understanding and performance.	
Bulgarian Corpus	citing_context	Shared Tasks on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages	https://doi.org/10.48550/arXiv.2404.00482 (2024)	https://doi.org/10.18653/v1/W19-3709 (2019)	The dataset 'Shared Tasks on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages' is cited in research contexts but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided in the given usage descriptions. Therefore, the actual research utilization and enabling capabilities of this dataset remain unspecified based on the available information.	
Bulgarian Corpus	citing_context	Snodgrass and Vanderwart picture set	https://doi.org/10.3758/s13428-017-0902-x (2017)	https://doi.org/10.1348/000712603321661859 (2003)	The Snodgrass and Vanderwart picture set is used to present common objects to participants, primarily to study visual recognition and processing. This dataset facilitates research in the context of Bulgarian language studies, where the images help in examining how visual stimuli are processed and recognized by speakers of Bulgarian. The dataset's standardized and widely recognized images enable consistent and reliable experimental designs, supporting research into cognitive processes related to language and visual perception.	
Bulgarian Corpus	citing_context	SpreadTheSign-Ten (SP-10)	https://doi.org/10.1109/CVIDL65390.2025.11085429 (2025)	https://doi.org/10.1038/s41597-025-04986-x (2025)	The SpreadTheSign-Ten (SP-10) dataset is used to derive a multilingual sign language dataset, including Bulgarian, for isolated sign language recognition. It focuses on dual-view recognition across ten countries, enabling researchers to develop and test algorithms that can recognize signs from multiple perspectives, enhancing the accuracy and robustness of sign language recognition systems.	
Bulgarian Corpus	citing_context	Temnikova et al., 2023	https://www.semanticscholar.org/paper/9e98fa93a9b23a2c331c2d7f4f8224e59e845d3d (2024)		The 'Temnikova et al., 2023' dataset is used to analyze social media content from platforms like Facebook, Telegram, and Twitter. It focuses on content analysis and enrichment by integrating additional Facebook posts and tweets. This dataset enables researchers to explore the dynamics and patterns of online communication, providing insights into user behavior and content dissemination across these platforms.	
Bulgarian Corpus	citing_context	TweepFake	https://doi.org/10.26615/978-954-452-092-2_122 (2023)	https://doi.org/10.1371/journal.pone.0251415 (2020)	The TweepFake dataset is used to test machine translation systems, specifically for translating language model-generated tweets into Bulgarian. Researchers employ this dataset to evaluate the performance and accuracy of translation systems, focusing on the nuances and challenges of translating synthetic social media content. This enables the assessment of translation quality and the effectiveness of machine translation models in handling tweet-specific language and context.	
Cameroon Pidgin Corpus	citing_context	APiCS	https://doi.org/10.1177/13670069211019126 (2021)	https://doi.org/10.5944/ENDOXA.37.2016.16613 (2016)	The APiCS dataset is used to analyze the presence of split systems in English- and Portuguese-lexifier contact languages of West Africa and the Americas. It supports research on linguistic structures and patterns in creole languages, employing a comparative approach to demonstrate the presence of these systems. This dataset enables detailed linguistic analysis by providing structured data on various contact languages.	
Cameroon Pidgin Corpus	citing_context	request strategy types in cameroon Pidgin English	https://doi.org/10.30564/JLER.V1I1.226 (2019)	https://doi.org/10.1093/APPLIN/5.3.196 (1984)	The dataset 'request strategy types in Cameroon Pidgin English' is used to analyze the distribution and frequency of various request strategies, such as explicit performative and locution derivable, within the language. Containing 38 instances, the dataset supports research focused on understanding the linguistic mechanisms and social functions of requests in Cameroon Pidgin English. This analysis helps in identifying the predominant types of request strategies and their contextual usage, providing insights into the communicative norms and pragmatic features of the language.	
Cameroon Pidgin Corpus	citing_context	Spoken Corpus of Cameroon Pidgin English	https://doi.org/10.1163/19552629-01201003 (2019)		The 'Spoken Corpus of Cameroon Pidgin English' is used to analyze the grammatical structure and linguistic features of spoken Cameroon Pidgin English. Researchers focus on syntactic patterns, morphological features, and usage patterns in natural speech. This dataset enables detailed linguistic analysis by providing authentic spoken data, facilitating a deeper understanding of the language's structure and usage.	
Catalan Corpus	cited_context | citing_context	4CatAc	https://doi.org/10.21437/iberspeech.2024-42 (2024)	https://www.semanticscholar.org/paper/131896b7c121d8c532bcbfc06e64e5be47e51b50 (2008)	The 4CatAc dataset is used to select sentences for the LaFresCat corpus, specifically focusing on the Catalan language for speech synthesis research. This involves curating sentences that are suitable for generating natural-sounding speech. The dataset's relevance lies in its ability to provide a diverse set of linguistic examples that enhance the quality and accuracy of speech synthesis systems tailored for Catalan.; The 4CatAc dataset is used to enhance the diversity of the Catalan language dataset for speech synthesis by selecting sentences for the LaFresCat corpus. This selection process focuses on improving the representativeness and variability of the dataset, which is crucial for developing more natural and accurate speech synthesis systems in Catalan.	
Catalan Corpus	cited_context	AnCora-9	https://doi.org/10.48550/arXiv.2403.08693 (2024)		The AnCora-9 dataset is used for part-of-speech tagging and named entity recognition in Catalan. It provides annotated text, enabling researchers to train and evaluate models for these tasks. This dataset supports the development and assessment of natural language processing techniques specific to the Catalan language, enhancing the accuracy and reliability of linguistic analyses.	
Catalan Corpus	citing_context	AnCora-CA	https://www.semanticscholar.org/paper/3f7ed762ee571e503c71da95e71431cbc7efa29d (2022)	https://doi.org/10.4000/BOOKS.AACCADEMIA.7084 (2020)	The AnCora-CA dataset is primarily used for evaluating and training Catalan language processing systems, focusing on tasks such as stance detection, part-of-speech tagging, and syntactic parsing. Researchers employ supervised methods and in-context learning with large language models to assess system performance. The dataset also supports the development of interaction-based embeddings and the determination of optimal embedding dimensions for stance detection, particularly in cross-lingual settings involving vaccination-related content.	
Catalan Corpus	cited_context	Babel	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	https://www.semanticscholar.org/paper/98b1d8a28fc6c9b89b7960ec4dd853a566a81925 (2014)	The Babel dataset is used to develop speech recognition models for low-resource languages, including Catalan, through keyword spotting and speech recognition techniques. It highlights the quality and scope of data for minority languages, addressing issues of open licensing in low-resource language datasets. This enables researchers to improve model performance and accessibility for underrepresented languages.	
Catalan Corpus	citing_context	corpus-based speech synthesis by Saratxaga et al. (2006)	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	https://www.semanticscholar.org/paper/b9c983f9a05044477401068f4f74fb9d08f19f1a (2008)	The dataset 'corpus-based speech synthesis by Saratxaga et al. (2006)' is used to develop and evaluate corpus-based speech synthesis systems. It focuses on high-quality audio recordings and detailed evaluations of speech quality and naturalness, enabling researchers to enhance the realism and clarity of synthesized speech.	
Catalan Corpus	citing_context	CaBBQ	https://www.semanticscholar.org/paper/a671f540fe1a2be6cfa3b4e0559c5d4d659445dc (2025)	https://doi.org/10.18653/v1/2020.acl-main.485 (2020)	The CaBBQ dataset is used to evaluate bias in Catalan language models, specifically to identify and quantify harmful biases in natural language processing systems. Researchers employ this dataset to assess the extent and nature of biases, ensuring more ethical and fair NLP models. The dataset's focus on the Catalan language makes it particularly valuable for regional linguistic studies and bias mitigation efforts.	
Catalan Corpus	citing_context	CARMEN-I species data	https://doi.org/10.1038/s41597-025-05320-1 (2025)	https://www.semanticscholar.org/paper/5acd3f0dd53378b174cc9a024af3dd88a1bc1325 (2022)	The CARMEN-I species data is used to evaluate the LivingNER system, specifically for detecting and normalizing species mentions in Catalan-language clinical documents. This dataset enables researchers to assess the accuracy and effectiveness of the LivingNER system in a linguistically specific context, contributing to the improvement of named entity recognition in clinical text processing.	
Catalan Corpus	cited_context | citing_context	Catalan dataset from UPC	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	https://doi.org/10.21437/ICSLP.1998-27 (1998)	The Catalan dataset from UPC is used to develop a bilingual Spanish-Catalan unit selection Text-to-Speech (TTS) system. It focuses on linguistic features and phonetic transcriptions, enabling researchers to enhance the naturalness and accuracy of speech synthesis in both languages. The dataset's detailed phonetic annotations are crucial for training and improving TTS models.; The 'Catalan dataset from UPC' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, its actual application and how it enables research remain unspecified in the provided contexts.	
Catalan Corpus	cited_context | citing_context	Catalan (Google, 2019b)	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)		The Catalan (Google, 2019b) dataset is used to provide open, high-quality speech resources for the Catalan language, supporting research and development in speech processing and natural language technologies. It enables researchers to enhance and develop speech recognition, synthesis, and other natural language processing applications specific to the Catalan language.; The Catalan (Google, 2019b) dataset is used to provide open, high-quality speech resources for the Catalan language. It supports research and development in speech processing and natural language technologies, enabling the creation and improvement of speech recognition and synthesis systems. This dataset facilitates the advancement of linguistic technologies specific to the Catalan language, enhancing their accuracy and usability.	
Catalan Corpus	citing_context	Catalan United Nations test set	https://www.semanticscholar.org/paper/02665643d168f7732af3e176a71c2cde7a400392 (2022)	https://www.semanticscholar.org/paper/800366078f063a637e6a4880c0c49c217c7905ea (2016)	The Catalan United Nations test set is used for evaluating the quality and accuracy of Catalan-English machine translation, particularly in the context of the United Nations Parallel Corpus and biomedical texts. Researchers employ this dataset to assess translation performance, focusing on specific domains such as biomedical content, ensuring that translations maintain high standards of accuracy and relevance.	
Catalan Corpus	cited_context	Catalan version of the Wikipedia	https://doi.org/10.3390/app11219872 (2021)	https://doi.org/10.18653/v1/2020.acl-main.156 (2020)	The Catalan version of Wikipedia is used to supply a large, up-to-date corpus of Catalan text, which enriches pretraining data with diverse and current content. This dataset expands the linguistic diversity and coverage of models, enhancing their performance in Catalan language tasks. It is particularly useful for improving the pretraining phase of natural language processing models by providing extensive and varied textual data.	
Catalan Corpus	citing_context	Catalan Wikipedia	https://doi.org/10.18653/v1/2021.findings-acl.437 (2021)	https://www.semanticscholar.org/paper/e00631018e737355f1b0b3db779641f8f26288b1 (2020)	The Catalan Wikipedia dataset is used to train and evaluate monolingual language models, such as WikiBERT-ca, focusing on the linguistic features and size of the dataset. It is employed to assess BERT models' performance on downstream tasks, particularly by comparing monolingual Catalan models against multilingual baselines. This dataset enables researchers to enhance and benchmark Catalan-specific natural language processing capabilities.	
Catalan Corpus	citing_context	CATalog	https://www.semanticscholar.org/paper/ac8e1888733971a50da48f32840fa4e5180a9b1e (2024)	https://www.semanticscholar.org/paper/77a2e761ed2dc2360043280bb543c7add804983d (2024)	The CATalog dataset is used to evaluate and compare the performance of different architectures and models for the Catalan language, particularly focusing on the effectiveness of pretraining corpora for mid-resourced languages. This dataset enables researchers to assess how various models handle Catalan, providing insights into the benefits and limitations of pretraining techniques in this context.	
Catalan Corpus	citing_context	CaText corpus	https://www.semanticscholar.org/paper/aa441b7dfdd1975cf5f5598039fa1bce33fd5900 (2021)	https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc (2019)	The CaText corpus is used to pre-select potential sentence pairs for developing the STS-ca dataset. Researchers apply Jaccard similarity, Doc2Vec, and DistilBERT embedding cosine similarity measures to identify suitable sentence pairs. This process leverages the corpus's extensive text data to enhance the quality and relevance of the selected pairs, facilitating more accurate semantic textual similarity evaluations.	
Catalan Corpus	citing_context	caWaC	https://www.semanticscholar.org/paper/8800be4b4201991c666e8aae2a2512648156edf7 (2014)	https://www.semanticscholar.org/paper/61fb29e6238a7780271a4a6f6b4ae399e97623d5 (2012)	The caWaC dataset is used to construct a large web corpus of Catalan text, with a focus on preprocessing and cleaning stages. Researchers employ the Brno pipeline tools to handle these tasks, ensuring the data is suitable for linguistic analysis. This dataset enables the creation of high-quality, cleaned corpora for Catalan language research.	
Catalan Corpus	cited_context	CC100	https://doi.org/10.48550/arXiv.2403.08693 (2024)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The CC100 dataset is used for unsupervised cross-lingual representation learning, enabling the training of models on a large-scale multilingual corpus. It is employed to evaluate model performance across multiple languages and tasks, facilitating research into the effectiveness of cross-lingual representations. The dataset's multilingual nature supports the development of models that can generalize across different linguistic contexts.	
Catalan Corpus	citing_context	CCAligned	https://www.semanticscholar.org/paper/02665643d168f7732af3e176a71c2cde7a400392 (2022)	https://doi.org/10.18653/v1/2020.emnlp-main.480 (2019)	The CCAligned dataset is used for collecting large-scale cross-lingual web-document pairs, primarily focusing on mass data collection without active quality assessment. This dataset enables researchers to gather extensive multilingual data, facilitating studies in cross-lingual information retrieval, machine translation, and other natural language processing tasks. The lack of quality assessment in the dataset highlights its utility for exploratory research where volume is more critical than precision.	
Catalan Corpus	citing_context	CLUB 6	https://www.semanticscholar.org/paper/6c8729f8ec74b4e5847ceb8c9bb7e89eb2ed25f0 (2025)	https://doi.org/10.48550/arXiv.2407.17479 (2024)	The CLUB 6 dataset is used to evaluate BERT and RoBERTa-based models on 8 Catalan datasets, focusing on language representation and model performance in Catalan. This involves assessing how well these models understand and process the Catalan language, providing insights into their effectiveness and potential areas for improvement.	
Catalan Corpus	citing_context	Colos-sal Clean Crawled corpus	https://doi.org/10.18653/v1/2022.naacl-main.434 (2022)	https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b (2019)	The Colos-sal Clean Crawled corpus is used to pretrain mT5, a multilingual variant of the T5 model, enhancing its transfer learning capabilities across multiple languages. This dataset enables researchers to improve the model's performance in cross-lingual tasks by providing a large, clean text corpus. It is particularly valuable for developing models that can generalize well across different linguistic contexts.	
Catalan Corpus	citing_context	COPA-ca 15	https://www.semanticscholar.org/paper/ac8e1888733971a50da48f32840fa4e5180a9b1e (2024)	https://doi.org/10.18653/v1/2020.emnlp-main.185 (2020)	The COPA-ca 15 dataset is used to assess causal common-sense reasoning in Catalan by evaluating the ability to choose plausible alternatives in causal scenarios. It is a translated version of the original COPA dataset, specifically designed to test reasoning skills in the Catalan language. This dataset enables researchers to evaluate and compare models' performance in understanding and reasoning about cause and effect in a linguistically diverse context.	
Catalan Corpus	citing_context	Corts Valencianes Speech Corpus	https://www.semanticscholar.org/paper/ccd806246b66310dfae899308688c2042808e5c7 (2025)		The Corts Valencianes Speech Corpus is used for training Catalan TTS systems, enhancing ASR models for Spanish and Catalan, and analyzing linguistic and phonetic aspects of spoken Catalan. It provides a multi-speaker corpus with diverse accents and dialects, enabling researchers to improve synthesis quality and study linguistic patterns in broadcast content.	
Catalan Corpus	citing_context	DACSA corpus	https://doi.org/10.18653/v1/2022.naacl-main.434 (2022)	https://doi.org/10.18653/v1/2020.acl-main.703 (2019)	The DACSA corpus is used to evaluate the performance of summarization systems, including unsupervised, extractive, and abstractive models, by assessing summarization quality and upper bounds. It is also utilized to pretrain the mBART model on a multilingual corpus, enhancing cross-lingual performance, particularly in Catalan. This dataset enables researchers to benchmark summarization techniques and improve multilingual natural language processing models.	
Catalan Corpus	citing_context	EMMediaTopic	https://doi.org/10.1109/ACCESS.2025.3544814 (2024)		The EMMediaTopic dataset is used as a freely available resource within the CLARIN infrastructure, focusing on media topics in the Catalan language. It supports research by providing accessible data for studies related to media content and linguistic analysis in Catalan. This dataset enables researchers to explore specific media topics, enhancing understanding and analysis within the Catalan linguistic context.	
Catalan Corpus	citing_context	E S BBQ 1	https://www.semanticscholar.org/paper/a671f540fe1a2be6cfa3b4e0559c5d4d659445dc (2025)	https://doi.org/10.18653/v1/2022.findings-acl.165 (2021)	The E S BBQ 1 dataset is used to assess social biases in QA tasks, specifically adapted to the Spanish and Catalan languages and cultures of Spain. It focuses on developing culturally relevant bias benchmarks. The dataset enables researchers to evaluate and mitigate biases in natural language processing models, ensuring they are culturally sensitive and fair.	
Catalan Corpus	citing_context	Europarl corpus	https://www.semanticscholar.org/paper/8c42451d7e3410b51c02fd09c9899bf87a523d29 (2024)	https://www.semanticscholar.org/paper/278f7495e50db8b3d01112ba36223e8976f73b60 (2019)	The Europarl corpus is used for linguistic annotation, specifically to add tags to sentences containing first-person singular references. This enrichment enhances the corpus, supporting research in natural language processing and linguistic analysis. The dataset's structured tagging facilitates detailed examination of personal reference usage in text.	
Catalan Corpus	cited_context | citing_context	FESTCAT	https://www.semanticscholar.org/paper/ccd806246b66310dfae899308688c2042808e5c7 (2025), https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020), https://doi.org/10.21437/iberspeech.2024-42 (2024), https://www.semanticscholar.org/paper/6c5deaec0e93ecb43a62c5cba16f7401132ee5b1 (2009)	https://www.semanticscholar.org/paper/6c5deaec0e93ecb43a62c5cba16f7401132ee5b1 (2009), https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	The Festcat dataset is primarily used for training and evaluating Text-to-Speech (TTS) systems in the Catalan language. It provides a multi-speaker corpus with diverse accents and dialects, particularly focusing on the Central variant. Researchers use this dataset to enhance the quality and naturalness of synthesized speech, employing Hidden Markov Model (HMM)-based approaches. The dataset's multi-accent feature is crucial for improving the robustness and versatility of TTS models.; The FESTCAT dataset is used to produce synthetic Catalan speech data, specifically for training two synthetic voices (one male and one female) for the Festival Speech Synthesis System. The primary focus is on achieving high-quality synthetic speech comparable to the best available English voices. This dataset enables researchers to enhance the naturalness and intelligibility of Catalan speech synthesis.	Festcat
Catalan Corpus	citing_context	Flores-200	https://www.semanticscholar.org/paper/8c42451d7e3410b51c02fd09c9899bf87a523d29 (2024)	https://doi.org/10.48550/arXiv.2207.04672 (2022)	The Flores-200 dataset is used to evaluate translation quality, particularly in the context of human-centered machine translation. It focuses on both spoken language and low-resource languages, enabling researchers to assess the performance and accuracy of translation models in these specific contexts. This dataset facilitates the development and refinement of translation systems by providing a benchmark for evaluating their effectiveness in handling diverse linguistic challenges.	
Catalan Corpus	citing_context	corpus for speech recognition	https://www.semanticscholar.org/paper/d6d69d8a8e8b77fc64fbc0308cb15e11fc2b8f07 (2024)	https://www.semanticscholar.org/paper/0be4d80ebe54e94af69c92706b8c955cee0fd11d (2020)	The 'corpus for speech recognition' dataset is used to develop speech recognition systems, with a focus on leveraging crowd-sourced contributions to enhance the diversity and scale of the data. This approach helps in building more robust and inclusive speech recognition models, addressing the need for varied and extensive training data.	
Catalan Corpus	citing_context	GEnCaTa	https://www.semanticscholar.org/paper/02665643d168f7732af3e176a71c2cde7a400392 (2022), https://www.semanticscholar.org/paper/4164d20aaad5df0471b931b50862defaf81d1cb0 (2022)	https://www.semanticscholar.org/paper/694b3c58712deefb59502847ba1b52b192c413e5 (2005)	The GEnCaTa dataset is used to train and evaluate mBERT models for Catalan language tasks, specifically by splitting the dataset into training, validation, and test sets to fine-tune both cased and uncased models. It also supports Catalan↔English translation research, focusing on parallel segments to analyze translation quality and linguistic features. The dataset includes metadata like source URLs and alignment scores, enhancing its utility for improving sentence alignment algorithms.	
Catalan Corpus	cited_context | citing_context	LaFresCat	https://doi.org/10.21437/iberspeech.2024-42 (2024)	https://doi.org/10.48550/arXiv.2306.00814 (2023)	The LaFresCat dataset is used to train the vocoder Vocos, specifically for generating high-quality audio waveforms in the Catalan language. This dataset contributes to the development of speech synthesis systems, enhancing the naturalness and clarity of generated audio. Its focus on Catalan makes it valuable for linguistic and technological advancements in voice technology for this language.; The LaFresCat dataset is used to train the vocoder Vocos, providing Catalan speech data for high-quality audio synthesis. It contributes additional speech data essential for waveform generation, enabling the production of final waveforms with a focus on the Catalan language. This dataset enhances the vocoder's performance by offering specific linguistic characteristics of Catalan, crucial for natural-sounding speech synthesis.	
Catalan Corpus	citing_context	MaCoCu Corpora	https://doi.org/10.1109/ACCESS.2025.3544814 (2024)	https://www.semanticscholar.org/paper/24f404b302d20aa9002abaea078a810b2ec42f1a (2022)	The MaCoCu Corpora is used to select and evaluate language-specific datasets, ensuring high comparability across different languages, including Catalan. It leverages comparable web corpora in Catalan, supporting research on less-resourced European languages through extensive collection and curation efforts. This dataset enables researchers to enhance the quality and utility of linguistic resources for underrepresented languages.	
Catalan Corpus	cited_context	mC4	https://www.semanticscholar.org/paper/77a2e761ed2dc2360043280bb543c7add804983d (2024)	https://www.semanticscholar.org/paper/8800be4b4201991c666e8aae2a2512648156edf7 (2014)	The mC4 dataset is used to train and evaluate language models and machine translation systems specifically for the Catalan language. It leverages large-scale web-crawled and open-source data to enhance the diversity and quality of training datasets. This enables researchers to improve the performance and robustness of Catalan language models, addressing challenges in natural language processing for this language.	
Catalan Corpus	citing_context	MEDDOCAN	https://doi.org/10.48550/arXiv.2204.04775 (2022)	https://www.semanticscholar.org/paper/5205f216ffd3777202e538d7cc283b08e28480da (2008)	The MEDDOCAN dataset is used to train mBERT for named entity recognition tasks, specifically focusing on coarse-grained Protected Health Information (PHI) categories. It employs the BIO scheme for evaluation and supports few-shot training data selection, enabling efficient model performance with limited labeled data. This dataset facilitates research in natural language processing, particularly in medical text analysis.	
Catalan Corpus	citing_context	Mozilla CommonVoice	https://www.semanticscholar.org/paper/ccd806246b66310dfae899308688c2042808e5c7 (2025)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Mozilla CommonVoice dataset is used to enhance speech recognition technologies for underrepresented languages, including potentially Catalan. It provides a large, open-source collection of voice recordings, enabling researchers to train and improve speech recognition models. This dataset supports the development of more inclusive and accurate speech technologies by offering diverse linguistic data.	
Catalan Corpus	citing_context	MuST-SHE	https://www.semanticscholar.org/paper/8c42451d7e3410b51c02fd09c9899bf87a523d29 (2024)	https://doi.org/10.18653/v1/2020.acl-main.619 (2020)	The MuST-SHE dataset is used to examine and mitigate gender bias in machine translation, particularly in the English-Catalan language pair. It enriches resources by creating gender-neutral English terms that require gendered translations in Catalan. Researchers use this dataset to evaluate the impact of gender representation in translated content and to assess the performance of speech translation technology in handling gender-specific translations. The dataset is organized by category, enabling detailed analysis of gender bias in various contexts.	
Catalan Corpus	citing_context	newspaper articles from different sources for Spanish and Catalan	https://www.semanticscholar.org/paper/4164d20aaad5df0471b931b50862defaf81d1cb0 (2022)	https://doi.org/10.3390/app11219872 (2021)	The dataset of newspaper articles from different sources for Spanish and Catalan is used to train a Transformer encoder-decoder model for abstractive summarization. This research focuses on evaluating the model's performance in a minority language context, specifically comparing Catalan and Spanish. The dataset enables researchers to assess how well the model can generate concise summaries in both languages, highlighting the challenges and effectiveness of summarization in less-resourced languages.	
Catalan Corpus	cited_context	corpus of Catalan	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	https://www.semanticscholar.org/paper/b9c983f9a05044477401068f4f74fb9d08f19f1a (2008)	The 'corpus of Catalan' dataset is used to develop and evaluate multimodal audiovisual databases of emotional speech, focusing on prosodic features and emotional expression. It is also employed to assess corpus-based speech synthesis, emphasizing high-quality recordings to enhance the naturalness of synthesized speech. These applications leverage the dataset's detailed audiovisual and prosodic characteristics to advance speech technology and emotional speech analysis.	
Catalan Corpus	cited_context	OpenSLR-69 Catalan subset	https://www.semanticscholar.org/paper/a70b760846af91db4dddafb132bf63f62f982841 (2024)	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	The OpenSLR-69 Catalan subset is used to train models with a central Catalan accent, enhancing performance through multi-speaker data. This dataset provides additional speakers to improve pronunciation and intonation, making it valuable for developing more accurate and natural-sounding speech synthesis systems in Catalan.	
Catalan Corpus	cited_context	OSCAR	https://doi.org/10.48550/arXiv.2403.08693 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The OSCAR dataset is used to evaluate multilingual text processing, specifically assessing the quality and coverage of the corpus across eleven non-English European languages. Researchers employ this dataset to analyze and compare the performance of text processing systems, ensuring they effectively handle diverse linguistic structures and content. This enables the development and refinement of multilingual natural language processing tools and techniques.	
Catalan Corpus	cited_context	OSCAR v23/01	https://doi.org/10.48550/arXiv.2403.08693 (2024)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The OSCAR v23/01 dataset is used for enhancing data quality in multilingual web text through language identification and duplicate detection. Researchers employ fastText for language classification and TLSH for identifying duplicates, improving the reliability and accuracy of the dataset. This methodology addresses the need for clean, high-quality multilingual data in various research applications.	
Catalan Corpus	citing_context	STS-ca(20)	https://www.semanticscholar.org/paper/aa441b7dfdd1975cf5f5598039fa1bce33fd5900 (2021)	https://www.semanticscholar.org/paper/528fa9bb03644ba752fb9491be49b9dd1bce1d52 (2012)	The STS-ca(20) dataset is primarily used to evaluate semantic textual similarity (STS) in Catalan, addressing the scarcity of resources for this language. It is employed to create and refine new datasets, pre-select potential sentence pairs using methods like Jaccard, Doc2Vec, and DistilBERT, and assess question answering performance by measuring F1 and Exact Match scores. This dataset enables researchers to develop and validate models specifically tailored for Catalan, enhancing the linguistic resources available for this language.	
Catalan Corpus	citing_context	VaxxStance	https://www.semanticscholar.org/paper/3f7ed762ee571e503c71da95e71431cbc7efa29d (2022)	https://doi.org/10.26342/2021-67-15 (2021)	The VaxxStance dataset is used to detect and classify stances towards vaccination-related content in cross-lingual settings, including Catalan. Researchers employ machine learning models to analyze and categorize textual data, enabling the identification of supportive, neutral, or opposing stances. This dataset facilitates the study of public opinion and sentiment analysis across different languages, enhancing understanding of vaccine hesitancy and acceptance.	
Catalan Corpus	citing_context	Vides de sants rosselloneses	https://doi.org/10.5565/rev/catjl.387 (2022)		The 'Vides de sants rosselloneses' dataset is primarily used to analyze linguistic patterns and syntactic structures in medieval Catalan, particularly focusing on anteposition and clitics. Researchers employ this dataset to study the frequency and context of anteposition in various syntactic environments, as well as the grammatical function of clitics in 13th and 14th-century Catalan texts, including biblical translations and medieval literature like 'Conde Lucanor'. This dataset enables detailed linguistic analysis by providing historical textual data that highlights specific syntactic and morphological features.	
Catalan Corpus	cited_context	VoxForge	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)		VoxForge is a community-driven, multilingual dataset released under an open license. It is utilized in research primarily for its open-access nature, allowing researchers to leverage a diverse set of speech data. However, specific research contexts, methodologies, or applications are not detailed in the provided descriptions. The dataset's key characteristic is its multilingual and community-sourced content, which supports a wide range of potential speech-related studies.	
Catalan Corpus	citing_context	weather forecast corpus	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	https://doi.org/10.1016/j.patrec.2009.11.014 (2010)	The weather forecast corpus is used for speech-to-speech translation, language identification, and acoustic modeling, primarily focusing on spoken daily weather forecast reports in Catalan and other languages. Collected over 28 months, this dataset supports research in natural language processing, enhancing models' ability to handle diverse linguistic data and improving the accuracy of speech recognition and translation systems.	
Catalan Corpus	citing_context	WikiMatrix	https://www.semanticscholar.org/paper/8db014c4409acb680b54836d3f2bbff74a8ec58d (2019)	https://doi.org/10.18653/v1/2021.eacl-main.115 (2019)	The WikiMatrix dataset is used to extract parallel sentences for over 1,620 language pairs, including Catalan-Spanish and English-Catalan. It supports multilingual research and translation tasks by providing high-quality parallel data, which enhances the quantity and quality of resources for machine translation and cross-lingual research. This dataset enables researchers to improve translation models and explore linguistic relationships across diverse languages.	
Catalan Corpus	citing_context	XQuAD-ca	https://doi.org/10.18653/v1/2021.findings-acl.437 (2021)	https://www.semanticscholar.org/paper/528fa9bb03644ba752fb9491be49b9dd1bce1d52 (2012)	The XQuAD-ca dataset is used to evaluate model performance in question answering tasks by providing an additional test set from a different distribution. This helps researchers assess how well models generalize across diverse data regimes, focusing on the robustness and adaptability of the models to varied linguistic contexts.	
Central Kurdish Corpus	citing_context	15,000 text documents	https://doi.org/10.48550/arXiv.2304.04703 (2023)	https://doi.org/10.21928/juhd.v1n4y2015.pp393-397 (2015)	The dataset of 15,000 text documents is used to develop sentiment analyzers for Kurdish, specifically focusing on social network texts. Researchers employ a naive Bayes classifier with a bag-of-words approach, using 8000 positive and 7000 negative reviews to train the model. This enables the analysis of sentiment in Kurdish online communications, enhancing understanding of public opinion and social dynamics.	
Central Kurdish Corpus	citing_context	An extensive dataset of handwritten central Kurdish isolated characters	https://doi.org/10.11591/ijeecs.v35.i3.pp1865-1875 (2024)	https://doi.org/10.1016/j.dib.2021.107479 (2021)	The dataset of handwritten central Kurdish isolated characters is used to develop and evaluate deep learning models for character recognition. It focuses on enhancing e-government services through automation, specifically by improving the accuracy and efficiency of processing handwritten documents. The dataset's extensive coverage of isolated characters supports robust model training and evaluation.	
Central Kurdish Corpus	citing_context	annotated corpus for abstractive Kurdish text summarization	https://doi.org/10.48550/arXiv.2504.14630 (2025)		The annotated corpus for abstractive Kurdish text summarization is used to develop and evaluate models that generate concise summaries from longer texts in the Kurdish language. This dataset enables researchers to focus on improving the accuracy and coherence of abstractive summarization techniques, specifically tailored for the linguistic nuances of Kurdish.	
Central Kurdish Corpus	cited_context | citing_context	AsoSoft corpus	https://doi.org/10.1007/s42803-022-00062-7 (2021)	https://doi.org/10.1093/LLC/FQY074 (2019)	The AsoSoft corpus is used for Central Kurdish language research, particularly in evaluating linguistic rules and processing. It contains 188M words from news, books, and magazines, enabling the assessment of rule consistency, word selection, and context extraction. The dataset supports experiments in language processing, morphological analysis, and the identification of standard and nonstandard forms through tagging and frequency analysis.; The AsoSoft corpus is used in research to analyze and process the Central Kurdish language, focusing on identifying standard and nonstandard forms, and evaluating language models. It contains 188M tokens from various sources, enabling training and evaluation of NLP tasks like tokenization, part-of-speech tagging, and parsing. The corpus supports linguistic experiments, morphological analysis, and the development of text corpora, addressing specific challenges in Central Kurdish language processing.	
Central Kurdish Corpus	cited_context | citing_context	AsoSoft text corpus	https://doi.org/10.1007/s10579-022-09594-4 (2022), https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021), https://doi.org/10.37652/juaps.2022.176501 (2022), https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	https://doi.org/10.1093/LLC/FQY074 (2019)	The AsoSoft text corpus is used extensively in Central Kurdish language research, focusing on linguistic processing, language modeling, and speech recognition. It is employed to develop and enhance language models, normalize linguistic patterns, and create pronunciation lexicons. The corpus, sourced from diverse texts including websites and publications, supports experiments in corpus development, linguistic analysis, and resolving language-specific issues like conjunctive و. Its large size and varied content enable robust training and testing of NLP systems, improving their performance and accuracy.; The AsoSoft text corpus is used to train and evaluate language models and processing systems for the Central Kurdish language. It focuses on linguistic features such as di-phone distribution, word extraction, and text analysis. The dataset, containing over 188 million words from various sources, is utilized for language identification, lexicon extraction, and resolving issues with the written form, particularly the conjunctive و. It supports research in corpus creation, linguistic analysis, and the development of speech recognition systems.	
Central Kurdish Corpus	citing_context	BD-4SK-ASR	https://doi.org/10.21928/uhdjst.v6n2y2022.pp117-125 (2022)	https://www.semanticscholar.org/paper/72dd600355084dc51d1ebc6d0b5cac26ec9e09ec (2019)	The BD-4SK-ASR dataset is used to develop and evaluate speech recognition systems for Sorani Kurdish, focusing on the linguistic and phonetic features of the Central Kurdish language. It includes primary school texts from Iraq’s Kurdistan Region, containing 200 sentences, which are utilized to enhance the accuracy and robustness of speech recognition models.	
Central Kurdish Corpus	cited_context	Central Kurdish-English parallel corpus	https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	The Central Kurdish-English parallel corpus is used to study translation quality and linguistic features in Central Kurdish, particularly in domains like TED Talks, digital books, and academic content. It is also utilized to build a speech recognition system, focusing on pronunciation lexicon and speech data. Additionally, the dataset is employed to correct common text errors using before-and-after pairs and to analyze and process textual data for linguistic research.	
Central Kurdish Corpus	cited_context | citing_context	CK verb database	https://doi.org/10.1007/s42803-022-00062-7 (2021)	https://www.semanticscholar.org/paper/4ddb51690435785cdadbe3ce3907a5f3495acb46 (1966)	The CK verb database is used to compile a comprehensive and accurate list of Central Kurdish verbs, leveraging dictionary entries and linguistic studies. It provides about 7K main entries and 18.9K subentries, serving as an essential resource for linguistic research by enabling the collection of a robust Central Kurdish lexicon. This dataset supports research requiring precise and extensive verb data, enhancing the accuracy and completeness of linguistic analyses.; The CK verb database is used to compile a comprehensive list of Central Kurdish verbs, focusing on their linguistic structure and usage within the Central Kurdish language. This dataset enables researchers to analyze and document the verb system, contributing to linguistic studies and the understanding of Central Kurdish grammar.	
Central Kurdish Corpus	citing_context	dataset containing 14,881 comments from various Facebook pages	https://doi.org/10.48550/arXiv.2304.04703 (2023)	https://doi.org/10.37652/juaps.2022.176501 (2022)	The dataset containing 14,881 comments from various Facebook pages is used for sentiment analysis of Kurdish comments. Researchers focus on creating and evaluating this dataset to understand public opinion and emotional responses. The methodology involves analyzing the textual content to gauge sentiments, providing insights into how the public reacts to various topics on social media.	
Central Kurdish Corpus	cited_context	Dicto	https://doi.org/10.1109/AICCSA.2013.6616470 (2013)	https://doi.org/10.1007/BFb0034731 (1996)	The Dicto dataset is used for cross-language information retrieval, specifically to automatically translate query terms from English to Sorani, a dialect of Central Kurdish. This enables researchers to enhance the accuracy and efficiency of information retrieval systems when dealing with multilingual data. The dataset's focus on precise translation supports the development of more effective cross-language search capabilities.	
Central Kurdish Corpus	citing_context	FarsDat	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	https://www.semanticscholar.org/paper/3911933c247f705b2488fdd067330820e8db07bf (2012)	FarsDat is used to analyze phonological and phonetic aspects of Central Kurdish, specifically focusing on di-phone probabilities and acoustic-phonetic features. Researchers employ this dataset to examine the sound patterns and speech characteristics of the language, enabling detailed phonetic analysis and contributing to linguistic understanding.	
Central Kurdish Corpus	citing_context	Jira speech corpus	https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	The Jira speech corpus is utilized in various aspects of Kurdish language processing. It is employed to design and build speech recognition systems, correct common text errors, and construct a parallel corpus for Central Kurdish-English translation. Additionally, it is used to train GloVe embeddings and collect textual data for NLP experiments, leveraging its extensive content from diverse sources such as TED Talks, digital books, and academic texts.	
Central Kurdish Corpus	citing_context	KSLexicon	https://www.semanticscholar.org/paper/364d25c670ab7d8cab8a05e7521aae0e342f23b0 (2022)		The KSLexicon dataset is used to support linguistic analysis and natural language processing tasks in the Central Kurdish language, specifically the Sorani dialect. It involves tagging 35,000 Sorani entries with 28 part-of-speech tags and annotating data according to six categories. This enables detailed linguistic research and enhances the development of NLP tools for Central Kurdish.	
Central Kurdish Corpus	citing_context	Kurdish BLARK	https://doi.org/10.48550/arXiv.2501.14528 (2025)	https://doi.org/10.1007/s10579-017-9400-0 (2017)	The Kurdish BLARK dataset is used to provide foundational resources for Kurdish Natural Language Processing (NLP), specifically supporting tokenization for both Sorani and Kurmanji dialects. This enables the development of language models, enhancing the processing and understanding of the Kurdish language in computational linguistics.	
Central Kurdish Corpus	cited_context	Kurdish Corpus	https://doi.org/10.37652/juaps.2022.176501 (2022)	https://doi.org/10.1177/0165551516683617 (2018)	The Kurdish Corpus is used for developing and evaluating tools for text pre-processing, tokenization, stemming, transliteration, and lemmatization in Central Kurdish language processing. These methodologies focus on enhancing the accuracy and efficiency of linguistic tools, addressing specific challenges in Central Kurdish, such as morphological complexity and script variations. The dataset enables researchers to test and refine algorithms, improving natural language processing capabilities for this under-resourced language.	
Central Kurdish Corpus	citing_context	Kurdish Language (2100 words)	https://doi.org/10.24271/psr.2022.351832.1149 (2022)	https://doi.org/10.1109/ICDT.2009.29 (2009)	The Kurdish Language (2100 words) dataset is used to develop a text-to-speech (TTS) system for the Kurdish language. Researchers focus on the phonetic and quality aspects of the data, employing methodologies that enhance the accuracy and naturalness of TTS output. This dataset enables the creation of more effective and realistic speech synthesis for Kurdish, addressing the need for high-quality TTS systems in this language.	
Central Kurdish Corpus	citing_context	Kurdish (Sorani) Named Entities	https://doi.org/10.1016/j.dib.2025.111839 (2025)	https://doi.org/10.48550/arXiv.2301.04962 (2023)	The Kurdish (Sorani) Named Entities dataset is used to enhance the Kurdish-BLARK Named Entities dataset, specifically focusing on improving named entity recognition in the Sorani Kurdish dialect. This involves refining the identification and classification of named entities, which aids in developing more accurate natural language processing tools for the Sorani Kurdish language.	
Central Kurdish Corpus	citing_context	Kurdish Textbook Corpus (KTC)	https://doi.org/10.48550/arXiv.2305.06747 (2023)	https://www.semanticscholar.org/paper/6ad3ffae7d0f087f04b20434fbd4d91e5ea5b7b7 (2019)	The Kurdish Textbook Corpus (KTC) is used to develop parallel corpora for the Central Kurdish language, specifically focusing on K-12 school textbooks from the Kurdistan Region of Iraq. This dataset enables researchers to create linguistic resources for less-resourced languages, enhancing the availability of educational materials and supporting language documentation and preservation efforts.	
Central Kurdish Corpus	citing_context	Kurdish Textbooks Corpus (KTC)	https://doi.org/10.1007/s10579-022-09594-4 (2022)		The Kurdish Textbooks Corpus (KTC) is used to collect and normalize Central Kurdish textbooks, comprising 693 K tokens, for linguistic and educational research. This dataset facilitates the analysis of language structure and educational content, supporting studies that aim to improve language teaching and learning methodologies. The normalization process ensures consistency, enhancing the reliability of research findings.	
Central Kurdish Corpus	citing_context	parallel corpora	https://doi.org/10.48550/arXiv.2501.14528 (2025)	https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	The 'parallel corpora' dataset is mentioned as a developed resource in Kurdish NLP, but its specific usage in current research is not detailed. It is acknowledged as a valuable tool for developing NLP systems for Central Kurdish, though the exact methodologies and research questions it addresses are not specified in the provided descriptions.	
Central Kurdish Corpus	cited_context | citing_context	Pewan text corpus	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021), https://doi.org/10.1007/s10579-022-09594-4 (2022)	https://www.semanticscholar.org/paper/2fb97d01e987a798958472d384ecbb533a321996 (2013)	The Pewan text corpus is used to analyze and compare Central Kurdish and Northern Kurdish texts, focusing on linguistic differences and similarities. Collected from online news agencies, the dataset enables researchers to introduce and examine these dialects using textual analysis methods, providing insights into their structural and lexical variations.; The Pewan text corpus is used to introduce and analyze Central Kurdish and Northern Kurdish languages, focusing on linguistic features and linguistic comparisons. Researchers employ the dataset to explore specific linguistic characteristics, enabling detailed analyses and comparisons between these Kurdish dialects. This corpus facilitates a deeper understanding of the structural and functional aspects of these languages.	
Central Kurdish Corpus	cited_context	raw corpus of Sorani Kurdish	https://www.semanticscholar.org/paper/b103e1ec19f19780c2b172be85b2f0863fa20c8b (2010)		The 'raw corpus of Sorani Kurdish' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. The dataset's role in enabling research is not clearly defined based on the provided evidence.	
Central Kurdish Corpus	cited_context | citing_context	small corpus of size 590K tokens	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	https://www.semanticscholar.org/paper/b103e1ec19f19780c2b172be85b2f0863fa20c8b (2010)	The small corpus of 590K tokens is used to develop a morphological lexicon for Central Kurdish, focusing on linguistic features. It is employed in training a semi-supervised method, enabling researchers to enhance the accuracy and coverage of morphological analysis in Central Kurdish. This dataset's size and focus on linguistic features make it suitable for building robust morphological models.; The dataset 'small corpus of size 590K tokens' is mentioned in the citation context but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.	
Central Kurdish Corpus	cited_context | citing_context	text corpus	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)		The text corpus dataset is used to analyze the frequency of forms in the Central Kurdish language, focusing specifically on the written language. Researchers employ a methodology that prioritizes standard forms through majority voting. This enables detailed linguistic analysis, addressing research questions related to the standardization and usage patterns of Central Kurdish in written contexts.; The text corpus dataset is used to analyze the frequency of linguistic forms in the Central Kurdish language. Researchers employ this dataset to focus on the frequency distribution of specific forms within the corpus, providing insights into the usage patterns and linguistic structure of Central Kurdish. This analysis helps in understanding the language's morphology and syntax, contributing to linguistic research and language documentation.	
Central Kurdish Corpus	citing_context	TIMIT	https://doi.org/10.1007/s10579-022-09594-4 (2022)	https://www.semanticscholar.org/paper/3911933c247f705b2488fdd067330820e8db07bf (2012)	The TIMIT dataset is used to measure probabilities based on di-phones, which provides a foundation for acoustic-phonetic analysis in the Central Kurdish language study. This involves analyzing phonetic segments and their transitions, enabling researchers to understand the acoustic properties and patterns of speech sounds in Central Kurdish.	
Central Pashto Corpus	citing_context	Central Pashto Corpus	https://doi.org/10.1155/2021/3543816 (2021)	https://doi.org/10.1117/12.2003731 (2013)	The Central Pashto Corpus is used to develop and evaluate natural language processing (NLP) models for the Central Pashto language. Specifically, it focuses on text normalization and tokenization tasks. This dataset enables researchers to improve the accuracy and efficiency of NLP models tailored for Central Pashto, addressing challenges unique to the language's structure and usage.	
Central Pashto Corpus	citing_context	Gold-Standard Pashto Dataset	https://doi.org/10.1109/ACCESS.2022.3216881 (2022)	https://doi.org/10.6017/ITAL.V40I1.12553 (2021)	The Gold-Standard Pashto Dataset is used to develop and evaluate segmentation algorithms for Central Pashto text. It consists of 300 text-line images from three Pashto books, enabling researchers to test and refine algorithms specifically tailored for the segmentation of Pashto text. This dataset facilitates the advancement of text processing techniques for the Central Pashto language.	
Central Pashto Corpus	cited_context	KPTI	https://doi.org/10.1109/ICDAR.2017.359 (2017)	https://doi.org/10.1109/ICFHR.2016.0090 (2016)	The KPTI dataset is used to provide a text imagebase and deep learning benchmark for the Pashto language, focusing on character recognition and text analysis. It contains real Pashto calligraphic contents and is employed in deep learning benchmarking and text image analysis, enabling researchers to develop and evaluate algorithms for processing and recognizing Pashto script.	
Central Pashto Corpus	citing_context	Pashto characters database	https://doi.org/10.1155/2021/3543816 (2021)	https://doi.org/10.22581/MUET1982.2101.14 (2021)	The Pashto characters database is used for simulation and experimental work focused on handwritten Pashto character recognition. Researchers apply Zernike Moments and Linear Discriminant Analysis to this dataset to develop and test algorithms for recognizing handwritten Pashto characters. This enables advancements in optical character recognition technology specific to the Pashto script.	
Central Pashto Corpus	cited_context	POLD	https://doi.org/10.7717/peerj-cs.1617 (2023)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The POLD dataset is used to fine-tune XLM-RoBERTa for transfer learning, specifically focusing on multilingual representation learning. It evaluates the effectiveness of pre-trained models in this context, enhancing the model's performance across languages. This dataset enables researchers to assess and improve the cross-lingual capabilities of deep learning models.	
Central Pashto Corpus	citing_context	synthetic dataset	https://doi.org/10.1109/ACCESS.2022.3216881 (2022)	https://doi.org/10.1109/ICET.2009.5353160 (2009)	The synthetic dataset is used to analyze the shape of Pashto script ligatures, focusing on 1000 unique ligatures across four different font sizes. This dataset supports OCR development by providing a structured set of ligatures, enabling researchers to test and improve recognition algorithms specifically for the Pashto script.	
Croatian Corpus	citing_context	24sata comment dataset	https://doi.org/10.26615/978-954-452-072-4_185 (2021)	https://doi.org/10.21248/jlcl.34.2020.224 (2020)	The 24sata comment dataset is used to study and benchmark the moderation of Croatian news comments, particularly focusing on identifying and managing hate speech, abuse, and spam. Researchers employ this dataset to develop and evaluate automated moderation systems, addressing the challenge of efficient content management with limited resources. This dataset enables the analysis of moderator actions and the testing of algorithms to enhance the accuracy and speed of comment moderation in Croatian news platforms.	
Croatian Corpus	cited_context	A Gold Standard Dependency Corpus	https://doi.org/10.1007/978-3-030-58323-1_11 (2020)	https://www.semanticscholar.org/paper/82cf69e48ede65b9d1f419da786c0349342d449d (2014)	The 'A Gold Standard Dependency Corpus' is used for dependency parsing and part-of-speech tagging, primarily in Croatian and English. It provides a structured, high-quality annotated resource for syntactic analysis, enabling researchers to train and evaluate models. This dataset supports the development of more accurate parsing algorithms and enhances the understanding of syntactic structures in both languages.	
Croatian Corpus	citing_context	Automated Student Assessment Prize (ASAP)	https://doi.org/10.23919/MIPRO57284.2023.10159981 (2023)	https://doi.org/10.1162/tacl_a_00236 (2013)	The Automated Student Assessment Prize (ASAP) dataset is used to evaluate automated student assessment systems, specifically focusing on enhancing the accuracy and reliability of grading in Croatian language processing. Researchers employ this dataset to develop and test algorithms that can effectively score student essays, aiming to improve educational technology tools. The dataset's relevance lies in its application to natural language processing tasks, particularly in the context of educational assessments.	
Croatian Corpus	cited_context | citing_context	BERTićdata	https://doi.org/10.18653/v1/2023.vardial-1.11 (2023)	https://www.semanticscholar.org/paper/8c71e90e6cf6a8101e59446faf6f5fbfef4cbdef (2021)	The BERTićdata dataset is used as a text collection for training the BERTić transformer model, specifically tailored for Bosnian, Croatian, Montenegrin, and Serbian languages. This dataset enables researchers to develop multilingual language models by providing a rich corpus of text data, facilitating advancements in natural language processing tasks for these closely related languages.; The BERTićdata dataset is used as a text collection for training the BERTić transformer model, specifically tailored for Bosnian, Croatian, Montenegrin, and Serbian languages. This dataset enables researchers to develop multilingual language models by providing a rich corpus of text data, enhancing the model's performance in understanding and generating content in these closely related languages.	
Croatian Corpus	cited_context	CoNLL 2006-2007 datasets	https://www.semanticscholar.org/paper/a79ae6b4ac855cf9482cb707826fac807f1d6293 (2015)	https://www.semanticscholar.org/paper/10a9abb4c78f0be5cc85847f248d3e8277b3c810 (2007)	The CoNLL 2006-2007 datasets are primarily used for dependency parsing experiments, providing a unified framework for syntactic annotations across multiple languages, including Croatian. These datasets enable researchers to focus on syntactic structures and consistency in dependency parsing, facilitating cross-linguistic comparisons and the development of robust parsing models.	
Croatian Corpus	cited_context	ConLL 2007	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	https://www.semanticscholar.org/paper/10a9abb4c78f0be5cc85847f248d3e8277b3c810 (2007)	The ConLL 2007 dataset is primarily used for multilingual data-driven dependency parsing, focusing on encoding styles and parsing methodologies. It enables researchers to explore and compare different parsing techniques across languages, facilitating advancements in natural language processing. The dataset's multilingual nature supports the development and evaluation of parsing models that can handle diverse linguistic structures.	
Croatian Corpus	cited_context	COPA data set	https://doi.org/10.48550/arXiv.2403.08693 (2024)		The COPA data set is primarily used for part-of-speech tagging and named entity recognition, particularly in Croatian, and contributes to the Universal Dependencies project. It provides a consistent annotation scheme and treebanks, facilitating cross-linguistic research and enabling the development of robust natural language processing models.	
Croatian Corpus	cited_context | citing_context	COPA-HR	https://doi.org/10.18653/v1/2024.vardial-1.7 (2024), https://www.semanticscholar.org/paper/8c71e90e6cf6a8101e59446faf6f5fbfef4cbdef (2021)	https://www.semanticscholar.org/paper/5cfbbf3cdff0f905874589bcd21b2646340a5447 (2011)	The COPA-HR dataset is primarily used to evaluate and benchmark large language models on commonsense reasoning tasks in the Croatian language, focusing on causal and effectual relationships. It is also utilized to train and develop models for various Croatian dialects, such as Cerkno, Torlak, and Chakavian, with an emphasis on preventing model contamination by restricting test data access. Additionally, the dataset supports linguistic research, providing a standardized resource for studying Croatian language features and structures.; The COPA-HR dataset is used to evaluate the BERTi´c model's performance on commonsense causal reasoning tasks, specifically with the Croatian translation of the COPA dataset. This evaluation focuses on assessing the model's ability to understand and reason about cause-and-effect relationships in the Croatian language. The dataset enables researchers to test and improve natural language processing models tailored for Croatian.	
Croatian Corpus	citing_context	CoSimLex	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://www.semanticscholar.org/paper/2e94d9ba9199b874f966881949f57ea6c417773c (2019)	CoSimLex is used to evaluate graded word similarity in context, emphasizing inter-rater agreement and correlation scores across languages. It focuses on morphological variations and contextual nuances, particularly in morphologically rich languages like Croatian. This dataset enables researchers to assess and compare the performance of word similarity models, enhancing understanding of linguistic complexities.	
Croatian Corpus	cited_context | citing_context	Croatian	https://www.semanticscholar.org/paper/418de221eae91c6e215c17e72a98c55fb5970ae6 (2016), https://www.semanticscholar.org/paper/a79ae6b4ac855cf9482cb707826fac807f1d6293 (2015)	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014), https://doi.org/10.1007/978-94-024-0881-2_21 (2017)	The 'Croatian' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how this dataset is used in research based on the provided evidence.; The Croatian dataset is used to evaluate the applicability of the Prague Dependency Treebank annotation scheme for Croatian, specifically focusing on the syntactic annotation of 50 manually annotated sentences. This supports discussions on linguistic annotation methodologies and their suitability for the Croatian language.	
Croatian Corpus	cited_context	Croatian Dependency Treebank 2.0	https://www.semanticscholar.org/paper/a79ae6b4ac855cf9482cb707826fac807f1d6293 (2015)	https://www.semanticscholar.org/paper/4e370875a571a3b9df80a91080ef8f2342d545d6 (2014)	The Croatian Dependency Treebank 2.0 is used to analyze Croatian newspaper text, focusing on specific annotations for subordinate clauses. Adhering to PDT standards, this dataset improves dependency parsing accuracy. Its detailed annotations enable researchers to enhance parsing models, addressing the complexities of Croatian syntax and morphology.	
Croatian Corpus	cited_context	Croatian Dependency Treebanks	https://doi.org/10.3115/v1/W14-0405 (2014)	https://doi.org/10.18653/v1/w13-4903 (2013)	The Croatian Dependency Treebanks dataset is used to train and evaluate parsing models for Croatian and Serbian languages. It focuses on improving tagging accuracy for lemmas, morphosyntactic descriptions, and labeled attachment scores. This dataset enables researchers to develop and refine natural language processing models specifically tailored for Slavic languages, enhancing their performance in syntactic analysis tasks.	
Croatian Corpus	citing_context	Croatian legal texts	https://doi.org/10.1145/3711542.3711587 (2024)	https://doi.org/10.2498/cit.2006.04.08 (2006)	The Croatian legal texts dataset is used to compare association measures for extracting collocations, specifically focusing on bigrams and trigrams. This methodology aids in enhancing document indexing by identifying significant word combinations within legal documents. The dataset's relevance lies in its structured content, which facilitates the analysis of linguistic patterns in a specialized domain.	
Croatian Corpus	cited_context	Croatian LGBT	https://www.semanticscholar.org/paper/ea2c0caebb858e0971c9cb2921d52c7bd33bcc5f (2020)	https://doi.org/10.1075/DUJAL.6.1.04BOO (2017)	The Croatian LGBT dataset is used to analyze manual annotations of SUD in the Croatian language, focusing on linguistic structures and dependencies. It serves as a basis for creating LiLaH emotion lexicons by translating and manually correcting the lexicon into Croatian, Dutch, and Slovene. Additionally, it measures the percentage of SUD comments in the Croatian LGBT community, revealing a higher prevalence compared to the Dutch dataset. This dataset enables detailed linguistic and emotional analysis, supporting cross-lingual and cultural comparisons.	
Croatian Corpus	citing_context	Croatian Network Lexicon	https://www.semanticscholar.org/paper/521226d684c390c76a3136c1e401e61de16d627c (2024)	https://doi.org/10.1093/IJL/ECY024 (2018)	The Croatian Network Lexicon is used to develop a computational framework for analyzing syntactic and semantic structures in the Croatian language. This involves focusing on network structures and linguistic relationships to enhance understanding of the language's grammatical and semantic properties. The dataset enables researchers to explore complex linguistic patterns and relationships, contributing to advancements in computational linguistics and natural language processing for Croatian.	
Croatian Corpus	cited_context	Croatian News Agency (HINA)	https://doi.org/10.1007/978-3-319-53640-8_11 (2016)	https://www.semanticscholar.org/paper/a6a245ae66e50b29a28d30a7a12b70c865f3ba5e (2014)	The Croatian News Agency (HINA) dataset is used to train and test keyword extraction models for Croatian news articles. Researchers focus on selectivity-based methods, employing human expert annotations to enhance model accuracy. This dataset enables the development and evaluation of keyword extraction techniques specifically tailored for news content, addressing the need for effective information retrieval and summarization in the Croatian language.	
Croatian Corpus	citing_context	Croatian Web Corpus (HrWAC)	https://doi.org/10.26615/978-954-452-072-4_185 (2021)	https://doi.org/10.1162/tacl_a_00325 (2019)	The Croatian Web Corpus (HrWAC) is used to pretrain 300D word2vec embeddings, which are then utilized for training the Embedded Topic Model (ETM) and initializing the BiLSTM embedding layer. This approach focuses on Croatian language modeling, enhancing the representation and understanding of textual data in Croatian. The dataset's extensive web content provides a rich resource for improving language models and embeddings, enabling more accurate and contextually relevant linguistic analysis.	
Croatian Corpus	citing_context	Croatian WordNet	https://www.semanticscholar.org/paper/0678f4a1e5d0af642b4659a3da7c0bf0538326e1 (2016)	https://www.semanticscholar.org/paper/eee21b60e5dded43be89a6853fdebdd3c26eb799 (2008)	The Croatian WordNet dataset is used to develop a sense inventory for the Croatian language, serving as a structured lexical database. It provides a comprehensive resource for semantic analysis, enabling researchers to create detailed lexical entries and support linguistic studies focusing on word senses and meanings.	
Croatian Corpus	cited_context	Croatia Weekly 100 thousand wordform (100 kw) subcorpus of Croatian newspaper text from Croatian National Corpus	https://www.semanticscholar.org/paper/da595cab032cc66c7289814bf22c3dabf19b82df (2013)		The Croatia Weekly 100 thousand wordform subcorpus of Croatian newspaper text is used to train hidden Markov model tri-gram taggers, specifically CroTag, for MSD-tagging and lemmatization. This involves utilizing a large, manually annotated and lemmatized subcorpus to enhance the accuracy of part-of-speech tagging and lemmatization in Croatian newspaper texts. The dataset's extensive annotation and lemmatization enable robust training of natural language processing models for linguistic analysis.	
Croatian Corpus	cited_context	Croatia Weekly newspaper corpus	https://www.semanticscholar.org/paper/da595cab032cc66c7289814bf22c3dabf19b82df (2013)	https://www.semanticscholar.org/paper/bb03f030dc650909ef51f2d98894d7ae6abc5f27 (2012)	The Croatia Weekly newspaper corpus is used to evaluate and compare lemmatization results for Croatian, specifically addressing spatial and temporal complexity within a newspaper context. This dataset enables researchers to assess the effectiveness of lemmatization techniques, providing insights into how these methods perform on real-world textual data.	
Croatian Corpus	cited_context	CW100	https://www.semanticscholar.org/paper/0a84fd7f00264ca5de0e857641afe29e041b2cf7 (2008)	https://www.semanticscholar.org/paper/eb1fa531e67c2038cecac89e94b51448fa05c800 (2010)	The CW100 dataset is used to develop multilingual morphosyntactic specifications and lexicons, with a focus on Croatian. It is manually tagged using MULTEXT-East version 3 morphosyntactic descriptors and encoded in XCES, enabling detailed linguistic analysis and the creation of robust morphosyntactic resources for Croatian. This dataset supports research in morphosyntactic tagging and lexicon development, enhancing the accuracy and comprehensiveness of linguistic studies in Croatian.	
Croatian Corpus	cited_context	DSL corpus collection (DSLCC)	https://www.semanticscholar.org/paper/74782d315d5cd472bef874ffbca589cc2285a99f (2016)	https://www.semanticscholar.org/paper/6a503c1fe5fd107612e7cb6e52cd21310e9840cb (2012)	The DSL corpus collection (DSLCC) is used to develop and evaluate systems for identifying language varieties, particularly in newspaper texts across multiple languages, including Croatian. It focuses on distinguishing between closely related languages in realistic language identification scenarios, enabling researchers to enhance the accuracy of language variety recognition.	
Croatian Corpus	cited_context	DSLCC	https://www.semanticscholar.org/paper/74782d315d5cd472bef874ffbca589cc2285a99f (2016)	https://www.semanticscholar.org/paper/0af1f6d083f1b7bc7632e857ea39f43b88ccee12 (2014)	The DSLCC dataset is used in research to discriminate between similar languages by merging comparable data sources. The methodology focuses on enhancing language discrimination techniques. This dataset enables researchers to address specific challenges in distinguishing closely related languages, leveraging its unique compilation of comparable linguistic data.	
Croatian Corpus	cited_context | citing_context	DSLCC collection	https://doi.org/10.18653/v1/2023.vardial-1.11 (2023)	https://doi.org/10.3115/v1/W14-5307 (2014)	The DSLCC collection is used in the VarDial shared task to evaluate dialect identification systems, focusing on cross-dialectal and cross-lingual classification. It is employed to assess models' performance in dialectal and cross-lingual language processing, enabling researchers to address specific challenges in language variation and classification. The dataset's relevance lies in its ability to provide a standardized benchmark for these tasks.; The DSLCC collection is used in the Var-Dial shared task for evaluating models on dialect identification and cross-lingual tasks. It focuses on cross-dialectal classification and language variation, enabling researchers to assess the performance of dialect identification systems. This dataset supports the development and testing of models that handle linguistic diversity and variation.	
Croatian Corpus	citing_context	EMMediaTopic	https://doi.org/10.1109/ACCESS.2025.3544814 (2024)		The EMMediaTopic dataset is used as a freely available resource for Croatian language studies, specifically focusing on media topics. It supports linguistic and content analysis by providing a structured collection of media-related data, enabling researchers to explore language use and media content in the Croatian context.	
Croatian Corpus	cited_context	fhrWaC	https://doi.org/10.18653/v1/W17-1411 (2017)	https://www.semanticscholar.org/paper/90477234a782117b1d0273b9c1f73194fa65e9e5 (2013)	The fhrWaC dataset is used to train 300-dimensional skip-gram word embeddings, specifically for the Croatian language. As a filtered version of the Croatian web corpus, it enhances the quality of these embeddings, which are then utilized in various natural language processing tasks. This dataset enables researchers to develop more accurate and contextually relevant language models for Croatian.	
Croatian Corpus	citing_context	fHrWaC1	https://www.semanticscholar.org/paper/0fa15803dda876d88d86b6ad5b2e443fa3f69f2e (2014)	https://www.semanticscholar.org/paper/90477234a782117b1d0273b9c1f73194fa65e9e5 (2013)	The fHrWaC1 dataset, a 1.2 billion words corpus derived from web data, is primarily used for building and evaluating distributional memories for the Croatian language. It serves as a foundational resource for studying Croatian language patterns and distributions, enabling researchers to filter and process large-scale web-derived data for linguistic and computational analyses. This dataset facilitates the development of linguistic models and supports research into the structural and semantic aspects of the Croatian language.	
Croatian Corpus	citing_context	dataset for sentiment analysis of Croatian news articles	https://doi.org/10.48550/arXiv.2305.08173 (2023)	https://www.semanticscholar.org/paper/bab199bb2504c670b384f0b4659fd454ce8d05df (2010)	The dataset for sentiment analysis of Croatian news articles is primarily used for analyzing sentiment and stance in various contexts, such as financial texts and user comments on online news articles. It employs rule-based annotation methods and provides labeled data for training and evaluating models. The dataset supports zero-shot classification using Slovene resources, enhancing cross-lingual transfer and addressing the scarcity of labeled Croatian data. This enables researchers to explore public opinion and model performance across multiple languages, including Croatian.	
Croatian Corpus	citing_context	dataset from Pelicon et al. (2021b)	https://doi.org/10.48550/arXiv.2211.06053 (2022)	https://doi.org/10.7717/peerj-cs.559 (2021)	The dataset from Pelicon et al. (2021b) is used for binary classification of abusive language in Croatian, employing machine learning models to distinguish abusive from non-abusive content. This dataset enables researchers to develop and evaluate algorithms that can accurately identify abusive language, contributing to the creation of safer online environments. The dataset's focus on the Croatian language provides a valuable resource for linguistic and computational studies in this specific context.	
Croatian Corpus	citing_context	gaming review text spans in Croatian	https://doi.org/10.48550/arXiv.2305.08173 (2023)	https://doi.org/10.18653/v1/W17-1411 (2017)	The 'gaming review text spans in Croatian' dataset is used to train and evaluate sentiment analysis models, specifically focusing on identifying positive and negative sentiments in gaming reviews. This dataset enables researchers to develop and test algorithms that can accurately classify sentiments in Croatian text, enhancing natural language processing capabilities for this language.	
Croatian Corpus	cited_context	HINA	https://doi.org/10.1007/978-3-319-53640-8_11 (2016)	https://doi.org/10.1016/J.AMC.2014.04.090 (2014)	The HINA dataset is used for extracting keywords from Croatian news articles, employing selectivity-based methods for keyword extraction. This focuses on enhancing the selectivity and relevance of extracted keywords in the context of news content, aiding in the analysis and summarization of news articles.	
Croatian Corpus	cited_context	HOBS	https://www.semanticscholar.org/paper/a79ae6b4ac855cf9482cb707826fac807f1d6293 (2015)	https://www.semanticscholar.org/paper/16568d23528ca9a4ebf12dd96038c7c90f067aba (2012)	The HOBS dataset, comprising over 3,000 sentences, is used for dependency parsing experiments in Croatian language processing. Researchers employ this dataset to develop and evaluate parsing models, focusing on syntactic structure analysis. The dataset's size and linguistic richness enable robust testing and validation of parsing algorithms, enhancing the accuracy of Croatian language computational tools.	
Croatian Corpus	cited_context	HR corpus	https://www.semanticscholar.org/paper/d7aef50b236193c60e9c5e573aeccad70393dbe9 (2015)	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	The HR corpus is used to study the Croatian language by analyzing 4,000 annotated sentences for linguistic features and patterns. Researchers employ this dataset to explore specific linguistic characteristics, using annotation data to identify and examine various grammatical and semantic structures. This enables detailed linguistic analysis and contributes to understanding the complexities of the Croatian language.	
Croatian Corpus	cited_context | citing_context	hr500k	https://doi.org/10.4312/SLO2.0.2016.2.156-188 (2016), https://doi.org/10.18653/v1/W19-3704 (2019), https://doi.org/10.1007/978-3-030-58323-1_11 (2020)	https://doi.org/10.1075/IJCL.7.2.05MAI (2002), https://www.semanticscholar.org/paper/0144ec309972cb885293deb0264dd2b347d7822e (2017)	The hr500k dataset is used to analyze Croatian language patterns, particularly by comparing part-of-speech distributions with Twitter data. This comparison helps identify linguistic shifts, enabling researchers to understand changes in language use over time and across platforms. The dataset's focus on part-of-speech tagging is crucial for these analyses.; The hr500k dataset is primarily used for training and evaluating models focused on Croatian language processing, particularly in morphological and syntactic analysis. It provides extensive inﬂectional lexicons with over 100 thousand lemmas and 3 million inﬂected forms, enhancing morphosyntactic annotation accuracy. The dataset also supports comparative studies of dataset sizes across Slavic languages, highlighting its utility in linguistic research and model development.	
Croatian Corpus	cited_context	H R dependency treebank	https://www.semanticscholar.org/paper/a79ae6b4ac855cf9482cb707826fac807f1d6293 (2015)	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	The H R dependency treebank is used for state-of-the-art Croatian lemmatization, tagging, named entity classification, and dependency parsing, providing linguistic annotations essential for these tasks. It is also utilized to analyze syntactic structures in Croatian, particularly focusing on dependency relations within the SETimes.HR corpus. This dataset enables detailed linguistic analysis and the development of NLP models tailored to the Croatian language.	
Croatian Corpus	cited_context	hrLex	https://doi.org/10.1017/jlg.2018.9 (2018), https://doi.org/10.18653/v1/W19-3704 (2019)	https://www.semanticscholar.org/paper/418de221eae91c6e215c17e72a98c55fb5970ae6 (2016)	The hrLex dataset is used to generate lists of word pairs with identical morphosyntactic descriptions and word forms, focusing on inflectional morphology in both Croatian and Serbian. It supports morphological analysis by providing extensive inﬂectional lexicons, including over 100 thousand lemmas and around 3 million inﬂected forms for Croatian. The dataset also facilitates comparative studies of dataset sizes across Slavic languages, highlighting the smaller scale of Serbian datasets.	
Croatian Corpus	citing_context	hrMWELex	https://doi.org/10.18653/v1/W17-1727 (2017)	https://www.semanticscholar.org/paper/d7aef50b236193c60e9c5e573aeccad70393dbe9 (2015)	The hrMWELex dataset is used to compile a lexicon of Croatian multi-word expressions (MWEs) by extracting high-recall, low-precision candidate n-grams from parsed corpora like hrWaC using predefined syntactic patterns. This dataset supports linguistic analysis, focusing on the identification and compilation of MWEs for further research.	
Croatian Corpus	cited_context | citing_context	hrWaC corpus	https://doi.org/10.18653/v1/W17-1727 (2017), https://doi.org/10.4312/SLO2.0.2013.2.35-57 (2013), https://doi.org/10.18653/v1/W17-1409 (2017), https://www.semanticscholar.org/paper/90477234a782117b1d0273b9c1f73194fa65e9e5 (2013)	https://doi.org/10.1007/978-3-642-23538-2_50 (2011), https://www.semanticscholar.org/paper/90477234a782117b1d0273b9c1f73194fa65e9e5 (2013)	The hrWaC corpus is primarily used for linguistic analysis, particularly focusing on multi-word expressions (MWEs) in Croatian. It is utilized to analyze the distribution and usage patterns of MWEs in web texts, compile a lexicon of MWEs, and train models for identifying MWEs. The dataset's large size (1.2B tokens) and web-derived content enable detailed statistical feature extraction and annotation adjudication, contributing to the development of linguistic resources and models.; The hrWaC dataset is used for Croatian language processing, specifically to build 300-dimensional word vectors using tools like word2vec and tf-idf-weighted bag-of-words. It supports tasks such as part-of-speech tagging, lemmatization, and dependency parsing, often employing skip-gram embeddings and MSTParser for training and evaluation. This web corpus enhances vector quality and facilitates various linguistic analyses.	hrWaC
Croatian Corpus	citing_context	hrWaC2	https://www.semanticscholar.org/paper/0678f4a1e5d0af642b4659a3da7c0bf0538326e1 (2016)	https://doi.org/10.3115/v1/W14-0405 (2014)	The hrWaC2 dataset, a Croatian language corpus containing web texts, is used for linguistic analysis and natural language processing tasks. Researchers utilize this dataset to extract and analyze data, focusing on the Croatian language. The corpus enables detailed linguistic studies and supports the development of NLP models tailored to the Croatian language.	
Croatian Corpus	citing_context	initial Croatian dataset	https://doi.org/10.4312/SLO2.0.2013.2.35-57 (2013)	https://doi.org/10.3115/1557769.1557830 (2007)	The initial Croatian dataset is used to train a part-of-speech (POS) tagger model for Croatian data, specifically employing the HunPos tagger. This dataset focuses on enhancing the accuracy of POS tagging for unseen data, enabling more reliable linguistic analysis and natural language processing tasks in the Croatian language.	
Croatian Corpus	cited_context	JOS corpus of Slovene	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	https://www.semanticscholar.org/paper/cedf0376f7ada5c478d3877b7ca2ef73714356fc (2010)	The JOS corpus of Slovene is used as a reference for linguistic tagging and treebanking practices, aiding in the development of similar resources for Croatian. It is also utilized for experiments in named entity recognition for both Croatian and Slovene, providing linguistic data for training and evaluation. This dataset's comprehensive tagging and structured content enable researchers to develop and refine natural language processing tools and methodologies.	
Croatian Corpus	citing_context	large dataset of tweets in Croatian	https://doi.org/10.3390/electronics13101991 (2024)	https://doi.org/10.3390/app112110442 (2021)	The large dataset of tweets in Croatian is used for sentiment analysis, particularly focusing on public sentiment during the pandemic. Researchers employ natural language processing (NLP) techniques to develop frameworks that analyze and understand the emotional tone of Croatian tweets related to the crisis. This dataset enables detailed insights into public reactions and perceptions, facilitating a nuanced understanding of societal responses to the pandemic.	
Croatian Corpus	citing_context	Leximirka data category thesaurus	https://www.semanticscholar.org/paper/3372b740359499a2b946f8132308b55813599749 (2024)		The Leximirka data category thesaurus is used to control and extract morphological, domain, syntactic, and semantic features of lexical entries in Croatian language research. It supports structured linguistic analysis by applying hand-crafted rules to establish links between entries, enabling detailed examination and categorization of lexical data.	
Croatian Corpus	citing_context	LIdioms	https://www.semanticscholar.org/paper/521226d684c390c76a3136c1e401e61de16d627c (2024)	https://www.semanticscholar.org/paper/a74fd4914cfd5c5cafd7607289ff3dc0b1a81a51 (2018)	The LIdioms dataset is used to link idioms across multiple languages, focusing on multilingual aspects of idiom usage and translation. It models data according to multilingual linked idioms specifications, emphasizing linguistic structures and cross-language relationships. The dataset's schema and format are referenced to structure lexical entries and senses, supporting natural language processing applications by enhancing cross-lingual understanding and processing.	
Croatian Corpus	citing_context	MaCoCu Corpora	https://doi.org/10.1109/ACCESS.2025.3544814 (2024)	https://www.semanticscholar.org/paper/24f404b302d20aa9002abaea078a810b2ec42f1a (2022)	The MaCoCu Corpora dataset is used to support research in under-resourced European languages, including Croatian, by leveraging comparable web corpora. It ensures comparability between language-specific datasets, facilitating the evaluation and development of methodologies tailored for these languages. This dataset enables researchers to address challenges in less-resourced language processing and improve the performance of natural language processing systems in such contexts.	
Croatian Corpus	cited_context	MULTEXT-East	https://doi.org/10.18653/v1/W19-3704 (2019)	https://doi.org/10.1007/s10579-011-9174-8 (2011)	The MULTEXT-East dataset is used to define morphosyntactic descriptions for Central and Eastern European languages, focusing on encoding part-of-speech and feature-value pairs. This enables detailed morphosyntactic analysis, supporting research in linguistic annotation and computational linguistics. The dataset's structured format facilitates consistent and standardized analysis across multiple languages.	
Croatian Corpus	citing_context	Multi-SimLex	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://doi.org/10.1162/coli_a_00391 (2020)	The Multi-SimLex dataset is used to evaluate multilingual and crosslingual lexical semantic similarity, specifically focusing on 1,888 concept pairs across 13 typologically diverse languages. Researchers employ this dataset to assess the performance of models in capturing semantic relationships between words in different languages, enabling the comparison of lexical semantic similarity across linguistic boundaries.	
Croatian Corpus	cited_context	OSCAR	https://doi.org/10.48550/arXiv.2403.08693 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The OSCAR dataset is used to evaluate multilingual text processing, particularly for non-English European languages, by assessing the quality and coverage of the corpus. It is created through deduplication of paragraphs from Common Crawl dumps, language identification with fastText, and text quality assessment using perplexity scores. This enables researchers to address specific linguistic tasks and improve multilingual corpora.	
Croatian Corpus	citing_context	ParlaMint corpus	https://www.semanticscholar.org/paper/2ff724bf76ebf908bf0ce0d08a10736ac60a549a (2022)	https://doi.org/10.1109/ICASSP.2015.7178964 (2015)	The ParlaMint corpus is used to develop and enhance Automatic Speech Recognition (ASR) systems for Croatian, specifically leveraging parliamentary proceedings. It is employed to build the first freely available ASR dataset for Croatian and to train in-house ASR models, focusing on improving speech recognition accuracy in legislative contexts. This dataset's extensive transcription of parliamentary speeches enables researchers to address the unique challenges of recognizing formal, legislative speech.	
Croatian Corpus	citing_context	PARSEME Croatian dataset	https://www.semanticscholar.org/paper/521226d684c390c76a3136c1e401e61de16d627c (2024)	https://www.semanticscholar.org/paper/1c7920a31c52fa9f7fe9731655cd4d75cd425788 (2018)	The PARSEME Croatian dataset is used to identify and annotate verbal multiword expressions (MWEs) in Croatian sentences. It focuses on automatic annotation and recognition of MWEs, enabling research in natural language processing tasks such as improving the accuracy of MWE identification in the Croatian language. This dataset supports the development and evaluation of algorithms for MWE recognition in shared tasks.	
Croatian Corpus	cited_context	PDT-based Croatian Dependency Treebank	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	https://doi.org/10.1007/978-3-642-40585-3_70 (2013)	The PDT-based Croatian Dependency Treebank is used as a reference for linguistic tagging and treebanking practices, aiding in the development of similar resources for Croatian. It supports experiments in named entity recognition for Croatian and Slovene, providing linguistic data for training and evaluation. Additionally, it facilitates inter-annotator agreement and statistical dependency parsing, enhancing syntactic formalisms and parsing performance in Croatian.	
Croatian Corpus	cited_context	Prague Dependency Treebank	https://www.semanticscholar.org/paper/a5e7d1e42e7c0010b450dbfb1dad52df9523e0a1 (2014)	https://doi.org/10.1007/978-94-010-0201-1_7 (2003)	The Prague Dependency Treebank is used as a reference for dependency parsing, providing a structured representation of syntactic dependencies in sentences. It serves as a benchmark for evaluating parsing algorithms and models, though its specific application to the Croatian language is not mentioned. The dataset's detailed annotation of linguistic structures enables researchers to develop and refine parsing techniques, enhancing the accuracy of natural language processing systems.	
Croatian Corpus	citing_context	SemEval-2017 Task 2	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://doi.org/10.18653/v1/S17-2002 (2017)	The SemEval-2017 Task 2 dataset is used to evaluate multilingual and cross-lingual semantic word similarity, focusing on pairs of words in five different languages. Researchers employ this dataset to assess models' ability to capture semantic similarities across languages, enabling the development and refinement of multilingual natural language processing techniques.	
Croatian Corpus	cited_context | citing_context	SETimes	https://doi.org/10.18653/v1/2023.vardial-1.11 (2023)		The SETimes dataset is utilized for compiling a corpus of 9,258 whole documents in Croatian, supporting linguistic and computational analysis. It is also employed to study language classification in social media, particularly for identifying the language of tweets. The dataset includes user entries with labeled tweets, facilitating the analysis of linguistic patterns in Croatian and other languages. This dual use enhances both corpus linguistics and social media language identification research.; The SETimes dataset is used to compile a corpus of 9,258 documents from the SETimes website, focusing on Southeast European languages, including Croatian. It is also utilized for language classification in social media, specifically to identify the language of tweets. The dataset contains user entries with labeled tweets, enabling researchers to analyze linguistic patterns in Croatian and other languages. This dual use supports both corpus linguistics and computational linguistics, facilitating studies on language identification and classification.	
Croatian Corpus	citing_context	SimLex-999	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://www.semanticscholar.org/paper/9ba6b2fdd0554dee0770fda2a413057482c51c65 (2019)	The SimLex-999 dataset is used to evaluate word similarity in Croatian and Finnish translations, focusing on semantic specialization and distributional vector spaces. It is adapted for annotation methodology to inform the development of similar resources for Croatian. The dataset also evaluates graded word similarity in context, examining inter-rater agreement and morphological variations in morphologically rich languages like Croatian.	
Croatian Corpus	citing_context	The Multilingual Amazon Reviews Corpus	https://doi.org/10.48550/arXiv.2305.08173 (2023)	https://www.semanticscholar.org/paper/5b476295e04c99577ec30865980102fb25832f18 (2013)	The Multilingual Amazon Reviews Corpus is used to train and evaluate models for sentiment analysis across multiple languages, including Croatian. Research focuses on cross-lingual transfer and model performance, leveraging the dataset's multilingual reviews to enhance and assess the effectiveness of sentiment analysis models in diverse linguistic contexts.	
Croatian Corpus	citing_context	tokenised and lemmatised versions of Wikipedia for 45 languages	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://www.semanticscholar.org/paper/3ee4ea72679b2ec3814053a0ec5b5a16188b3948 (2017)	The tokenised and lemmatised versions of Wikipedia for 45 languages are used in research to conduct linguistic processing and analysis. This involves examining linguistic patterns across multiple languages. The dataset's comprehensive coverage of 45 languages and its pre-processed format enable researchers to efficiently analyze large-scale linguistic data, facilitating cross-linguistic studies and the identification of common and unique linguistic features.	
Croatian Corpus	cited_context	Twitter	https://doi.org/10.1017/jlg.2018.9 (2018)	https://doi.org/10.3115/v1/E14-1011 (2014)	The Twitter dataset is used to map dialectal variation in linguistic features, particularly in comparing the spatial distribution of these features to those established by traditional dialectological methods. This involves analyzing tweets to identify and geolocate linguistic patterns, enabling researchers to explore how language varies across different regions. The dataset's large volume and geotagged nature facilitate detailed spatial linguistic analysis.	
Croatian Corpus	citing_context	UWA dataset	https://doi.org/10.18653/v1/2020.semeval-1.3 (2020)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The UWA dataset is augmented with SemCor to train pretrained embeddings for each sense in WordNet, specifically supporting research on Croatian language processing. This methodology enhances the representation of word senses, enabling more accurate and nuanced natural language processing tasks in Croatian.	
Croatian Corpus	citing_context	VoxPopuli	https://www.semanticscholar.org/paper/2ff724bf76ebf908bf0ce0d08a10736ac60a549a (2022)	https://doi.org/10.21437/interspeech.2022-143 (2021)	The VoxPopuli dataset is used to study parliamentary debates in Croatian, leveraging 42 hours of transcribed speech data. It supports speech and language processing research, enabling the analysis of spoken content in legislative contexts. This dataset facilitates the development and evaluation of speech recognition and natural language processing techniques tailored to the Croatian language.	
Croatian Corpus	citing_context	XCOPA	https://doi.org/10.18653/v1/2024.vardial-1.7 (2024)	https://doi.org/10.22364/bjmc.2022.10.3.19 (2022)	The XCOPA dataset is used to evaluate cross-lingual language understanding, particularly in the choice of plausible alternatives task. It spans 11 languages, including Croatian, and is employed to assess models' ability to select the most appropriate continuation of a given context. This dataset enables researchers to compare performance across languages, providing insights into the effectiveness of cross-lingual transfer and model generalization.	
Czech Corpus	cited_context | citing_context	AKCES-GEC	https://doi.org/10.18653/v1/2021.acl-short.89 (2021), https://doi.org/10.18653/v1/D19-5545 (2019), https://doi.org/10.18653/v1/2020.aacl-main.83 (2020)	https://www.semanticscholar.org/paper/13e7359db140a0648f1f3a51532d97ab34fd80e9 (2016), https://www.semanticscholar.org/paper/cb90769caefe8a204d6c4839b6c0466efb041316 (2011)	The AKCES-GEC dataset is primarily used for grammatical error correction in non-native Czech writing, providing manually annotated transcripts of essays. It supports the study of syntactic and semantic structures through two-level annotation methods. Additionally, it is utilized for training and evaluating error correction models, particularly in low-resource scenarios, enhancing the performance of such systems in both Czech and German contexts.; The AKCES-GEC dataset is used for grammatical error correction in Czech, specifically focusing on identifying and correcting errors in learner texts. It leverages the morphological richness and syntactic complexity of the dataset to enhance error detection and correction. This dataset is employed to evaluate and improve grammatical error correction models, ensuring they effectively handle the nuances of written Czech.	
Czech Corpus	citing_context	Belebele	https://doi.org/10.48550/arXiv.2412.17933 (2024)	https://doi.org/10.18653/v1/2024.acl-long.44 (2023)	The Belebele dataset is used to evaluate reading comprehension across 122 languages, with a focus on translation quality and cross-lingual performance. Researchers employ this dataset to assess how well models understand and translate text, enabling the analysis of linguistic nuances and performance metrics across diverse languages. This dataset facilitates the comparison of model accuracy and robustness in multilingual settings.	
Czech Corpus	cited_context	CC-100	https://doi.org/10.18653/v1/2024.acl-long.44 (2023)	https://doi.org/10.48550/arXiv.2301.10472 (2023)	The CC-100 dataset is used as training data for multilingual masked language models, enabling researchers to compare the performance of models like XLM-V, XLM-R, and INFOXLM on low-resource languages. This dataset facilitates the evaluation of these models' effectiveness in handling less commonly studied languages, contributing to advancements in multilingual natural language processing.	
Czech Corpus	cited_context	CC25	https://doi.org/10.48550/arXiv.2212.10140 (2022)	https://www.semanticscholar.org/paper/c20c68c45127439139a08adb0b1f2b8354a94d6c (2019)	The CC25 dataset is used to train the mBART model, leveraging high-quality monolingual data extracted from web crawl data. This dataset enables researchers to enhance the performance of multilingual neural machine translation models by providing extensive, contextually rich textual content. The focus is on improving the quality and fluency of translations, particularly in low-resource languages.	
Czech Corpus	citing_context	CDSC	https://doi.org/10.48550/arXiv.2304.01922 (2023)	https://doi.org/10.48550/arXiv.2212.01692 (2022)	The CDSC dataset is primarily used for natural language processing tasks, including named entity recognition, classification, natural language inference, and question answering. It contains between 8,000 to 10,000 annotated instances, which are utilized to evaluate few-shot learning models and compositional distributional semantics in context-dependent scenarios. The dataset's thematic diversity, particularly its focus on Slavic languages, makes it valuable for assessing logical reasoning and sentiment analysis in these linguistic contexts.	
Czech Corpus	cited_context | citing_context	claim matching	https://doi.org/10.1007/s10579-023-09654-3 (2022)	https://doi.org/10.18653/v1/2021.acl-long.347 (2021)	The 'claim matching' dataset is used to detect and match claims in multilingual texts, specifically focusing on identifying and aligning similar claims across five languages. It includes over 5k examples for detection and over 2k claim pairs for matching. This dataset enables researchers to focus on verifying statements and aligning similar claims, enhancing cross-lingual claim analysis and verification methodologies.; The 'claim matching' dataset is used to align and detect claims in multilingual texts, specifically focusing on five languages. It includes over 2,000 claim pairs for matching and over 5,000 examples for detection. The dataset enables researchers to identify and verify similar statements across languages, facilitating cross-lingual claim analysis and verification.	
Czech Corpus	citing_context	C L ANG -8	https://doi.org/10.18653/v1/2021.acl-short.89 (2021)	https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b (2019)	The C L ANG -8 dataset is used to fine-tune T5 models for grammatical error correction (GEC), demonstrating superior performance compared to models trained on LANG-8. This dataset simplifies the training process for GEC models, making it an effective resource for enhancing the accuracy and efficiency of grammatical error correction in natural language processing tasks.	
Czech Corpus	cited_context	CLEF 2007 Czech test-collection	https://doi.org/10.1016/J.IPM.2009.06.001 (2009)	https://doi.org/10.1007/978-3-540-85760-0_4 (2008)	The CLEF 2007 Czech test-collection is used to evaluate morphological processing challenges in the Czech language, specifically focusing on stemming approaches and their effectiveness in information retrieval tasks. Researchers employ this dataset to assess and compare different stemming techniques, addressing the complexities of Czech morphology. This enables the development and refinement of algorithms that improve the accuracy and efficiency of information retrieval systems tailored for the Czech language.	
Czech Corpus	cited_context | citing_context	CNEC	https://doi.org/10.26615/978-954-452-072-4_149 (2021), https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	https://doi.org/10.18653/v1/W19-3712 (2019), https://doi.org/10.1007/978-3-540-74628-7_26 (2007)	The CNEC dataset is used to evaluate the performance of language models, specifically SlavicBERT and Czert-B, on named entity recognition tasks in the Czech language. It focuses on achieving state-of-the-art results and assessing model performance on language-specific tasks, contributing to advancements in Slavic language processing.; The CNEC dataset is primarily used for developing and evaluating named entity recognition (NER) systems for the Czech language. It focuses on annotating named entities in a corpus of approximately 6000 sentences with 150,000 tokens. Researchers use it to improve tagger accuracy, assess annotation quality, and compare annotation schemes with other corpora like CoNLL. The dataset serves as a crucial resource for NER in Czech, enabling the evaluation and development of models tailored to the language.	
Czech Corpus	cited_context	CNEC 1.1	https://doi.org/10.26615/978-954-452-072-4_065 (2021)	https://www.semanticscholar.org/paper/421fc2556836a6b441de806d7b393a35b6eaea58 (2018)	The CNEC 1.1 dataset is used to evaluate named entity recognition (NER) models, particularly Flair and BERT, in the Czech language. Research focuses on comparing model performance and the effects of combining these models. The dataset enables detailed performance assessments and helps identify the strengths and weaknesses of different NER approaches in Czech.	
Czech Corpus	citing_context	CNEC 2	https://doi.org/10.26615/978-954-452-072-4_119 (2021)	https://doi.org/10.18653/v1/P19-1527 (2019)	The CNEC 2 dataset is used to train the Czech model of the NameTag 2 neural named entity tagger, specifically for named entity recognition in the Czech language. This involves employing neural network methodologies to identify and classify named entities within Czech text, enhancing the accuracy of natural language processing tasks in this language.	
Czech Corpus	cited_context	CNEC 2.0	https://www.semanticscholar.org/paper/325955a4a078988450c73a6581d25f305dd133cc (2020)	https://www.semanticscholar.org/paper/f4ba954b0412773d047dc41231c733de0c1f4926 (2001)	The CNEC 2.0 dataset is used to evaluate Seq2Seq and LSTM-CRF models for Czech named entity recognition, focusing on segmentation and labeling accuracy in sequence data. This dataset enables researchers to assess model performance in identifying and classifying named entities within Czech text, providing a benchmark for comparing different approaches in natural language processing.	
Czech Corpus	cited_context	CoMMuTE	https://doi.org/10.48550/arXiv.2212.10140 (2022)	https://doi.org/10.3115/1073083.1073135 (2002)	The CoMMuTE dataset is used to evaluate machine translation models from English to Czech. Researchers employ metrics such as BLEU, COMET, and accuracy scores to assess the performance of these models. This dataset enables the systematic evaluation and comparison of different machine translation approaches, focusing on their effectiveness in translating between these two languages.	
Czech Corpus	cited_context	CoNLL corpora	https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	https://doi.org/10.3115/1118853.1118877 (2002)	The CoNLL corpora are used to evaluate the performance of language-independent named entity recognition (NER) systems, focusing on cross-lingual generalization and accuracy. These datasets enable researchers to assess how well NER models perform across different languages, providing a standardized benchmark for comparing system effectiveness in multilingual settings.	
Czech Corpus	cited_context	CoNLL-2009	https://doi.org/10.3115/1596409.1596418 (2009)	https://doi.org/10.3115/1596324.1596360 (2008)	The CoNLL-2009 dataset is used for training and evaluating models in syntactic and semantic dependency parsing across multiple languages, including Czech. It focuses on linguistic features and structures, enabling researchers to analyze and model syntactic and semantic dependencies. This dataset supports the development of robust parsing models by providing annotated data that captures complex linguistic relationships.	
Czech Corpus	citing_context	CoNLL version of the dataset	https://doi.org/10.26615/978-954-452-072-4_149 (2021)	https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	The CoNLL version of the dataset is used for training and evaluating a Conditional Random Fields (CRF)-based Czech named entity recognizer. It focuses on enhancing the accuracy of identifying entities in Czech text, specifically addressing the challenges of named entity recognition in this language. The dataset's structured annotations enable researchers to develop and refine models that improve entity identification performance.	
Czech Corpus	citing_context	Corpus of Czech Verse	https://doi.org/10.18653/v1/2025.nlp4dh-1.45 (2025)	https://doi.org/10.12697/SMP.2015.2.1.05 (2015)	The Corpus of Czech Verse is used for linguistic and literary analysis of Czech poetry, containing 80,229 poems. Researchers employ this dataset to study the structural and thematic elements of Czech verse, enabling detailed examinations of poetic language and literary trends. The large scale of the corpus supports comprehensive and statistically robust analyses, facilitating insights into the evolution and characteristics of Czech poetry.	
Czech Corpus	citing_context	CSFD	https://doi.org/10.48550/arXiv.2204.13915 (2022)	https://www.semanticscholar.org/paper/1c61f9ef06fe74505775a833ff849185757199e7 (2011)	The CSFD dataset is used for analyzing sentiment in Czech movie reviews, often compared with the English IMDB dataset to explore cross-lingual sentiment analysis methodologies. Research focuses on understanding the effectiveness of sentiment analysis techniques across languages, leveraging the dataset's specific context in Czech cinema to enhance cross-lingual studies.	
Czech Corpus	cited_context	CSFD CZ	https://doi.org/10.1007/978-3-030-27947-9_18 (2019)	https://www.semanticscholar.org/paper/4d16a47fb6708704b155855045c9e5d2ea380bb0 (2013)	The CSFD CZ dataset is used to train sentiment analysis models, specifically focusing on user comments from the ČSFD film database. This enables researchers to analyze and understand public opinion and sentiment expressed in Czech social media. The dataset's focus on film-related comments provides a rich source of data for evaluating sentiment in a culturally relevant context.	
Czech Corpus	citing_context	CsFEVER	https://www.semanticscholar.org/paper/b0135059c206073c4a876eaab21d3f6bc11c3a94 (2022)	https://doi.org/10.18653/v1/N19-1423 (2019)	The CsFEVER dataset is used to fine-tune the M-Bert model for Czech language fact verification. It focuses on retrieving top-ranking documents for queries, enhancing the model's ability to accurately verify facts in the Czech language. This dataset enables researchers to improve the performance of natural language processing models in specific linguistic contexts.	
Czech Corpus	cited_context | citing_context	CsFEVER-NLI	https://doi.org/10.1007/s10579-023-09654-3 (2022), https://doi.org/10.48550/arXiv.2412.17933 (2024), https://doi.org/10.1007/s00521-024-10113-5 (2023)	https://doi.org/10.1609/aaai.v33i01.33016859 (2018)	The CsFEVER-NLI dataset is used to evaluate and improve model performance on fact extraction and verification tasks, particularly through neural semantic matching. It includes 228k translated FEVER-NLI pairs and data from CTK articles, expanding multilingual NLI benchmarks with native Czech content. This dataset supports research in natural language inference, enhancing multilingual benchmarks and machine translation accuracy for fact verification.; The CsFEVER-NLI dataset is used to facilitate fact extraction and verification in the Czech language. It involves translating 228k FEVER-NLI pairs into Czech, enabling researchers to evaluate model performance on these tasks, particularly through neural semantic matching. Studies compare macro-F1 scores between English and Czech translations, highlighting the dataset's role in cross-lingual evaluation and improvement of fact verification models.	
Czech Corpus	citing_context	CsRest-M	https://www.semanticscholar.org/paper/0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e (2024)	https://doi.org/10.1109/IJCNN.2018.8489042 (2018)	The CsRest-M dataset is used to evaluate the RobeCzech model on the E2E-ABSA task, specifically for aspect extraction and sentiment analysis in Czech restaurant reviews. This dataset enables researchers to assess the performance of natural language processing models in understanding and analyzing customer sentiments and specific aspects mentioned in reviews.	
Czech Corpus	citing_context	CTK corpus	https://www.semanticscholar.org/paper/b0135059c206073c4a876eaab21d3f6bc11c3a94 (2022)	https://www.semanticscholar.org/paper/98bb75dcb7dfe8e675781fe2008170e8f00a5dee (2021)	The CTK corpus is used to annotate claims and evidence from a news agency archive, focusing on non-encyclopedic language styles. It is employed to verify facts over unstructured and structured information, using a ground truth corpus extracted from press agency articles. This dataset enables research into fact-checking and claim verification methods, particularly in contexts that differ from Wikipedia-based datasets.	
Czech Corpus	citing_context	CTKFacts	https://doi.org/10.48550/arXiv.2304.01922 (2023)	https://doi.org/10.1007/s10579-023-09654-3 (2022)	The CTKFacts dataset is used for natural language inference (NLI) in the context of fact verification, specifically for the Czech language. It consists of premises extracted from Wikipedia and manually annotated hypotheses. Researchers use this dataset to assess NLI in standard settings, focusing on the accuracy of fact verification tasks. The dataset's manual annotations and Czech-specific content enable detailed analysis of NLI models' performance in verifying factual claims.	
Czech Corpus	cited_context | citing_context	Czech corpus of 11,000 manually annotated NEs	https://www.semanticscholar.org/paper/0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e (2024), https://doi.org/10.18653/v1/W19-3709 (2019), https://doi.org/10.18653/v1/2023.bsnlp-1.21 (2023), https://doi.org/10.18653/v1/W17-1412 (2017)	https://doi.org/10.13053/cys-20-3-2469 (2016), https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	The Czech dataset (Hercig et al., 2016) is primarily used to enhance and evaluate aspect-based sentiment analysis in the Czech language. It has been re-annotated and expanded with over 1,000 new reviews, improving its coverage and quality. The dataset is utilized to evaluate tasks such as aspect term extraction (ATE), aspect category detection (ACD), and sentiment classification, often serving as a reference format similar to SemEval-2014. It supports both supervised and unsupervised methods, particularly in the context of restaurant reviews, and has been used to assess the performance of CNN and RNN models in identifying sentiment polarity of aspect categories.; The Czech corpus of 11,000 manually annotated NEs is primarily used to train and evaluate named entity recognition (NER) models for the Czech language. Researchers focus on improving the accuracy and coverage of entity annotations, using the dataset to develop and evaluate NER tools and systems. The corpus's fine-grained annotation and large, manually labeled dataset enable detailed assessments of entity recognition performance in Czech text.	Czech dataset (Hercig et al., 2016)
Czech Corpus	citing_context	Czech ABSA dataset	https://www.semanticscholar.org/paper/0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e (2024)	https://doi.org/10.3115/v1/S14-2004 (2014)	The Czech ABSA dataset is used to evaluate aspect term extraction (ATE), aspect category detection (ACD), and sentiment classification tasks in the Czech language, particularly for restaurant reviews. It provides a comparable format to SemEval-2014, serving as a reference for creating and annotating Czech datasets. Research focuses on the annotation format's limitations, such as the lack of linking aspect terms and categories, and its effectiveness in conducting aspect-based sentiment analysis.	
Czech Corpus	citing_context	Czech aspect-based sentiment dataset	https://doi.org/10.48550/arXiv.2204.13915 (2022)	https://doi.org/10.3115/v1/S14-2004 (2014)	The Czech aspect-based sentiment dataset is used to evaluate aspect-based sentiment analysis specifically in the Czech language. Researchers focus on comparing the performance of models using the same domain and label set as those used in English datasets. This allows for a direct comparison between languages, ensuring that the methodologies and evaluation metrics are consistent. The dataset's relevance lies in its ability to provide insights into the nuances of sentiment analysis in the Czech language, facilitating cross-lingual research and model development.	
Czech Corpus	cited_context	Czech Aspect-Based Sentiment Analysis	https://doi.org/10.13053/cys-20-3-2469 (2016)	https://www.semanticscholar.org/paper/f56e63471a1773f1068e3c962b2cbf29d2f28608 (2015)	The Czech Aspect-Based Sentiment Analysis dataset is used to conduct sentiment analysis on Czech IT product reviews, specifically focusing on aspect-based evaluation. Researchers employ this dataset to analyze sentiments related to various aspects of products, such as performance or design, and report preliminary results. This dataset enables detailed, aspect-focused sentiment analysis in the Czech language, providing insights into consumer opinions and preferences.	
Czech Corpus	cited_context	Czech Cess	https://www.semanticscholar.org/paper/90b4b1b6e5ef410dd2f285734789422d79bb2974 (2008)	https://doi.org/10.1007/S11168-004-7429-X (2004)	The Czech Cess dataset is used to build AnCora incrementally, primarily from newspaper and newswire articles. It focuses on linguistic annotation and corpus development, enabling researchers to enhance the quality and scope of annotated linguistic resources. This dataset supports the creation of detailed linguistic annotations, which are crucial for developing and improving natural language processing tools and systems.	
Czech Corpus	citing_context	Czech CMC4	https://doi.org/10.48550/arXiv.2404.18534 (2024)	https://doi.org/10.1201/b18055-8 (2006)	The Czech CMC4 dataset is used to train and evaluate models on Czech language tasks, addressing specific linguistic challenges. Researchers focus on performance metrics to assess model effectiveness, leveraging the dataset's relevance to Czech linguistic nuances. This enables detailed analysis and improvement of model performance in Czech language processing tasks.	
Czech Corpus	cited_context	Czech CNEC	https://doi.org/10.18653/v1/P19-1527 (2019)	https://doi.org/10.1093/BIOINFORMATICS/BTG1023 (2003)	The Czech CNEC dataset is used to evaluate nested Named Entity Recognition (NER) models, specifically focusing on their performance with Czech language nested entities. Researchers compare the proposed models against state-of-the-art methods to assess improvements in recognizing complex, hierarchical named entities within the Czech language. This dataset enables detailed performance evaluations and advancements in NER for Czech.	
Czech Corpus	citing_context	Czech Court Decisions	https://doi.org/10.48550/arXiv.2412.17933 (2024)	https://www.semanticscholar.org/paper/3d8401b052a507a785d7dd53b0ef8c57d303c919 (2014)	The Czech Court Decisions dataset is used to train and evaluate named entity recognition (NER) models, specifically focusing on identifying legal entities in Czech court decisions. Researchers employ the OAF annotation scheme to enhance model accuracy. This dataset enables the development of NER models tailored to the legal domain, addressing the need for precise entity recognition in complex legal texts.	
Czech Corpus	citing_context	Czech Court Decisions Corpus (CzCDC)	https://www.semanticscholar.org/paper/272647d6865ea48b430d9a2ee50f6138a3f63986 (2020)	https://www.semanticscholar.org/paper/5f9e0986f88666c1448a82bbaae8c086bce9d540 (2019)	The Czech Court Decisions Corpus (CzCDC) is used to compile and analyze court decisions from the Czech Supreme Court, Supreme Administrative Court, and Constitutional Court. It focuses on legal language, decision-making processes, and the availability and completeness of legal documents. The dataset supports research in legal text processing, including automatic segmentation of court decisions and recognition of court identifiers, enhancing the accessibility and utility of legal texts for analysis.	
Czech Corpus	cited_context | citing_context	Czech Facebook dataset (CFD)	https://doi.org/10.1007/978-3-030-83527-9_17 (2021)	https://www.semanticscholar.org/paper/142407d3cb61067e88d385f95ae238c74b19d554 (2013)	The Czech Facebook dataset (CFD) is used to evaluate sentiment analysis on Czech Facebook posts, specifically focusing on the accuracy of sentiment classification algorithms. Researchers employ this dataset to assess and improve the performance of these algorithms, leveraging the dataset's relevance to social media content in the Czech language.; The Czech Facebook dataset (CFD) is used to evaluate sentiment analysis models, focusing on their performance with social media data in the Czech language. Researchers employ this dataset to assess model accuracy and effectiveness in understanding nuanced sentiments expressed on platforms like Facebook. This dataset enables the development and refinement of natural language processing techniques tailored to the Czech language, enhancing the reliability of sentiment analysis in social media contexts.	
Czech Corpus	cited_context	Czech named entity corpus (CNEC)	https://www.semanticscholar.org/paper/325955a4a078988450c73a6581d25f305dd133cc (2020)	https://doi.org/10.3115/1699705.1699748 (2009)	The Czech named entity corpus (CNEC) is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset in the provided contexts. Therefore, based on the available evidence, the actual use of CNEC in research cannot be accurately described beyond its mere mention.	
Czech Corpus	cited_context | citing_context	Czech Named Entity Corpus	https://doi.org/10.26615/978-954-452-072-4_119 (2021), https://doi.org/10.3115/v1/P14-5003 (2014)	https://www.semanticscholar.org/paper/0a1cf8dbb859c13cbdb40788d7e69060155f9d77 (2014), https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	The Czech Named Entity Corpus is used for named entity recognition tasks, providing richly annotated data for entities in the Czech language. Researchers employ this dataset to train and evaluate models that identify and classify named entities, enhancing the accuracy of natural language processing systems specifically for Czech. This corpus enables detailed analysis and improvement of entity recognition algorithms, contributing to advancements in Czech language technology.; The Czech Named Entity Corpus is used for evaluating named entity recognition systems, particularly by allowing ambiguous labels in the annotation process. It supports both coarse and fine-grained classification tasks, enabling researchers to assess system performance in identifying and categorizing named entities with varying levels of specificity. This dataset facilitates the development and refinement of NER models tailored to handle ambiguity and complexity in entity recognition.	
Czech Corpus	cited_context	Czech Named Entity Corpus 1.1	https://doi.org/10.5220/0006595904860492 (2018)	https://doi.org/10.3233/SW-140134 (2015)	The Czech Named Entity Corpus 1.1 is utilized to enhance named entity recognition (NER) processes, specifically for identifying and contextualizing entities in Czech text. It is used to train and evaluate NER models, improving the accuracy and contextual understanding of recognized entities. This dataset supports research in natural language processing (NLP), particularly in web-derived text, aiding in tasks such as entity linking and natural language understanding and generation.	
Czech Corpus	citing_context	Czech Named Entity Recognizer	https://doi.org/10.18653/v1/2023.bsnlp-1.1 (2023)	https://doi.org/10.1007/978-3-642-40585-3_20 (2013)	The Czech Named Entity Recognizer dataset is used to evaluate the RobeCzech language model's performance on Czech named entity recognition tasks. It serves as a benchmark to assess model accuracy and effectiveness in identifying named entities within the Czech language. This dataset enables researchers to compare and improve language models specifically for Czech language applications.	
Czech Corpus	citing_context	Czech national corpus	https://doi.org/10.48550/arXiv.2212.08550 (2022), https://doi.org/10.18653/v1/2023.wassa-1.10 (2023)	https://doi.org/10.18653/v1/N19-1423 (2019)	The Czech national corpus is used as a large-scale training resource for developing deep bidirectional transformers, specifically the RoBERTa model (RobeCzech) and the Czert model. These models employ BERT-like architectures to enhance Czech language understanding and representation. The dataset's extensive size and linguistic diversity enable robust training, facilitating advanced language processing tasks and improving model performance in Czech language applications.	
Czech Corpus	citing_context	Czech OOV In-flection Dataset	https://doi.org/10.48550/arXiv.2404.08974 (2024)		The Czech OOV In-flection Dataset is used to address limitations in covering neologisms and out-of-vocabulary (OOV) inflections in Czech language morphology. It serves as a morphological dictionary with high precision but low recall, providing inflections and identifying lemmas not present in the dataset. Researchers use it to construct lemma-disjoint train-dev-test splits, ensuring minimal bias and serving as a benchmark for evaluating OOV inflection models.	
Czech Corpus	citing_context	Czech part of the MALACH project	https://doi.org/10.1007/978-3-031-16270-1_25 (2022)		The Czech part of the MALACH project dataset is used to provide linguistic data for the Czech language, focusing on research and development in natural language processing and computational linguistics. It supports the creation and improvement of algorithms and models that process and analyze Czech text, enhancing the accuracy and efficiency of computational linguistic tools.	
Czech Corpus	citing_context	Czech version of Rey Auditory Verbal Learning Test	https://doi.org/10.1044/2016_JSLHR-L-15-0301 (2016)	https://doi.org/10.1080/13825585.2013.865699 (2014)	The Czech version of the Rey Auditory Verbal Learning Test is used to establish normative data for memory performance and verbal learning patterns in the Czech population. It assesses immediate and delayed recall, as well as the sum of trials 1 to 5, providing standardized psychological assessment data for Czech participants. This dataset enables researchers to compare individual memory performance against established norms, facilitating the identification of potential memory impairments or deviations from typical cognitive functioning.	
Czech Corpus	cited_context	Czech Web 2014	https://doi.org/10.5220/0006595904860492 (2018)		The Czech Web 2014 dataset is used to train and evaluate models for Czech language processing, specifically focusing on web-derived text data. This dataset enables researchers to improve natural language understanding and generation tasks by providing a rich source of real-world textual content, enhancing the performance and accuracy of language models in processing web-based Czech language data.	
Czech Corpus	citing_context	Czech Web Corpus	https://www.semanticscholar.org/paper/a1dfc352e54394381fa239e7e02b2b246b3e0d01 (2016)	https://www.semanticscholar.org/paper/fcc7e0c241c456997a1f2452b77e1b20397ec305 (2013)	The Czech Web Corpus is used to create a specialized dataset of 10 million Czech sentences, focusing on verbs in the past tense. This dataset is divided into training, validation, and test sets, enabling researchers to evaluate and develop models specifically for handling past tense verbs in the Czech language. The corpus's large size and structured format facilitate robust model training and performance assessment.	
Czech Corpus	cited_context	Czech Wikipedia	https://www.semanticscholar.org/paper/325955a4a078988450c73a6581d25f305dd133cc (2020)	https://www.semanticscholar.org/paper/8e7be72c11949fffff2c01b5d4eb19b67ddad634 (2017)	The Czech Wikipedia dataset is used as a training corpus for the SentencePiece tokenizer, specifically to develop subword tokenization techniques for the Czech language. This methodology focuses on breaking down words into smaller units to improve natural language processing tasks, enhancing the tokenizer's performance in handling morphologically rich languages like Czech.	
Czech Corpus	citing_context	Czech WordNet	https://doi.org/10.26615/978-954-452-056-4_010 (2019)	https://www.semanticscholar.org/paper/58e00cde75cf11589ca53e7d2e62179a80cad19a (2010)	The Czech WordNet dataset is used to project Senti-WordNet scores into the Czech language, enabling the creation of a sentiment lexicon. This lexicon is specifically applied for sentiment analysis and opinion mining in Czech, facilitating the analysis of textual data to identify and extract subjective information.	
Czech Corpus	cited_context	CzEng 0	https://doi.org/10.1007/978-3-642-14770-8_33 (2010)	https://www.semanticscholar.org/paper/9ddf0087d0d2f115d1ce7e550dfa8729bd7a2aeb (2008)	CzEng 0 is used to construct a large, automatically annotated parallel English-Czech treebank, specifically focusing on aligning tectogrammatical structures of parallel sentences. This dataset enables researchers to develop and evaluate methods for deep syntactic alignment, enhancing cross-lingual natural language processing tasks.	
Czech Corpus	cited_context	CzEng-v2	https://www.semanticscholar.org/paper/8d188daf721fde8de4877718e96f89ae9d7a1925 (2022)	https://www.semanticscholar.org/paper/800366078f063a637e6a4880c0c49c217c7905ea (2016)	The CzEng-v2 dataset is primarily used for parallel data selection and machine translation tasks, focusing on Czech-English translations. It enhances translation accuracy in various contexts, including European Parliament proceedings, United Nations documents, news commentary, and multilingual Wikipedia articles. The dataset supports the training and evaluation of machine translation models, particularly by providing parallel sentence pairs. Its use in diverse text types improves cross-lingual understanding and translation quality in official, journalistic, and general contexts.	
Czech Corpus	cited_context | citing_context	CzeSL-GEC	https://doi.org/10.18653/v1/D19-5545 (2019)		The CzeSL-GEC dataset is used for grammatical error correction in Czech, specifically to train and evaluate models on correcting errors in learner texts. It provides annotated sentences in M2 format, enabling researchers to compare the quantity and structure of edits, facilitating advancements in natural language processing tasks.; The CzeSL-GEC dataset is used for grammatical error correction in Czech, providing annotated data to train and evaluate models on correcting errors in learner texts. It is also utilized to compare the structure and size of Czech language datasets, focusing on annotated edits in M2 format and the number of sentences. This dataset enables researchers to develop and benchmark error correction models and analyze dataset characteristics.	
Czech Corpus	citing_context	CzeSLSGT	https://www.semanticscholar.org/paper/8b394737caf81a6de812182d2715469db1110384 (2015)	https://www.semanticscholar.org/paper/06e75c3bf89fa3c6fc78f279e0ed0977b9e03b79 (2012)	The CzeSLSGT dataset is used to study and correct linguistic errors in Czech text, including spelling and grammar, through the evaluation and training of language models like Korektor. It focuses on contextual spell-checking and diacritics completion, particularly using the top 3000 tokens and a 111 million words corpus of primarily news articles. The dataset enhances the reliability of error detection and correction in both native and non-native Czech texts, with applications in audio-based input and model performance evaluation.	
Czech Corpus	citing_context	DareCzech	https://doi.org/10.48550/arXiv.2411.12921 (2024)	https://doi.org/10.1609/aaai.v36i11.21502 (2021)	The DareCzech dataset is used to test and evaluate Information Retrieval (IR) models on Czech documents and queries. Researchers focus on assessing the retrieval and ranking performance of these models, employing the dataset to measure how effectively they can identify and rank relevant documents in response to specific queries. This enables the development and refinement of IR systems tailored to the Czech language.	
Czech Corpus	citing_context	D cs → en	https://doi.org/10.18653/v1/2021.emnlp-main.742 (2022)	https://doi.org/10.18653/v1/2020.acl-main.703 (2019)	The 'D cs → en' dataset is used to fine-tune mBART and mBART-50 models for Czech-to-English translation, employing supervised learning techniques. This dataset focuses on enhancing cross-lingual performance, enabling researchers to improve the accuracy and fluency of translations between these languages. The dataset's specific characteristics and its role in supervised learning are crucial for advancing machine translation systems.	
Czech Corpus	cited_context | citing_context	Demagog dataset	https://doi.org/10.1007/s10579-023-09654-3 (2022), https://doi.org/10.1007/s00521-024-10113-5 (2023)	https://doi.org/10.26615/978-954-452-056-4_113 (2019)	The Demagog dataset is used to train machine learning models for fact-checking, specifically focusing on the accuracy and reliability of political statements in the Czech locale. It also supports the analysis of claims and their veracity in West Slavic languages, particularly Czech, using human-annotated rationales and metadata for statistical analysis. This dataset enables researchers to develop and evaluate algorithms that can automatically verify the truthfulness of statements, enhancing the reliability of information in political discourse.; The Demagog dataset is used to analyze fact-checking in West Slavic languages, particularly focusing on claims, veracity verdicts, and human-annotated rationales. It supports statistical analysis of political affiliations and trains machine learning models to enhance the accuracy and reliability of fact-checking political statements in the Czech locale.	
Czech Corpus	cited_context	DeriNet	https://doi.org/10.18653/v1/W19-4818 (2019)	https://doi.org/10.1007/978-3-319-45510-5_27 (2016)	DeriNet is used primarily to train word vectors and word embeddings, focusing on the Czech language and Czech-English parallel corpora. It is employed to compare normalized and non-normalized versions of word vectors, assess the impact of normalization, and evaluate the quality of word embeddings in both monolingual and bilingual contexts. The dataset supports research on derivational relations and morphological connections in Czech, enhancing translation quality and language modeling through supervised learning and NMT models.	
Czech Corpus	cited_context	ELRC EU Acts	https://www.semanticscholar.org/paper/8d188daf721fde8de4877718e96f89ae9d7a1925 (2022)	https://www.semanticscholar.org/paper/09cd7876b72d6105c83db59052572433a0a2b36c (2012)	The ELRC EU Acts dataset is primarily used for training and evaluating machine translation models, particularly focusing on Czech-English parallel sentences and other multilingual pairs. It provides parallel texts of European Union legal acts and transcribed talks, enabling large-scale machine translation research and legal document translation. The dataset's multilingual nature supports the development of robust translation models across various languages, including Czech.	
Czech Corpus	cited_context	EnFEVER	https://doi.org/10.1007/s00521-024-10113-5 (2023)		The EnFEVER dataset is used to evaluate fever detection and fact-checking models, particularly in the Czech and English languages, focusing on the leading sections of Wikipedia pages. It supports comparative analysis across multiple language versions (Czech, English, Polish, Slovak) by providing full content, enabling researchers to assess model performance and cross-linguistic consistency in claim verification.	
Czech Corpus	citing_context	Europarl corpus	https://doi.org/10.1007/978-94-024-0881-2_17 (2020), https://www.semanticscholar.org/paper/31417959f27eaf572e6413fe0a3c4e5213638238 (2024)	https://www.semanticscholar.org/paper/694b3c58712deefb59502847ba1b52b192c413e5 (2005)	The Europarl corpus is used as a parallel corpus for statistical machine translation, facilitating multilingual text alignment and enhancing translation quality. It provides prepared speeches from the European Parliament, which are utilized in multilingual translation studies, particularly for the Europarl genre items in Disco-GeM 1.0. This dataset supports research in improving translation accuracy and linguistic consistency across multiple languages.	
Czech Corpus	citing_context	Facebook posts	https://doi.org/10.26615/978-954-452-072-4_149 (2021)	https://www.semanticscholar.org/paper/4d16a47fb6708704b155855045c9e5d2ea380bb0 (2013)	The Facebook posts dataset is used to evaluate sentiment classification models, specifically focusing on opinions about movies and broader social media content in Czech. It provides structured and real-world user-generated text, enabling researchers to analyze positive and negative sentiments, public opinion, and emotional tones in review and social contexts.	
Czech Corpus	citing_context	dataset from Hercig et al. (2016)	https://www.semanticscholar.org/paper/0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e (2024)	https://www.semanticscholar.org/paper/7f65c8fe3bd8e44613e10b55d04db55244fce131 (2016)	The dataset from Hercig et al. (2016) is used to evaluate the performance of CNN and RNN models in identifying sentiment polarity of aspect categories in Czech. It focuses on assessing the effectiveness of these models in sentiment analysis tasks, providing a benchmark for comparing different machine learning approaches. The dataset's relevance lies in its application to Czech language sentiment analysis, enabling researchers to refine and test their models on a specific linguistic context.	
Czech Corpus	cited_context	general-domain Czech acoustic data	https://doi.org/10.3115/v1/W14-4311 (2014)	https://www.semanticscholar.org/paper/3a1a2cff2b70fb84a7ca7d97f8adcc5855851795 (2011)	The general-domain Czech acoustic data is used to train the Kaldi ASR (Automatic Speech Recognition) engine, specifically for enhancing acoustic modeling in the Czech language. This dataset focuses on improving recognition accuracy with in-domain call data and city/stop lists, enabling more effective speech-to-text transcription in practical scenarios.	
Czech Corpus	citing_context	Gur65	https://doi.org/10.26615/978-954-452-049-6_053 (2017)	https://doi.org/10.1007/11562214_67 (2005)	The Gur65 dataset is used to translate 65 pairs from the Rubenstein-Goodenough English dataset into Czech, focusing on semantic relatedness. This translation facilitates the study of semantic relationships in the Czech language, enabling researchers to evaluate and compare semantic similarity metrics in Czech. The dataset's specific focus on these 65 pairs allows for precise and targeted analysis in linguistic and computational linguistics research.	
Czech Corpus	citing_context	HistoryIR	https://doi.org/10.48550/arXiv.2412.17933 (2024)	https://www.semanticscholar.org/paper/4f4a409f701f7552d45c46a5b0fea69dca6f8e84 (2021)	The HistoryIR dataset is used to train and evaluate search engines specifically for historical periodicals. Native speaker annotators focus on query formulation and result labeling, enhancing the accuracy and relevance of search results. This dataset enables researchers to improve information retrieval systems tailored for historical documents, addressing challenges in query design and result assessment.	
Czech Corpus	cited_context	JRC-Names	https://doi.org/10.18653/v1/W19-3709 (2019)	https://doi.org/10.3233/SW-160228 (2016)	The JRC-Names dataset is used to enhance systems' capabilities in handling multilingual named entities by exploiting multilingual entity name variants and titles as linked data. This approach improves the system's ability to manage and integrate diverse linguistic data, facilitating more robust and versatile natural language processing applications.	
Czech Corpus	citing_context	Mallcz	https://doi.org/10.26615/978-954-452-072-4_128 (2021)	https://doi.org/10.1016/j.neunet.2005.06.042 (2005)	The Mallcz dataset is primarily used for training and evaluating bidirectional LSTM models, specifically for phoneme classification tasks in the Czech language. It employs 128 units in the BiLSTM layers and a batch size of 128. This dataset enables researchers to improve the accuracy of phoneme recognition, contributing to advancements in speech processing and natural language understanding.	
Czech Corpus	citing_context	mC4	https://doi.org/10.5220/0013374100003890 (2025)	https://doi.org/10.1162/tacl_a_00343 (2020)	The mC4 dataset is used to train multilingual models, including those for Czech, by leveraging a large-scale multilingual web text corpus. This dataset enables researchers to develop and refine models that can process and generate text in multiple languages, enhancing their performance and applicability in diverse linguistic contexts.	
Czech Corpus	cited_context	MLQA	https://doi.org/10.18653/v1/2024.acl-long.44 (2023)	https://doi.org/10.18653/v1/2020.acl-main.421 (2019)	The MLQA dataset is used to evaluate the cross-lingual transfer performance of monolingual models, particularly their ability to generalize across languages, including Czech. It assesses the effectiveness of these models in handling questions and contexts in multiple languages, focusing on cross-lingual question answering tasks. This dataset enables researchers to test and compare the robustness and adaptability of monolingual models in multilingual settings.	
Czech Corpus	cited_context | citing_context	Multi30K	https://doi.org/10.1007/s10590-021-09259-z (2021), https://doi.org/10.48550/arXiv.2212.10140 (2022), https://doi.org/10.18653/v1/W18-6402 (2018)	https://doi.org/10.18653/v1/W17-4718 (2017), https://doi.org/10.18653/v1/W16-2346 (2016)	The Multi30K dataset is used in WMT shared tasks for multimodal machine translation and multilingual image description, particularly for translating English image descriptions into Czech. It supports research in these areas by providing human-translated data, enabling the development and evaluation of models that can generate accurate translations and descriptions across languages and modalities.; The Multi30K dataset is used to evaluate multimodal machine translation models, focusing on the alignment of images with descriptions in multiple languages, including English, French, German, and Czech. It facilitates cross-lingual translation tasks by comparing model performance across these languages, enabling researchers to assess the effectiveness of multimodal approaches in generating accurate translations.	
Czech Corpus	citing_context	Multi30k2	https://doi.org/10.18653/v1/2020.findings-emnlp.376 (2020)	https://doi.org/10.18653/v1/W17-4718 (2017)	The Multi30k2 dataset is used to conduct experiments on multilingual machine translation, focusing on parallel sentences in English, French, Czech, and German. It enables researchers to evaluate and improve translation models across these languages, addressing specific research questions related to multilingual translation accuracy and performance. The dataset's parallel sentence structure is crucial for training and testing these models.	
Czech Corpus	citing_context	multilingual corpus	https://doi.org/10.48550/arXiv.2204.13915 (2022)	https://doi.org/10.18653/v1/W19-3709 (2019)	The multilingual corpus is used for named entity recognition (NER) in Slavic languages, including Czech, to standardize labels across four languages. This dataset facilitates the development and evaluation of NER models by providing a consistent labeling scheme, enhancing cross-lingual comparability and model performance.	
Czech Corpus	citing_context	NER 9k	https://doi.org/10.48550/arXiv.2304.01922 (2023)	https://doi.org/10.1016/j.procs.2021.06.075 (2020)	The NER 9k dataset is primarily used for Named Entity Recognition (NER) and natural language inference tasks, particularly in Czech and across multiple languages. It contains 9,000 to 10,000 annotated instances, which are transformed into a sequence-to-sequence format for cross-lingual evaluation. The dataset is also utilized for sentiment classification and emotion detection in texts, with specific applications in social media analysis and evaluating model performance.	
Czech Corpus	cited_context | citing_context	Náplava (2017) Czech dataset	https://doi.org/10.18653/v1/D19-5545 (2019)	https://www.semanticscholar.org/paper/f0284b6a449126d69638e7ae7bd976b62e2f22a1 (2014)	The Náplava (2017) Czech dataset is used to develop grammatical error correction systems for Czech, specifically focusing on annotated learner corpora. This dataset enables researchers to improve language learning and error detection methodologies by providing annotated data that enhances the accuracy and effectiveness of grammatical error correction tools.; The Náplava (2017) Czech dataset is used to develop and evaluate grammatical error correction systems for Czech, focusing on annotated learner corpora. This dataset supports research in improving language learning by enhancing error detection methodologies and correcting grammatical errors in learner-generated texts. The annotated nature of the dataset facilitates the training and testing of these systems, enabling more accurate and effective language learning tools.	
Czech Corpus	citing_context	corpus of 32 historical Czech newspapers from 1872	https://doi.org/10.48550/arXiv.2305.16718 (2023)	https://www.semanticscholar.org/paper/14744eeb7861340d7fbed456a9a0e67e8161b320 (2020)	The dataset of 32 historical Czech newspapers from 1872 is used to create a historical Czech language dataset, focusing on OCR and expert annotation. This methodology improves text recognition and linguistic analysis in historical documents, enabling researchers to enhance the accuracy and reliability of digital transcriptions and linguistic studies of historical texts.	
Czech Corpus	citing_context	dataset of Truong et al. (2022)	https://doi.org/10.48550/arXiv.2503.22395 (2025)	https://doi.org/10.48550/arXiv.2210.03256 (2022)	The dataset of Truong et al. (2022) is used to evaluate sub-clausal negation in the Czech language, focusing on the analysis of negation expressions. It employs methodologies that specifically examine linguistic aspects of negation, enabling detailed research into how negation is structured and used within the Czech language. This dataset facilitates precise evaluation and analysis of negation patterns, contributing to a deeper understanding of Czech linguistic structures.	
Czech Corpus	cited_context	OpenSubtitles	https://doi.org/10.18653/v1/W18-6402 (2018)	https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The OpenSubtitles dataset is used as a multilingual, text-only resource, primarily for training and evaluating language models and machine translation systems. It leverages movie and TV subtitles to provide a rich, diverse corpus of textual data, enabling researchers to enhance model performance in language understanding and translation tasks.	
Czech Corpus	citing_context	OpenSubtitles 4	https://www.semanticscholar.org/paper/153d03e7a8bf64b389ddecff2cc6437f22a084d2 (2019)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The OpenSubtitles 4 dataset is used to select source sentences for Czech data, focusing on parallel data and subtitles for linguistic and translation studies. It enables researchers to conduct translation and linguistic analysis by providing a rich corpus of parallel sentences, enhancing the accuracy and depth of their studies.	
Czech Corpus	citing_context	OPUS corpora	https://doi.org/10.48550/arXiv.2506.04929 (2025)	https://www.semanticscholar.org/paper/182659cac24a2d9f0e02857c529d368830bb6026 (2004)	The OPUS corpora dataset is used to train text-to-text models, leveraging 53 million sentence pairs primarily for parallel translation tasks. This dataset enables researchers to develop and refine machine translation systems by providing extensive bilingual data, enhancing the accuracy and fluency of translations across languages.	
Czech Corpus	citing_context	Pang and Lee, 2004	https://doi.org/10.48550/arXiv.2204.13915 (2022)	https://doi.org/10.3115/1218955.1218990 (2004)	The Pang and Lee (2004) dataset is used as a benchmark for zero-shot cross-lingual subjectivity classification, where it tests the performance of pre-trained multilingual models against an established English dataset. This usage focuses on evaluating model transferability and effectiveness in classifying subjective content across languages without additional training data.	
Czech Corpus	cited_context	PDT training set	https://doi.org/10.3115/1220575.1220641 (2005)		The PDT training set is primarily used for linguistic analysis and processing of Czech language data, focusing on non-projective dependencies and flexible word order. It is utilized to train models for dependency parsing, extracting syntactic features, and performing experiments on syntactic structures and linguistic annotations. The dataset's 13,450,672 features enhance model performance in parsing tasks, making it valuable for both syntactic and semantic studies.	
Czech Corpus	citing_context	poleval2018	https://doi.org/10.18653/v1/2023.bsnlp-1.20 (2023)	https://www.semanticscholar.org/paper/3aa64238d641b01950fb2feae20c2be6d3e22a20 (2012)	The 'poleval2018' dataset is used to train and evaluate models for Czech Named Entity Recognition (NER). It is labeled with entity types such as person, location, and organization, similar to the NKJP corpus. This dataset enables researchers to develop and assess NER models specifically tailored for the Czech language, focusing on the accuracy and performance of entity identification in text.	
Czech Corpus	cited_context	Prague Czech English Dependency Treebank	https://doi.org/10.1007/978-3-642-14770-8_33 (2010)	https://doi.org/10.2478/v10108-009-0026-2 (2009)	The Prague Czech English Dependency Treebank is used for complex pre-annotation of English tectogrammatical trees, facilitating rich syntactic and semantic analysis. It is also employed for tagging and syntactic annotation of Czech data, supporting evaluation in shared NLP tasks. This dataset enables detailed linguistic analysis and enhances the accuracy of syntactic annotations in both English and Czech contexts.	
Czech Corpus	cited_context | citing_context	Prague Dependency Treebank	https://doi.org/10.48550/arXiv.2506.18091 (2025), https://www.semanticscholar.org/paper/4d16a47fb6708704b155855045c9e5d2ea380bb0 (2013), https://doi.org/10.1007/978-3-030-83527-9_17 (2021), https://www.semanticscholar.org/paper/90b4b1b6e5ef410dd2f285734789422d79bb2974 (2008), https://www.semanticscholar.org/paper/8bf9bd7f81446236b8a8b4d67ffba3b9ce61ec25 (2006), https://doi.org/10.1007/978-3-642-14770-8_33 (2010)	https://www.semanticscholar.org/paper/d8b2f910368c643f2764168e7fd161b644913726 (2013), https://www.semanticscholar.org/paper/79f51a3344abdcc9d019e9187c2e3511061f6f03 (1999)	The Prague Dependency Treebank is used in Czech language research to evaluate methods for anaphora resolution, focusing on linguistic dependencies and syntactic structures. It provides a lemmatizer for morphological analysis and supports the creation of tectogrammatical graphs by modifying tree structures to include co-reference. These applications leverage the dataset's detailed syntactic annotations to enhance natural language processing tasks.; The Prague Dependency Treebank is used extensively in Czech language research, primarily for morphological and syntactic tagging, dependency parsing, and linguistic analysis. It contains 630K tokens and supports multiple layers of annotation, including deep syntactic structures and semantic roles. Researchers use it to train and evaluate NLP models, conduct morphological and syntactic experiments, and develop rule-based approaches for tasks like pronominal anaphora resolution. The dataset's rich annotations enable detailed studies of Czech language structures and their application in natural language processing.	
Czech Corpus	citing_context	Prague Discourse Treebank	https://www.semanticscholar.org/paper/31417959f27eaf572e6413fe0a3c4e5213638238 (2024)		The Prague Discourse Treebank is used to compare and analyze discourse annotation in Czech, focusing on the limitations of genre, topic, and size relative to other datasets. It provides discourse annotation for Czech language research, emphasizing syntactic and semantic structures in written texts. This dataset enables researchers to explore and highlight the specific linguistic annotations and structural characteristics of Czech discourse, facilitating comparative studies and improving understanding of discourse phenomena in the language.	
Czech Corpus	citing_context	Propaganda Datasets	https://doi.org/10.48550/arXiv.2412.17933 (2024)	https://doi.org/10.1016/j.eswa.2024.124085 (2024)	The Propaganda Datasets are used to recognize propaganda techniques in newspaper texts, serving as a resource for 13 distinct tasks within BenCzechMark. These tasks focus on the fusion of content and style analysis, enabling researchers to identify and understand various propaganda methods employed in textual content.	
Czech Corpus	citing_context	RotoWire dataset	https://doi.org/10.18653/v1/2020.inlg-1.13 (2020)	https://doi.org/10.18653/v1/D17-1239 (2017)	The RotoWire dataset is used to generate Czech language documents from structured data, addressing the challenges of translation and data-to-document generation in a small, translated dataset. This involves methodologies focused on natural language processing and machine translation, specifically tailored to handle the nuances of the Czech language. The dataset enables researchers to explore and improve automated document generation techniques in a less commonly studied language context.	
Czech Corpus	citing_context	SemEval-2015	https://www.semanticscholar.org/paper/0122f11e2f92c2eae8a0d8001b97c5410f4e6f2e (2024)	https://doi.org/10.18653/v1/S16-1002 (2016)	The SemEval-2015 dataset is used as a reference for annotation format in constructing a Czech dataset for Aspect-Based Sentiment Analysis, specifically in the restaurant domain. It provides a standardized structure that facilitates consistent annotation practices, enabling researchers to develop and evaluate sentiment analysis models tailored to Czech language contexts.	
Czech Corpus	citing_context	Shared Tasks on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages	https://doi.org/10.48550/arXiv.2404.00482 (2024)	https://doi.org/10.18653/v1/W19-3709 (2019)	The dataset is used to evaluate and compare baseline models against shared task results, specifically for multilingual named entity recognition and normalization in Slavic languages. It facilitates research by providing a standardized benchmark for assessing model performance across these linguistic tasks, enabling researchers to identify strengths and weaknesses in their approaches.	
Czech Corpus	citing_context	SIGMORPHON’s 2016 dataset	https://doi.org/10.48550/arXiv.2404.08974 (2024)	https://doi.org/10.18653/v1/W16-2010 (2016)	The SIGMORPHON 2016 dataset is used to compare the size of training datasets, particularly in morphological reinflection tasks. Researchers employ this dataset to evaluate network capacity, assessing how different sizes of training data impact model performance. This enables a deeper understanding of the efficiency and effectiveness of neural networks in handling morphological inflections.	
Czech Corpus	citing_context	Simple Question Answering Database (SQAD)	https://doi.org/10.5220/0008979206440651 (2020)	https://www.semanticscholar.org/paper/2898799d4a77ae114f198f19f763c853c8b39e05 (2014)	The Simple Question Answering Database (SQAD) is used to enhance and develop question answering systems specifically for the Czech language. Researchers employ the dataset to make modifications and improvements, focusing on simple questions and their answers. The dataset's updates and expansions enable more robust and accurate question answering capabilities in Czech, addressing the need for linguistically tailored systems.	
Czech Corpus	citing_context	Small-E-Czech	https://doi.org/10.18653/v1/2022.nlp4pi-1.10 (2022)	https://doi.org/10.1609/aaai.v36i11.21502 (2021)	The Small-E-Czech dataset is used to evaluate the performance of a Siamese BERT-based model in web search relevance ranking, specifically within the Czech language context. This involves assessing the model's ability to accurately rank search results. The dataset's focus on the Czech language provides a specialized resource for testing and improving natural language processing models in this linguistic domain.	
Czech Corpus	cited_context | citing_context	SQAD	https://doi.org/10.5220/0010827000003116 (2022), https://doi.org/10.48550/arXiv.2304.01922 (2023), https://doi.org/10.48550/arXiv.2412.17933 (2024), https://www.semanticscholar.org/paper/7f312663b0dadcef805be0fb55da6b188bc8255b (2017)	https://www.semanticscholar.org/paper/2898799d4a77ae114f198f19f763c853c8b39e05 (2014)	The SQAD dataset is primarily used to evaluate and develop question answering (QA) systems for the Czech language. It contains full-length Wikipedia articles, manually crafted questions, and associated answer texts, enabling researchers to assess factual knowledge extraction, reading comprehension, and information retrieval. Studies use it to benchmark performance metrics like mean average precision (MAP) and evaluate systems such as AQA and Czert, focusing on accuracy and effectiveness in handling over 13,000 QA pairs.; The dataset 'SQAD' is mentioned in research citations but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. There is no evidence to suggest it is used for Czech language research or any other specific application. Its role in enabling research is unclear based on the provided information.	
Czech Corpus	citing_context	SQAD database	https://doi.org/10.5220/0008979206440651 (2020)	https://www.semanticscholar.org/paper/9f5b37ef274da3e9a92d902664674ad98d59e0cc (2018)	The SQAD database is used to evaluate and improve answer selection in automatic question answering systems, focusing on enhancing the performance of recurrent networks. It contains over 13,000 question-answer pairs, which are utilized to develop and refine algorithms, leading to improved results in answer selection tasks.	
Czech Corpus	cited_context	Subtitles2018	https://doi.org/10.48550/arXiv.2212.10140 (2022)	https://doi.org/10.1016/j.protcy.2014.11.024 (2015)	The Subtitles2018 dataset is primarily used to build subject-aligned comparable corpora and mine parallel sentence pairs, leveraging multilingual subtitle data, book texts, and transcribed talks from sources like TED Talks. It facilitates the creation of large-scale multilingual corpora, enabling researchers to align subjects and extract parallel sentences across various languages. This dataset supports methodologies focused on corpus construction and parallel data extraction, enhancing research in natural language processing and multilingual text analysis.	
Czech Corpus	cited_context | citing_context	SumeCzech	https://doi.org/10.5220/0013374100003890 (2025), https://doi.org/10.48550/arXiv.2307.10666 (2023), https://doi.org/10.48550/arXiv.2412.17933 (2024), https://doi.org/10.18653/v1/2021.emnlp-main.742 (2022), https://doi.org/10.14712/00326585.012 (2021), https://www.semanticscholar.org/paper/bf128364371d2fd9c42451c435c0503aa00c4ca8 (2022)	https://www.semanticscholar.org/paper/6247dd5de4e69effc4816f8c788e0dbb416fdde5 (2018)	The SumeCzech dataset is primarily used for text summarization and news topic classification in the Czech language. It supports the development and evaluation of summarization models, particularly in generating concise summaries of news articles and evaluating methods like Named Entity Density. The dataset also enhances named entity recognition, enabling the creation of additional features for summarization tasks. Additionally, it is used for news topic classification, improving content organization and retrieval. The dataset includes a large collection of Czech news articles and has been utilized to fine-tune multilingual models, including mBART, for improved summarization accuracy and coherence. It also extends to analyzing linguistic and stylistic features in texts by Karel Čapek and presidential speeches, providing insights into literary and political discourse.; The SumeCzech dataset is primarily used for summarization tasks, specifically in the context of Czech news articles. It contains over one million articles and is employed to develop, train, and evaluate summarization models. Researchers use it to analyze word count statistics, achieve state-of-the-art performance, and compare model performance. The dataset's large scale and news-based content enable robust evaluation and advancement in natural language processing, particularly in automatic summarization.	
Czech Corpus	cited_context	Swiss Legislation Corpus (SLC)	https://www.semanticscholar.org/paper/5f9e0986f88666c1448a82bbaae8c086bce9d540 (2019)	https://doi.org/10.5167/UZH-54761 (2011)	The Swiss Legislation Corpus (SLC) is used to build corpora for the philological analysis of Czech legal texts, focusing on the linguistic and stylistic features of Swiss legal documents. Researchers employ the dataset to conduct detailed philological studies, examining the language and style specific to Swiss legislation. This enables in-depth analysis of legal terminology and writing conventions in a cross-linguistic context.	
Czech Corpus	citing_context	SYN v4	https://doi.org/10.1007/978-3-030-83527-9_17 (2021)	https://doi.org/10.3115/v1/P14-5003 (2014)	The SYN v4 dataset is primarily used to train RobeCzech, a language model for the Czech language. It includes a variety of sources such as Czech Wikipedia dumps, newspaper and magazine articles, and web corpus documents, totaling over 4 billion tokens. These sources provide encyclopedic, media, and web-specific language content, enabling the model to capture linguistic and contextual diversity. The dataset's large size and diverse content enhance the model's performance in understanding and generating Czech text.	
Czech Corpus	citing_context	test 2016 Flickr	https://doi.org/10.18653/v1/2020.findings-emnlp.376 (2020)	https://doi.org/10.3115/1073083.1073135 (2002)	The 'test 2016 Flickr' dataset is used to evaluate the quality and relevance of English-to-Czech translations, specifically in the context of image captioning. Researchers employ this dataset to assess how well automated systems can generate accurate and contextually appropriate captions for images, focusing on the effectiveness of translation models in cross-lingual tasks.	
Czech Corpus	citing_context	TinyStories	https://doi.org/10.5220/0013064500003837 (2024)	https://www.semanticscholar.org/paper/28085f480ce456a376ebace9b899e3bc93dbc048 (2023)	The TinyStories dataset is used to translate English stories into Czech, creating a parallel corpus. This involves leveraging advanced translation capabilities to study language translation and coherence in a Slavic language. The dataset enables researchers to focus on the nuances of translating coherent narratives, enhancing understanding of cross-linguistic structures and translation methodologies.	
Czech Corpus	cited_context | citing_context	UD Czech PDT treebank	https://doi.org/10.1007/978-3-030-83527-9_17 (2021)	https://www.semanticscholar.org/paper/57133ef4c4de4d54a57686b8a914b06e4ff4aab5 (2017)	The UD Czech PDT treebank is used to evaluate joint morphosyntactic analysis, specifically focusing on the accuracy of morphological and syntactic annotations in Czech. As part of the Universal Dependencies 2.3 release, this dataset enables researchers to assess and improve the performance of models in handling complex linguistic structures, ensuring robust and accurate parsing and tagging in Czech language processing tasks.; The 'UD Czech PDT treebank' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. Therefore, based on the provided evidence, it cannot be accurately described how this dataset is used in research.	
Czech Corpus	cited_context | citing_context	Wiki abstracts	https://doi.org/10.1007/s10579-023-09654-3 (2022)	https://doi.org/10.18653/v1/P17-1171 (2017)	The Wiki abstracts dataset is used to support claims in the CsFEVER dataset by providing verifiable evidence. Specifically, it supplements non-verifiable claims with the top result from a DrQA model. This approach enhances the reliability of claims in fact-checking research, leveraging the dataset's extensive coverage and structured format.; The Wiki abstracts dataset is used to support the CsFEVER dataset by providing evidence for claims. Specifically, it supplements non-verifiable claims with the top result from a DrQA model. This approach enhances the verifiability of claims in the CsFEVER dataset, leveraging the extensive and diverse content of Wikipedia abstracts.	
Czech Corpus	citing_context	WildGuardMix test data	https://doi.org/10.48550/arXiv.2504.04377 (2025)	https://doi.org/10.1016/S1053-8119(05)70011-2 (1923)	The WildGuardMix test data is used to evaluate the quality and consistency of NLLB and TowerInstruct translations by creating a stratified train-eval split in a 70:30 ratio. This dataset facilitates the assessment of translation models, ensuring that the evaluation is representative and robust. It is specifically designed to support research in machine translation, focusing on the performance and reliability of these models.	
Czech Corpus	cited_context	WMT11 manual evaluation	https://doi.org/10.1007/978-3-642-40585-3_59 (2013)	https://www.semanticscholar.org/paper/e0c7a5f5fc258a9a17c4ba20c6ac4d4063904d9a (2011)	The WMT11 manual evaluation dataset is primarily used to evaluate machine translation outputs, ensuring consistent assessment across different systems. It serves as a reference for comparing human post-edits to original machine-translated texts, aiding in the ranking of systems in the WMT11 competition. This dataset provides insights into human perception of translation quality, facilitating the identification of strengths and weaknesses in machine translation models.	
Czech Corpus	cited_context	WMT18	https://doi.org/10.1038/s41467-020-18073-9 (2020)	https://doi.org/10.18653/v1/W18-6401 (2018)	The WMT18 dataset is used to evaluate the quality of machine translations, particularly focusing on translations from English to Czech. It preserves 1-1 segment translations and aims for literal accuracy, making it suitable for assessing the performance of translation systems like those developed by Translated.net. This dataset enables researchers to benchmark and compare different machine translation models, ensuring they meet high standards of fidelity and fluency.	
Czech Corpus	citing_context	WMT 2019	https://doi.org/10.18653/v1/2020.inlg-1.13 (2020)	https://doi.org/10.3115/1073083.1073135 (2002)	The WMT 2019 dataset is mentioned in research citations but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling features in any particular research area.	
Czech Corpus	citing_context	X-Fact	https://www.semanticscholar.org/paper/b0135059c206073c4a876eaab21d3f6bc11c3a94 (2022)	https://doi.org/10.18653/v1/2021.acl-short.86 (2021)	The X-Fact dataset is used to benchmark multilingual fact-checking models, evaluating their performance across 25 languages and seven veracity classes. This dataset enables researchers to assess the effectiveness and robustness of these models in a multilingual context, providing insights into their capabilities and limitations in diverse linguistic environments.	
Czech Corpus	citing_context	XFUND	https://doi.org/10.48550/arXiv.2503.19658 (2025)	https://doi.org/10.1109/ICDARW.2019.10029 (2019)	The XFUND dataset is used for multilingual form understanding, addressing the challenges of script variation and key-value pair extraction across multiple languages. It enables researchers to develop and test methodologies that improve the accuracy of form understanding in diverse linguistic contexts, enhancing the robustness of natural language processing systems.	
Czech Corpus	citing_context	XNLI	https://doi.org/10.48550/arXiv.2304.01922 (2023), https://www.semanticscholar.org/paper/b0135059c206073c4a876eaab21d3f6bc11c3a94 (2022)	https://doi.org/10.18653/v1/D18-1269 (2018)	The XNLI dataset is primarily used for cross-lingual natural language inference, often transformed into a sequence-to-sequence format to enhance multilingual model training and evaluation. It contains 399,000 annotated instances and is utilized to evaluate cross-lingual sentence representations, fine-tune pretrained models like XLM-RoBERTa-large, and improve in-context learning performance, particularly in Czech and Russian. The dataset supports research on enhancing task performance through the inclusion of complementary QA datasets and is crucial for named entity recognition in Czech.	
Danish Corpus	cited_context | citing_context	AngryTweets	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://www.semanticscholar.org/paper/0e8b3e895fffee070a9748aba38c33db0000ed4f (2021)	The AngryTweets dataset is used for sentiment classification of Danish tweets, leveraging crowdsourced annotations to analyze public opinion and emotional content in social media. It also serves to illustrate the trade-off between differentiation and benchmarking speed, focusing on performance metrics in Danish NLP tasks. This dataset enables researchers to evaluate and improve the accuracy and efficiency of sentiment analysis models in the Danish language context.; The AngryTweets dataset is used in Danish NLP research to analyze sentiment classification of tweets, focusing on public opinion and emotional content through crowdsourced annotations. It also demonstrates the trade-off between differentiation and benchmarking speed, evaluating performance metrics in Danish NLP tasks. This dataset enables researchers to explore the nuances of emotional expression in social media and optimize NLP models for Danish language tasks.	
Danish Corpus	cited_context	CC100	https://www.semanticscholar.org/paper/7e555915106090b5e33acef49f94a41b678392c1 (2023)	https://doi.org/10.18653/v1/P19-1310 (2019)	The CC100 dataset is used as a large-scale, multilingual corpus representing Danish, Icelandic, Norwegian, and Swedish. It serves as a raw and cleaned-up web crawl resource, enabling researchers to train and evaluate multilingual models for language modeling and other NLP tasks. The dataset's extensive size and linguistic diversity support the development of robust multilingual systems.	
Danish Corpus	citing_context	CMP data	https://doi.org/10.1111/J.1467-9477.2008.00202.X (2008)	https://doi.org/10.1017/S0007123404000201 (2004)	The CMP data is used to analyze Danish political parties' reactions to public opinion and past election results, focusing on ideological changes and stability. Researchers employ this dataset to examine how parties adapt their positions over time, using methods that track shifts in party ideology. This enables studies on political dynamics and the responsiveness of parties to electoral outcomes.	
Danish Corpus	cited_context | citing_context	Common Crawl	https://doi.org/10.48550/arXiv.2303.17183 (2023), https://www.semanticscholar.org/paper/c637fae348e758278f6267d0e42c03519febbe6a (2020)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019), https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The Common Crawl dataset is used as a foundational source for constructing The Nordic Pile, contributing web-derived, multilingual text data. It provides a language-filtered version of C4, enhancing the dataset with Danish and other Nordic languages. This enables research focused on multilingual text processing and filtering, supporting the development of language models and natural language processing tasks tailored to Nordic languages.; The Common Crawl dataset is used to provide large corpora of Danish text for pretraining language models, encompassing diverse sources such as web text, subtitles, and structured content. This enables researchers to focus on various linguistic aspects, including conversational, colloquial, and encyclopedic language, enhancing the models' ability to handle naturally occurring and structured data.	
Danish Corpus	citing_context	CoNLL-2003	https://www.semanticscholar.org/paper/c637fae348e758278f6267d0e42c03519febbe6a (2020)	https://www.semanticscholar.org/paper/da4cf77ef8d1a0f56ef14ec71957ba7056419fdb (2013)	The CoNLL-2003 dataset is used to train and evaluate Danish Named Entity Recognition (NER) models, focusing on cross-lingual transfer and performance metrics. It enables researchers to assess the effectiveness of NER models in recognizing entities like names, organizations, and locations in Danish text, contributing to advancements in multilingual natural language processing.	
Danish Corpus	cited_context	CoNLL 2006	https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015)	https://doi.org/10.3115/1596276.1596305 (2006)	The CoNLL 2006 dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on its application, methodology, research questions, or specific characteristics. Therefore, based on the provided evidence, it cannot be accurately described beyond its mere mention in the literature.	
Danish Corpus	citing_context	contemporary Danish Gigaword Corpus	https://www.semanticscholar.org/paper/bd8b50b6faec00365bb53db970b4720abde1f35e (2024)	https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776 (2017)	The contemporary Danish Gigaword Corpus is primarily used to train Danish language models such as DanskBERT and ScandiBERT, focusing on contemporary and historical Danish texts. It enhances model performance in low-resource settings and across various contexts, including literary, legal, and social media texts. The corpus, containing over 7 billion words from diverse sources, supports improved language understanding and generation, particularly for Danish and other Scandinavian languages.	
Danish Corpus	cited_context	Copenhagen Dependency Treebank	https://www.semanticscholar.org/paper/c637fae348e758278f6267d0e42c03519febbe6a (2020), https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015)	https://www.semanticscholar.org/paper/1e84ba06ef34e4c93465437009baa0236f4cee64 (2003)	The Copenhagen Dependency Treebank is used in linguistic research and natural language processing tasks, particularly for dependency parsing of Danish text. It is employed to compare and contrast structural and content differences in Danish text, as well as to convert annotation formats, focusing on syntactic structures. This dataset supports the analysis and processing of Danish language data, enabling detailed linguistic studies and NLP applications.	
Danish Corpus	cited_context | citing_context	Copenhagen Dependency Treebank (CDT)	https://doi.org/10.3115/v1/E14-2016 (2014)	https://www.semanticscholar.org/paper/4299ec697947e91bf761daf6187e3b997846f424 (2010)	The Copenhagen Dependency Treebank (CDT) is used to provide syntactic and discourse annotations for Danish, enhancing linguistic research by building on and including previously released corpora. This dataset supports the analysis and understanding of Danish syntax and discourse structures, facilitating more comprehensive and nuanced linguistic studies.	
Danish Corpus	citing_context	COR register	https://www.semanticscholar.org/paper/f852bdbfde7e53510f677d715eda71df8bf1c45b (2022)	https://www.semanticscholar.org/paper/b56cc5cea3fea350a9cf7020760df16f37bd9127 (2022)	The COR register dataset is used to integrate a sentiment lexicon at the lemma level, linking to the COR-S sense inventory. This linkage addresses differences from the DDO sense inventory, enabling more precise sentiment analysis and lexical resource development in linguistic research.	
Danish Corpus	citing_context	DaN+	https://doi.org/10.18653/v1/2021.findings-acl.158 (2021)	https://www.semanticscholar.org/paper/0a1cf8dbb859c13cbdb40788d7e69060155f9d77 (2014)	The DaN+ dataset is primarily used for developing and refining named entity recognition (NER) annotation guidelines, particularly in Danish and German corpora. It serves as a foundational resource for training models to process Danish language data, with a focus on zero-shot transfer capabilities. The dataset's annotation guidelines are often referenced and adapted, enabling cross-lingual research and enhancing model performance when combined with German data.	
Danish Corpus	cited_context | citing_context	DaNE dataset	https://www.semanticscholar.org/paper/e9c0378ae58fb48c776605a855dd52abcaa9aa27 (2021), https://www.semanticscholar.org/paper/84b3e22fbdefa58452c5136659c12de2adf0e93f (2023), https://www.semanticscholar.org/paper/7e555915106090b5e33acef49f94a41b678392c1 (2023)	https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015), https://www.semanticscholar.org/paper/0322f23d07085e474254f1a40962a72ccfbd4b47 (2019)	The DaNE dataset is used to provide syntactic and named entity annotations for Danish text, supporting the development and evaluation of dependency parsing and named entity recognition models. It serves as a benchmark for Danish NLP tasks, enabling researchers to train and evaluate models according to the CoNLL-2003 schema. This dataset facilitates advancements in Danish language processing by offering annotated data for model training and performance assessment.; The DaNE dataset is primarily used for named entity recognition (NER) and dependency parsing in Danish text. It serves as a benchmark for Danish NLP tasks, providing annotated data essential for training and evaluating linguistic models. Researchers use it to enhance the accuracy of NER and syntactic analysis, making it a foundational resource for improving Danish language processing systems.	DaNE
Danish Corpus	citing_context	Danish test set	https://doi.org/10.18653/v1/2020.semeval-1.277 (2020), https://doi.org/10.48550/arXiv.2205.01381 (2022)	https://www.semanticscholar.org/paper/6fd1f59a0e8ad74fdd1fa0afb6cbb9dcc0ce22da (2019)	The Danish test set is used to detect offensive language and hate speech in Danish tweets, analyzing a dataset of 2961 tweets categorized into offensive and non-offensive content. It also evaluates model performance on Danish language tasks, comparing models like BERTbase, JobBERT, and RemBERT using human-corrected gold labels for high-quality assessment.	
Danish Corpus	cited_context	Danish Arboretum	https://www.semanticscholar.org/paper/1e84ba06ef34e4c93465437009baa0236f4cee64 (2003)	https://www.semanticscholar.org/paper/0a3713b0e86b282b1821e29688d3b28a7bfbc3e5 (2001)	The Danish Arboretum dataset is used to study filler dependencies in Danish syntax, focusing on aspects of linguistic structure where these dependencies are left unspecified. Researchers employ this dataset to analyze and understand the syntactic mechanisms and structures in Danish, particularly in contexts where filler gaps occur. This dataset enables detailed linguistic analysis by providing a rich corpus of Danish sentences with complex syntactic elements.	
Danish Corpus	cited_context	Danish CDI-study	https://doi.org/10.1017/S0305000908008714 (2008)	https://doi.org/10.1017/S0305000999004006 (2000)	The Danish CDI-study dataset is used to investigate the development of early vocabulary in Danish children. Researchers compare vocabulary size and composition within this dataset to data from other studies, employing a comparative analysis approach. This enables insights into how early vocabulary develops in Danish children relative to other populations, highlighting specific characteristics and trends in vocabulary acquisition.	
Danish Corpus	citing_context	Danish Dependency Treebank (DDT)	https://www.semanticscholar.org/paper/fb5c2ffcc9e7066a92bc3d91c0f89a34b57a8ee8 (2021)	https://www.semanticscholar.org/paper/1e84ba06ef34e4c93465437009baa0236f4cee64 (2003)	The Danish Dependency Treebank (DDT) is primarily used for dependency parsing and syntactic analysis in Danish, providing grammatically annotated data. It supports the evaluation and training of linguistic models such as DaLUKE, focusing on syntactic structures, grammatical relations, and coreference resolution. The dataset is also utilized to produce and refine NER annotations, enhancing natural language processing tasks in Danish. Its conversion to Universal Dependency format facilitates broader linguistic research.	
Danish Corpus	cited_context | citing_context	Danish FrameNet Lexicon	https://www.semanticscholar.org/paper/8cf914c2e1f2ecbcfcfb58a1b2d6a057a7b54265 (2024), https://www.semanticscholar.org/paper/b56cc5cea3fea350a9cf7020760df16f37bd9127 (2022)	https://www.semanticscholar.org/paper/410f9dc198fb317c1dc157f6697ae1c565e252d8 (2017)	The Danish FrameNet Lexicon is used to assign semantic frames to Danish verbs and de-verbal nouns, facilitating the extraction of specific verb types like change-of-state, communication, and mental verbs. It serves as a gold standard for linguistic analysis, particularly for frame semantics in Danish, providing a structured lexical resource that supports detailed linguistic research and analysis.; The Danish FrameNet lexicon is used to assign semantic frames to Danish verbs and deverbal nouns, enhancing lexical resources for natural language processing tasks. This dataset supports the development and improvement of NLP systems by providing structured semantic information, which is crucial for tasks requiring deep linguistic understanding.	Danish FrameNet lexicon
Danish Corpus	cited_context | citing_context	Danish Gigaword	https://www.semanticscholar.org/paper/4968f4809a197d67a75080bd43cb31c3f428b1d9 (2023), https://www.semanticscholar.org/paper/73cb8b7a02d1b9c9f3e5c21d25752cb4115fa83d (2020)	https://doi.org/10.18653/v1/2021.eacl-main.156 (2020)	The Danish Gigaword dataset is used for linguistic analysis, specifically to estimate meaningful tokens in Danish text. Researchers filter the dataset to enhance its relevance and quality, focusing on token frequency. This approach aids in understanding linguistic patterns and structures within Danish text, enabling detailed analyses of language use and frequency.; The Danish Gigaword dataset is used to create multiple random test sets with varying biases, adhering to best practices in dataset splitting for natural language processing tasks. This methodology ensures robust evaluation of NLP models by addressing potential biases in test data, enhancing the reliability and generalizability of research findings.	
Danish Corpus	cited_context | citing_context	Danish Gigaword Corpus	https://www.semanticscholar.org/paper/bd8b50b6faec00365bb53db970b4720abde1f35e (2024), https://www.semanticscholar.org/paper/e9c0378ae58fb48c776605a855dd52abcaa9aa27 (2021)	https://www.semanticscholar.org/paper/bbe6b4ec62ad36dba3eb376a805fffdfe5ec6387 (2022), https://www.semanticscholar.org/paper/73cb8b7a02d1b9c9f3e5c21d25752cb4115fa83d (2020)	The Danish Gigaword Corpus is primarily used to train and evaluate language models, particularly for Danish and other Scandinavian languages. It serves as a large-scale text resource, containing over 7 billion words from diverse sources and time periods, which enhances the performance of models like ScandiBERT and supports linguistic analysis and NLP tasks. This dataset enables researchers to improve language understanding, generation, and processing, specifically for Danish and Norwegian, by providing extensive textual data for training and evaluation.; The Danish Gigaword Corpus is used to enhance and support Danish NLP research by providing a large-scale text corpus. It is utilized for training and evaluating NLP models and tools, enabling researchers to develop and refine language models and other NLP tasks. The dataset's extensive size and comprehensive coverage of Danish text make it a valuable resource for improving the accuracy and performance of Danish NLP systems.	
Danish Corpus	citing_context	Danish PAROLE corpus	https://www.semanticscholar.org/paper/fb5c2ffcc9e7066a92bc3d91c0f89a34b57a8ee8 (2021)		The Danish PAROLE corpus is used for linguistic research, specifically for dependency parsing and coreference resolution to evaluate the performance of the DaLUKE model. It contains 5512 sentences and 100,733 characters from Danish books, newspapers, and journals (1983-1992). The dataset enables researchers to analyze syntactic structures, grammatical relations, and entity linking in Danish text.	
Danish Corpus	cited_context	Danish Plunkett corpus	https://doi.org/10.1111/LANG.12325 (2018)		The Danish Plunkett corpus is used to study Danish child-directed speech, focusing on language development in children aged 1;1 to 3;11 and twins aged 0;9 to 2;6. It serves as a repository providing access to child language data, enabling researchers to analyze linguistic patterns and developmental milestones in early childhood.	
Danish Corpus	cited_context	Danish sentiment lexicon	https://www.semanticscholar.org/paper/255ad6efe3a104bfeb79093d14d8c4f5239602a6 (2023)	https://www.semanticscholar.org/paper/f852bdbfde7e53510f677d715eda71df8bf1c45b (2022)	The Danish sentiment lexicon is used for sentiment analysis in Danish, providing a list of words and their associated sentiment scores to analyze textual data. It is specifically employed to adjust the VADER sentiment analysis technique for Danish, enhancing its accuracy in processing Danish-language content. This dataset enables researchers to perform nuanced sentiment analysis, addressing research questions related to public opinion, social media analysis, and other textual data in Danish.	
Danish Corpus	citing_context	Danish Sentiment Lexicon (DSL)	https://doi.org/10.18653/v1/2024.wassa-1.15 (2024)	https://www.semanticscholar.org/paper/70cf70502bec945b4658e21bc2f04703d5f8b570 (2022)	The Danish Sentiment Lexicon (DSL) is used to evaluate sentiment analysis performance across various domains, such as fiction and social media. It is compared with other lexicons like Afinn and Sentida to assess its effectiveness and correlation with human judgments. This dataset enables researchers to benchmark and refine sentiment analysis models, ensuring they accurately reflect nuanced sentiments in different contexts.	
Danish Corpus	cited_context | citing_context	Danish (Sigurbergsson and Derczynski, 2020)	https://doi.org/10.1017/S1351324923000517 (2023), https://doi.org/10.18653/v1/2020.semeval-1.270 (2020)	https://www.semanticscholar.org/paper/1e3170311bf21a70b6f974e40b8932fbc0f3052b (2020), https://www.semanticscholar.org/paper/6fd1f59a0e8ad74fdd1fa0afb6cbb9dcc0ce22da (2019)	The Danish (Sigurbergsson and Derczynski 2020) dataset is primarily used in the OffensEval 2020 multilingual competition to analyze and detect offensive language in Danish social media posts. It is employed to evaluate classification performance and cross-lingual transfer, contributing to the broader study of multilingual offensive content detection. The dataset's focus on Danish content enables researchers to assess the effectiveness of offensive language detection models specifically within the Danish linguistic context.; The Danish (Sigurbergsson and Derczynski, 2020) dataset is used to detect offensive language and hate speech in Danish, focusing on classification accuracy and model performance. It is also utilized to train multilingual models, particularly for the Danish language, leveraging its substantial corpus size to enhance model training and performance in specific linguistic contexts.	Danish (Sigurbergsson and Derczynski 2020)
Danish Corpus	cited_context	Danish Similarity Dataset	https://www.semanticscholar.org/paper/0e8b3e895fffee070a9748aba38c33db0000ed4f (2021)	https://www.semanticscholar.org/paper/ee907c58ea2c1909f5df2cae15e16fb601fb3679 (2020)	The Danish Similarity Dataset is used to evaluate Danish word embeddings, focusing on word similarity and semantic similarity tasks. It serves as a gold standard and benchmark for comparing different embedding models, particularly within the DaNLP project. This dataset enables researchers to assess the quality of word representations in Danish, facilitating cross-lingual evaluation and model performance comparisons.	
Danish Corpus	citing_context	Danish Thesaurus	https://www.semanticscholar.org/paper/8cf914c2e1f2ecbcfcfb58a1b2d6a057a7b54265 (2024)	https://www.semanticscholar.org/paper/02b5f428e8d1b0679febeb989522655e223847d8 (2014)	The Danish Thesaurus dataset is used to structure Danish lemmas and senses into hierarchical groups, facilitating lexical and semantic research. It supports the analysis of semantic similarity and synonymy by organizing words into chapters, sections, and subgroups based on their topic and relatedness. This structured approach enables researchers to explore and deduce semantic relationships within the Danish language.	
Danish Corpus	cited_context	Danish Universal Dependencies treebank	https://doi.org/10.18653/v1/W18-6017 (2018)	https://www.semanticscholar.org/paper/a63e4d7fc0c83b27dc9a656ce4f0bce448ae92e8 (2009)	The Danish Universal Dependencies treebank is used for training delexicalised models to parse Danish syntactic structures and for translating content into Danish and other Scandinavian languages using the Apertium machine translation system. It supports cross-lingual parsing experiments, enabling researchers to focus on syntactic analysis and translation accuracy.	
Danish Corpus	citing_context	Danish Universal Dependency tree-bank	https://doi.org/10.18653/v1/2021.findings-acl.158 (2021)	https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015)	The Danish Universal Dependency tree-bank is used to enhance Named Entity Recognition (NER) in Danish text by integrating nested NER layers, improving syntactic and semantic parsing accuracy. It is also utilized to train and evaluate dependency parsers, focusing on syntactic structures and grammatical relations in Danish. This dataset enables researchers to refine parsing models and enhance the accuracy of linguistic analyses in Danish.	
Danish Corpus	citing_context	Danish word-in-Context	https://www.semanticscholar.org/paper/8cf914c2e1f2ecbcfcfb58a1b2d6a057a7b54265 (2024)	https://doi.org/10.18653/v1/N19-1128 (2018)	The Danish word-in-Context dataset is used to evaluate context-sensitive meaning representations in Danish, specifically focusing on word senses and their usage in context. It serves as a lexical-semantic component, providing a comprehensive sense inventory for Danish words. This dataset enables researchers to analyze and understand the nuanced meanings of words within specific contexts, enhancing the accuracy of semantic representation models.	
Danish Corpus	cited_context	Danish WordNet	https://www.semanticscholar.org/paper/02b5f428e8d1b0679febeb989522655e223847d8 (2014)		The Danish WordNet dataset is used to explore the reuse of thesaurus data in Danish, particularly focusing on sense id numbers and the DDO framework. It serves as a predecessor resource, emphasizing core concepts of the Danish language. This dataset enables researchers to analyze and enhance lexical resources, contributing to the development of more robust linguistic tools and frameworks.	
Danish Corpus	cited_context	DanNet	https://doi.org/10.7146/danskestudier.vi2021.134544 (2022), https://www.semanticscholar.org/paper/b56cc5cea3fea350a9cf7020760df16f37bd9127 (2022)	https://doi.org/10.1007/s10579-009-9092-1 (2009)	The DanNet dataset is primarily used to compile a wordnet for Danish, focusing on reusing monolingual dictionaries and aligning with Princeton WordNet. It supports research in lexical semantics, linguistic structure, and cross-lingual mappings by linking Danish lemmas to Princeton WordNet concepts. The dataset also facilitates the study of polysemous and complex lemmas, as well as the semantic features of Danish words, including connotations and gender coding, providing a structured lexical resource for linguistic analysis.	
Danish Corpus	citing_context	dasem word intrusion dataset	https://www.semanticscholar.org/paper/8cf914c2e1f2ecbcfcfb58a1b2d6a057a7b54265 (2024)	https://www.semanticscholar.org/paper/083f59aa5e92edd1191e76e61c87eba952fb1063 (2017)	The dasem word intrusion dataset is used for word-level semantic analysis in Danish, providing lists of semantically related words and outliers. Researchers use this dataset to test and evaluate models' semantic understanding by identifying intruders in word lists, thus assessing the accuracy and robustness of semantic analysis techniques.	
Danish Corpus	citing_context	DAST	https://doi.org/10.36370/tto.2019.17 (2019)	https://doi.org/10.18653/v1/S17-2006 (2017)	The DAST dataset is used to analyze and compare discrepancies between English Twitter data and Danish Reddit data, specifically focusing on rumour veracity and support for rumours. This involves examining how rumours spread and are perceived differently across these platforms and languages. The dataset enables researchers to conduct cross-platform and cross-linguistic analyses, providing insights into the dynamics of online misinformation.	
Danish Corpus	cited_context	DDO	https://www.semanticscholar.org/paper/b56cc5cea3fea350a9cf7020760df16f37bd9127 (2022)	https://www.semanticscholar.org/paper/809d41efa45a3298cb552aa8d524ca94b5f85b48 (2018)	The DDO dataset is used to investigate highly polysemous nouns in Danish, focusing on developing methods to reduce the number of senses through principled approaches. This involves analyzing the semantic variability of nouns and refining methodologies to enhance lexical disambiguation. The dataset's detailed annotations and rich linguistic content enable researchers to address specific challenges in polysemy reduction, contributing to more accurate natural language processing systems.	
Danish Corpus	cited_context	Den Danske Ordbog	https://www.semanticscholar.org/paper/410f9dc198fb317c1dc157f6697ae1c565e252d8 (2017)	https://www.semanticscholar.org/paper/b3c9d563d16960a12b7c90b8a5f3c574fee29862 (2012)	The 'Den Danske Ordbog' dataset is used to compile standardized lexical-semantic data for a Danish Frame lexicon, focusing on both conceptual relationships and semantic categories. This involves creating monolingual Danish dictionary entries, which helps in understanding and structuring the semantic nuances of the Danish language. The dataset enables researchers to develop comprehensive and standardized lexical resources, enhancing the accuracy and depth of linguistic research in Danish.	
Danish Corpus	cited_context	Den Danske Ordbog (DDO)	https://doi.org/10.7146/danskestudier.vi2021.134544 (2022)	https://www.semanticscholar.org/paper/e6db6b2218667d40873118c308565103ce094d9c (2018)	The Den Danske Ordbog (DDO) is used to support lexicographical research and dictionary senses in Danish, providing a comprehensive lexical resource for linguistic analysis and sense clustering. It is also utilized to describe semantic frames for Danish verbs and verbalsubstantives, enhancing lexical coverage and frame-based semantic analysis. Additionally, DDO serves as a lexical resource to provide semantic information for Danish terms, aiding in the development of the Danish FrameNet Lexicon.	
Danish Corpus	cited_context	DT	https://www.semanticscholar.org/paper/70cf70502bec945b4658e21bc2f04703d5f8b570 (2022)	https://www.semanticscholar.org/paper/f852bdbfde7e53510f677d715eda71df8bf1c45b (2022)	The DT dataset is used to construct the Danish Sentiment Lexicon (DSL) by linking groups of words listed in semantic order. This methodology focuses on sentiment analysis in Danish text, enabling researchers to analyze and understand the emotional tone of written content in the Danish language.	
Danish Corpus	cited_context | citing_context	EuroParl	https://www.semanticscholar.org/paper/c637fae348e758278f6267d0e42c03519febbe6a (2020)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The EuroParl dataset is used to pre-train models on Danish text, specifically for Danish language processing and embedding generation. This involves leveraging the dataset's extensive collection of Danish text to develop robust language models that can effectively process and generate embeddings for Danish language tasks. The dataset's rich textual content enables researchers to enhance model performance in Danish-specific natural language processing applications.; The EuroParl dataset is used to pre-train Danish FLAIR embeddings, leveraging Danish text from European Parliament proceedings. This enhances language modeling capabilities, specifically improving the performance of natural language processing tasks in Danish. The dataset's focus on parliamentary text provides a rich source of formal and structured language, which is crucial for developing robust language models.	
Danish Corpus	citing_context	Gigawords dataset	https://doi.org/10.48550/arXiv.2504.01540 (2025)		The Gigawords dataset is used to train generative transformer models, specifically for Danish language processing. It provides a comprehensive corpus of one billion Danish words, enabling researchers to develop and refine models that can generate and process Danish text effectively. This dataset supports research focused on improving natural language generation and understanding in Danish.	
Danish Corpus	citing_context	HASOC 2019	https://doi.org/10.1145/3457610 (2021)	https://doi.org/10.18653/v1/S19-2007 (2019)	The HASOC 2019 dataset is used to experiment with offensive language and hate speech detection across multiple languages, including Arabic, Danish, Greek, Turkish, Hindi, and Spanish. Research focuses on transfer learning aspects, evaluating model performance and cross-lingual capabilities. The dataset enables researchers to assess the effectiveness of models in identifying offensive content and hate speech in multilingual contexts, contributing to the development of more robust and versatile natural language processing systems.	
Danish Corpus	citing_context	Heliport	https://doi.org/10.48550/arXiv.2502.06692 (2025)	https://doi.org/10.18653/v1/2023.acl-short.75 (2023)	The Heliport dataset is used for language identification in Nordic languages, with a focus on Danish. It supports research into language detection and classification methodologies, particularly enhancing the efficiency and accuracy of language identification in multilingual settings. The dataset enables the development and testing of faster language identification tools, such as HeLI-OTS, which are crucial for improving automated language detection systems.	
Danish Corpus	cited_context	HurtLex	https://doi.org/10.18653/v1/2020.semeval-1.188 (2020)	https://doi.org/10.4000/BOOKS.AACCADEMIA.3085 (2018)	HurtLex is a multilingual lexicon of hurtful words used to identify and analyze offensive language across various languages. Researchers apply it to detect and study hate speech and harmful expressions, focusing on the identification of offensive terms. This dataset enables cross-linguistic analysis of harmful language, providing a robust resource for understanding and mitigating online toxicity.	
Danish Corpus	cited_context | citing_context	JW300	https://doi.org/10.48550/arXiv.2206.02230 (2022)	https://doi.org/10.18653/v1/P19-1310 (2019)	The JW300 dataset is used to evaluate pretrained models, particularly in the context of low-resource languages. Comprising parallel texts, it facilitates cross-lingual transfer and evaluation, enabling researchers to assess model performance across different linguistic contexts. This dataset supports the development and refinement of multilingual models by providing a robust resource for testing and validation.; The JW300 dataset is used to evaluate pretrained models on a test set derived from its wide-coverage parallel corpus, particularly focusing on low-resource languages. This evaluation helps assess the model's performance and adaptability in contexts where training data is limited. The dataset's broad linguistic coverage and parallel nature enable researchers to conduct robust cross-lingual evaluations.	
Danish Corpus	cited_context	Massively Parallel Bible corpus	https://www.semanticscholar.org/paper/73cb8b7a02d1b9c9f3e5c21d25752cb4115fa83d (2020)	https://doi.org/10.1007/s10579-014-9287-y (2014)	The Massively Parallel Bible corpus is used to obtain a Danish translation of the Bible, primarily for linguistic analysis. Research involves minimal pre-processing, focusing on file format conversion. This dataset enables detailed linguistic studies by providing a standardized and accessible version of the Bible in Danish, facilitating analysis without extensive data preparation.	
Danish Corpus	citing_context	MB-CDI normative data for Danish	https://doi.org/10.1177/0023830919893390 (2019)	https://doi.org/10.1017/S0305000908008714 (2008)	The MB-CDI normative data for Danish is used to identify and select target words known by most Danish two-year-olds, ensuring comparability in the frequency of occurrence in Danish child-directed speech. This dataset enables researchers to standardize word selection in studies involving young Danish children, facilitating more reliable and comparable results.	
Danish Corpus	citing_context	MeMo	https://www.semanticscholar.org/paper/bd8b50b6faec00365bb53db970b4720abde1f35e (2024)	https://www.semanticscholar.org/paper/77eb39373db02895e082bcbf9661ff7ea66a5dd7 (2022)	The MeMo dataset is used to train pre-trained language models (PLMs) on Danish and Norwegian novels from 1870-1900, specifically to analyze linguistic modernity in literature. This involves applying these models to understand how language evolved during this period, focusing on literary texts. The dataset's historical and linguistic characteristics enable researchers to explore the nuances of language change and modernization in late 19th-century Scandinavian literature.	
Danish Corpus	cited_context	MIM-GOLD-NER	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://doi.org/10.1007/978-3-030-59430-5_4 (2020)	The MIM-GOLD-NER dataset is used for named entity recognition in Danish, specifically focusing on the linguistic aspects of the Danish language. It employs high-quality gold-standard annotations to enhance the accuracy and reliability of named entity recognition models. This dataset enables researchers to address specific challenges in Danish language processing, ensuring robust and linguistically informed entity identification.	
Danish Corpus	cited_context	Natural Questions (NQ)	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://doi.org/10.1162/tacl_a_00433 (2020)	The Natural Questions (NQ) dataset is primarily used as a benchmark for multilingual open-domain question answering, emphasizing linguistic diversity and cross-lingual performance. It serves as the foundational dataset for MKQA, providing a large-scale annotated corpus for training and evaluation, particularly in Danish. This enables researchers to assess and improve models' ability to handle diverse languages and contexts.	
Danish Corpus	cited_context	Nordic Syntax Database (NSD)	https://doi.org/10.1017/S0332586513000218 (2013)	https://www.semanticscholar.org/paper/e293d637030f98057571281516c6647ffd9c61f4 (2009)	The Nordic Syntax Database (NSD) is used to study spontaneous corpus data and linguistic structures in Danish, providing insights into natural language use, dialectal variations, and grammaticality judgments. Researchers employ the dataset to analyze open-source data, focusing on the acceptance of unshifted pronominal objects in Danish linguistic structures. This enables detailed examinations of syntactic patterns and their variations in real-world usage.	
Danish Corpus	cited_context	NorNE	https://www.semanticscholar.org/paper/c637fae348e758278f6267d0e42c03519febbe6a (2020)	https://www.semanticscholar.org/paper/c4adb896b0c905d610b96891b4e5019627783f82 (2014)	The NorNE dataset, annotated with named entities for Norwegian, is used to study the distribution and usage of named entities in Danish text. It serves as a resource for named entity recognition tasks in Nordic languages, including Danish. This dataset enables researchers to analyze and improve the accuracy of named entity recognition models across related languages.	
Danish Corpus	citing_context	NoSTA-D	https://doi.org/10.18653/v1/2021.findings-acl.158 (2021)	https://www.semanticscholar.org/paper/f6ea166f38804860dc6b4ef66d5dca0eed17a2d1 (2014)	The NoSTA-D dataset serves as a foundational reference for developing named entity annotation guidelines, particularly in adapting German guidelines to Danish. It is used to inform and refine annotation practices, ensuring consistency and accuracy in named entity recognition tasks. This dataset enables researchers to build robust annotation frameworks, facilitating more reliable natural language processing studies.	
Danish Corpus	citing_context	NSD	https://doi.org/10.5617/nals.10102 (2023)	https://www.semanticscholar.org/paper/f718c41e5b432b276430ee1b6aac100ae8b8a64a (2009)	The NSD dataset is used to study syntactic structures in Danish, particularly focusing on embedded conditional clauses and preposed negation in embedded contexts. Researchers employ this dataset to analyze these specific linguistic phenomena, leveraging its detailed annotations to explore complex grammatical structures and their usage in natural language.	
Danish Corpus	cited_context	OLID	https://doi.org/10.18653/v1/2020.semeval-1.206 (2020)	https://doi.org/10.18653/v1/N19-1144 (2019)	The OLID dataset is used to fine-tune BERT models for detecting and categorizing offensive language in Danish social media. It focuses on identifying biased or offensive content, enabling researchers to analyze specific aspects of offensive language. This dataset supports the development of more accurate and context-aware natural language processing models for social media monitoring.	
Danish Corpus	citing_context	OSCAR	https://doi.org/10.48550/arXiv.2303.17183 (2023)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The OSCAR dataset is used as a derivative of Common Crawl data to train multilingual models, emphasizing language diversity and coverage. It serves as a foundational dataset, filtered and processed to create Multilingual C4, which focuses on language-specific content. This dataset is crucial for enhancing multilingual training and evaluation, enabling researchers to develop models that perform well across various languages.	
Danish Corpus	citing_context	PHEME	https://doi.org/10.36370/tto.2019.17 (2019)	https://www.semanticscholar.org/paper/30d1385b3729dc5f93746eb5bac284e681e98b6d (2014)	The PHEME dataset is used for cross-linguistic and cross-platform experiments to assess the veracity of information in social media. It includes Danish language data and is employed to analyze how information spreads and is perceived across different languages and platforms. This enables researchers to address questions related to misinformation and trust in online content.	
Danish Corpus	citing_context	Scandeval	https://www.semanticscholar.org/paper/8cf914c2e1f2ecbcfcfb58a1b2d6a057a7b54265 (2024)	https://doi.org/10.48550/arXiv.2304.00906 (2023)	The Scandeval dataset is used to provide semantic benchmarks for Danish, Swedish, and Norwegian languages, focusing on tasks such as sentiment classification and linguistic acceptability. It supports the evaluation of NLP models by offering standardized test cases, enabling researchers to assess model performance across these Nordic languages.	
Danish Corpus	cited_context	SemDaX	https://doi.org/10.7146/danskestudier.vi2021.134544 (2022)	https://www.semanticscholar.org/paper/d3e80cf7478aa8c6ddd3482d07d39ef6b22f58d7 (2015)	The SemDaX dataset is used to analyze annotator discrepancies in coarse-grained sense annotation of Danish nouns across various textual domains. Researchers focus on consistency in meaning tagging, employing the dataset to evaluate and improve the reliability of semantic annotations. This analysis helps address research questions related to the variability and consistency of human annotators in linguistic tasks.	
Danish Corpus	cited_context	SemDaX lexical sample dataset	https://www.semanticscholar.org/paper/b56cc5cea3fea350a9cf7020760df16f37bd9127 (2022)	https://www.semanticscholar.org/paper/3139e0085d5d7d88c4f908818da66fec28f27f17 (2016)	The SemDaX lexical sample dataset is used to tag and annotate the 20 most polysemous nouns in Danish, focusing on sense annotations with scalable sense inventories. This methodology involves detailed linguistic analysis to address research questions related to polysemy and semantic variation in Danish. The dataset's focus on polysemous nouns enables researchers to explore and refine sense inventories, enhancing understanding of lexical semantics.	
Danish Corpus	citing_context	SenSALDO	https://www.semanticscholar.org/paper/f852bdbfde7e53510f677d715eda71df8bf1c45b (2022)	https://www.semanticscholar.org/paper/58ea9645b0e1418f0c6e528e8c004e134ab9738b (2018)	The SenSALDO dataset is used to support ontological approaches to sentiment analysis in Danish by linking a comprehensive monolingual dictionary at the sense level. It is employed to compare the distribution of sentiment polarities, particularly focusing on the predominance of negative lemmas, and to enhance the coverage and accuracy of sentiment analysis by considering the inclusion of neutral lemmas. This dataset facilitates detailed lexical and ontological analyses, improving the precision of sentiment classification in Danish texts.	
Danish Corpus	citing_context	SOLID	https://doi.org/10.1017/S1351324923000517 (2023), https://doi.org/10.18653/v1/2020.semeval-1.188 (2020)	https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de (2019)	The SOLID dataset is used to fine-tune RoBERTa-large models, primarily through unsupervised masked language modeling and for offensive language detection in a multi-lingual setting. This enables researchers to achieve high F1 scores, enhancing the model's performance in identifying offensive content. The dataset's multi-lingual capabilities and large scale support these specific research applications.	
Danish Corpus	cited_context	UD-Danish	https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015)	https://doi.org/10.3115/1219840.1219852 (2005)	The UD-Danish dataset is used to train and evaluate graph-based dependency parsers, focusing on Danish language syntax and dependency structures. This dataset enables researchers to analyze and improve the accuracy of dependency parsing models specifically for Danish, contributing to advancements in natural language processing for the Danish language.	
Danish Corpus	cited_context | citing_context	Universal Dependencies datasets	https://doi.org/10.48550/arXiv.2304.00906 (2023), https://www.semanticscholar.org/paper/248bdbf1410819c92c0f929dc682f2f57b0ef17c (2015)	https://www.semanticscholar.org/paper/daddb6421bbff147afb19d2d705727aadf9cd2c6 (2006), https://www.semanticscholar.org/paper/b3ebfe54eb8d5d6b566768d5dcef2d6026089133 (2020)	The Universal Dependencies datasets are used to create the ScaLA datasets for Danish and other Nordic languages, providing annotated linguistic data. This data is specifically employed for dependency parsing and syntactic analysis, enabling researchers to develop and evaluate natural language processing models that accurately capture grammatical structures in these languages.; The Universal Dependencies datasets are used to create annotated linguistic data for dependency parsing and syntactic analysis, particularly in the development of the ScaLA datasets for Danish and other Nordic languages. These datasets provide essential resources for enhancing the accuracy and robustness of natural language processing models in these languages.	
Danish Corpus	citing_context	Universal Dependencies 2.14 treebanks	https://doi.org/10.48550/arXiv.2502.06692 (2025)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The Universal Dependencies 2.14 treebanks dataset is used to maintain train/dev/test splits for linguistic analysis, focusing on multilingual treebank data for dependency parsing and syntactic structure. It is also utilized to fine-tune large language models for zero-shot translation between Bokmål, Danish, Nynorsk, and Swedish, enhancing translation reliability through high-quality data. This dataset supports research in multilingual dependency parsing and cross-lingual transfer learning.	
Danish Corpus	citing_context	WikiANN	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://doi.org/10.1007/978-3-030-59430-5_4 (2020)	The WikiANN dataset is used for named entity recognition (NER) in Danish, specifically focusing on the Danish portion of the dataset. Researchers employ this dataset to train and evaluate NER models, leveraging its annotated text to improve the accuracy of identifying and classifying named entities in Danish text. This enables advancements in natural language processing for Danish, enhancing applications such as information extraction and text analysis.	
Danish Corpus	citing_context	Wikipedia Detox dataset	https://doi.org/10.18653/v1/2020.semeval-1.188 (2020)	https://doi.org/10.4000/BOOKS.AACCADEMIA.3085 (2018)	The Wikipedia Detox dataset is used to study toxic behavior in online discussions, particularly focusing on identifying aggressive and harmful comments. It is applied to analyze harmful language across multiple languages, including Danish, to identify and categorize offensive words and expressions. This dataset enables researchers to develop methods for detecting and mitigating toxic content in online platforms.	
Danish Corpus	citing_context	ZuCo 2.0	https://doi.org/10.48550/arXiv.2204.13311 (2022)	https://www.semanticscholar.org/paper/a1030846ad58681bd33b6c936e7d79a7f458dbe7 (2019)	The ZuCo 2.0 dataset is used to analyze physiological responses, particularly eye tracking data, during natural reading to understand cognitive processes in Danish readers. This dataset enables researchers to explore how specific linguistic features influence reading behavior and cognitive load, providing insights into the mechanisms of reading comprehension and processing in Danish.	
Dholuo Corpus	citing_context	Autshumato	https://doi.org/10.21248/jlcl.36.2023.243 (2022)	https://www.semanticscholar.org/paper/feed9002765ede951407801628ab5e4cdc96b60b (2009)	The Autshumato dataset is used for machine translation research, specifically to improve the translational accuracy between official South African languages. It focuses on evaluating language pairs, which may include Dholuo, to enhance the quality and reliability of translations. This dataset enables researchers to test and refine machine translation models, addressing the specific challenges of translating between less commonly studied languages.	
Dholuo Corpus	cited_context	Crúbadán project archive	https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)	https://www.semanticscholar.org/paper/ef9f81a662aa9618173d033e95d46d74e14aeafc (2007)	The Crúbadán project archive is used to access a small web-mined corpus for the Dholuo language, supporting research on under-resourced languages. This dataset enables researchers to study linguistic features and develop computational tools for languages with limited digital resources. The corpus provides essential data for analyzing and preserving under-documented languages.	
Dholuo Corpus	citing_context	Dholuo	https://doi.org/10.21248/jlcl.36.2023.243 (2022)	https://doi.org/10.1162/tacl_a_00317 (2020)	The Dholuo dataset is used to investigate question answering in the Dholuo language, specifically focusing on information-seeking queries. Researchers employ this dataset to explore typological diversity in language processing, utilizing it to analyze and improve the handling of diverse linguistic structures in computational models. This enables more effective and culturally relevant information retrieval systems for under-resourced languages like Dholuo.	
Dholuo Corpus	cited_context	SAWA corpus	https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)	https://doi.org/10.5788/18-0-488 (2009)	The SAWA corpus is used to construct a trilingual parallel corpus (English-Luo-Swahili) for computational and linguistic analysis. It leverages chapter and verse indications for paragraph alignment, facilitating cross-linguistic studies, particularly focusing on the New Testament data. This enables detailed morphological and linguistic analyses across the three languages.	
Eastern Oromo Corpus	citing_context	Aya-101	https://doi.org/10.48550/arXiv.2412.17837 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The Aya-101 dataset is used to evaluate the performance of a fine-tuned mT5 model on the Eastern Oromo language, specifically focusing on translation quality and linguistic accuracy. This involves assessing the model's ability to translate text accurately while maintaining linguistic nuances. The dataset enables researchers to test and improve machine translation systems for under-resourced languages like Eastern Oromo.	
Eastern Oromo Corpus	citing_context	CommonCrawl 2	https://doi.org/10.48550/arXiv.2403.13737 (2024)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The dataset 'CommonCrawl 2' is mentioned in the citation context but lacks detailed descriptions of its usage in specific research. Therefore, there is no evidence to support claims about its application, methodology, research questions, or enabling capabilities in any particular field, including Eastern Oromo language research.	
Eastern Oromo Corpus	cited_context	Eastern Oromo Dataset	https://doi.org/10.7176/nmmc/92-01 (2020)		The Eastern Oromo Dataset is primarily used for sentiment analysis of Afaan Oromoo text, focusing on opinion mining and classification of sentiments in reviews and social media posts. Researchers employ various machine learning models, including Naive Bayes, Maximum Entropy, Support Vector Machines, MNB, LSTM, and CNN, to analyze and classify positive, negative, and neutral sentiments. This dataset enables detailed understanding of public opinion and social media reactions in the Eastern Oromo language.	
Eastern Oromo Corpus	cited_context	Eastern Oromo language dataset	https://www.semanticscholar.org/paper/2ac19d63e1adba20473a6d1122c598f81efc3c58 (2022)	https://doi.org/10.18653/v1/2022.acl-long.125 (2021)	The Eastern Oromo language dataset is used to train and evaluate models for Named Entity Recognition (NER) and to develop language processing models for text classification and other NLP tasks. Researchers adapt mBERT from English CoNLL03 to African languages, leveraging the dataset to enhance model performance and linguistic understanding in the Eastern Oromo language.	
Eastern Oromo Corpus	citing_context	Jibril and Tantuğ (2023)	https://doi.org/10.48550/arXiv.2403.13737 (2024)	https://doi.org/10.23919/ISTAFRICA.2017.8102402 (2017)	The Jibril and Tantuğ (2023) dataset is used for named entity recognition in the Eastern Oromo language, featuring four classes: PER (person), LOC (location), ORG (organization), and O (other). It is compared with the Gambäck and Sikdar dataset to evaluate performance and accuracy in identifying these entities. This comparison helps researchers assess the effectiveness of different datasets in handling named entity recognition tasks in the Eastern Oromo language.	
Eastern Oromo Corpus	citing_context	K-MHaS	https://doi.org/10.1109/CCET56606.2022.10080837 (2022)	https://doi.org/10.48550/arXiv.2208.10684 (2022)	The K-MHaS dataset is used for multi-label hate speech recognition in the Eastern Oromo language, specifically focusing on intersectionality and subjectivity in news comments. Researchers employ this dataset to analyze and classify hate speech, considering the nuanced contexts and overlapping identities present in the data. This enables a deeper understanding of how hate speech manifests in online discourse within the Eastern Oromo community.	
Eastern Oromo Corpus	citing_context	MasakhaNEWS	https://doi.org/10.48550/arXiv.2403.13737 (2024)	https://doi.org/10.48550/arXiv.2304.09972 (2023)	The MasakhaNEWS dataset is used for evaluating machine learning models on tasks such as named entity recognition, news topic classification, and sentiment analysis for African languages, including Oromo. It supports the assessment of model performance, accuracy, and coverage in these tasks, enabling researchers to compare different models and tokenization strategies. The dataset's relevance lies in its application to multiple African languages, facilitating cross-lingual evaluations.	
Eastern Oromo Corpus	citing_context	Unsupervised OPDO official Facebook page	https://doi.org/10.7176/nmmc/92-01 (2020)		The Unsupervised OPDO official Facebook page dataset is used for unsupervised opinion mining to analyze sentiments in Afaan Oromoo, specifically focusing on positive, negative, and neutral reviews from the OPDO official Facebook page. This methodology involves extracting and categorizing textual data to understand public opinions and reactions to political content. The dataset's relevance lies in its ability to capture real-time, unfiltered public sentiment, enabling researchers to study the dynamics of political discourse and public perception in the Afaan Oromoo-speaking community.	
Finnish Corpus	cited_context	25,000 sentences by Öhman et al. (2020)	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://doi.org/10.18653/v1/W18-6205 (2018)	The dataset '25,000 sentences by Öhman et al. (2020)' is used for sentiment analysis in Finnish language contexts, specifically comparing it with the authors' own datasets derived from Finnish movie subtitles and social media content. This comparison helps evaluate and enhance sentiment analysis models, leveraging the dataset's large size and diverse content to improve accuracy and robustness in detecting sentiments in Finnish text.	
Finnish Corpus	cited_context	6427 sentences published by Kajava (2018)	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://www.semanticscholar.org/paper/5fc07cf30b933e920f905bab6aa83e277edb56d4 (2018)	The dataset of 6427 sentences published by Kajava (2018) is primarily used for training and evaluating natural language processing models, particularly in sentiment analysis. It serves as a benchmark for comparing with researchers' own datasets, focusing on sentiment analysis in Finnish movie subtitles and social media content. The dataset supports cross-lingual sentiment preservation and transfer learning, enabling binary and multi-class classification tasks.	
Finnish Corpus	citing_context	CMU Wilderness corpus	https://www.semanticscholar.org/paper/c38485e82c6e426332f802217e7024870d1b8f2c (2019)	https://doi.org/10.1109/ICASSP.2019.8683536 (2019)	The CMU Wilderness corpus is used to provide multilingual speech data, primarily from recorded readings of the New Testament, supporting research in speech processing and multilingual speech recognition. It enables the development of ASR and TTS models for over 700 languages, including Finnish, by offering audio excerpts and transcriptions. This dataset facilitates the collection and analysis of speech data across multiple languages, enhancing multilingual speech research and technology development.	
Finnish Corpus	cited_context	CoNLL'09 shared task	https://doi.org/10.1007/s10579-013-9244-1 (2013)	https://doi.org/10.3115/1596409.1596411 (2009)	The CoNLL'09 shared task dataset is used for syntactic and semantic dependency parsing across multiple languages. It supports the collection and evaluation of parsing results, enabling cross-lingual comparisons. The dataset's transformed treebank format facilitates the analysis of both syntactic and semantic dependencies, enhancing research in natural language processing and computational linguistics.	
Finnish Corpus	cited_context | citing_context	CoNLL 2018	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	https://doi.org/10.18653/v1/K18-2001 (2018)	The CoNLL 2018 dataset is used to evaluate end-to-end parsing from raw text into dependency structures across 57 languages, including Finnish. It focuses on assessing multilingual parsing performance, employing methodologies that test the accuracy and efficiency of parsing models across diverse linguistic contexts. This dataset enables researchers to benchmark and improve parsing algorithms, addressing specific research questions related to cross-lingual consistency and model generalization.; The CoNLL 2018 dataset is used for end-to-end parsing from raw text into dependency structures, particularly in multilingual contexts including Finnish. It employs the Universal Dependencies framework to ensure comparability with results from the CoNLL 2018 shared task, facilitating research in multilingual parsing and dependency analysis.	
Finnish Corpus	citing_context	Corpus of Translated Finnish (CTF)	https://doi.org/10.1017/S0332586520000013 (2020)	https://doi.org/10.4324/9781315759951-9 (2000)	The Corpus of Translated Finnish (CTF) is used to study various aspects of Finnish language, including advanced learner proficiency, linguistic development, and translation patterns. Researchers employ quantitative contrastive approaches to analyze unique linguistic features, translation consistency, and syntactic structures in both F1 and FT subsets. The dataset also supports cross-linguistic comparisons, such as between Finnish and German, and examines formal writing styles in academic contexts. It enables detailed analyses of language acquisition, error patterns, and disciplinary language use in non-native speakers.	
Finnish Corpus	cited_context	Digitoday corpus	https://doi.org/10.1007/s10579-019-09471-7 (2019)	https://www.semanticscholar.org/paper/b5fa61681f62e024ee326e7c78b2e52f53564de0 (2018)	The Digitoday corpus is used to study the impact of morphology on named entity recognition in the Finnish language. Researchers focus on disambiguating morphological tags, employing the dataset to enhance the accuracy of named entity recognition systems by addressing morphological complexities. This corpus enables detailed analysis of morphological disambiguation, crucial for improving natural language processing tasks in Finnish.	
Finnish Corpus	citing_context	Digitoday 2014 corpus	https://doi.org/10.1109/BESC57393.2022.9995536 (2022)	https://doi.org/10.1007/s10579-019-09471-7 (2019)	The Digitoday 2014 corpus is used for named entity recognition in Finnish technology news, specifically to identify and classify named entities within news articles. This dataset enables researchers to develop and evaluate natural language processing models tailored for the Finnish language, focusing on the technological domain. The corpus's relevance lies in its specialized content, which facilitates the training and testing of algorithms designed to recognize entities such as organizations, people, and products in technology-related texts.	
Finnish Corpus	citing_context	Eduskunta	https://doi.org/10.48550/arXiv.2401.07923 (2024)	https://doi.org/10.1007/s10579-023-09644-5 (2023)	The Eduskunta dataset is used to classify ministers' answers to questions from Members of Parliament, labeled by ministry, to analyze ministerial responsibilities and communication. It is also utilized to classify sentiment polarity in Finnish social media sentences, distinguishing between positive, negative, and neutral sentiments. This dual application supports research in political communication and social media analysis, leveraging the dataset's structured content and sentiment labels.	
Finnish Corpus	citing_context	Europarl	https://www.semanticscholar.org/paper/0ad7fa9d1db5a92666727530ecf3c80ab1bd8bcf (2020)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The Europarl dataset is primarily used to create parallel sentences for training and evaluation in machine translation, particularly for English-Finnish models. It contributes to a large, diverse multilingual corpus, enabling the improvement of cross-lingual sentence embeddings and the analysis of linguistic patterns. The dataset's extensive range of language pairs enhances its utility in developing robust translation systems.	
Finnish Corpus	citing_context	FESC	https://doi.org/10.48550/arXiv.2506.08717 (2025)	https://doi.org/10.1159/000091405 (2006)	The FESC dataset is used to analyze emotions in Finnish speech, specifically focusing on vowel segments and glottal flow characteristics. Researchers employ the normalized amplitude quotient to study these features, enabling detailed examination of emotional expression in speech. This dataset facilitates research into the acoustic correlates of emotion, contributing to understanding how emotional states are conveyed through speech signals.	
Finnish Corpus	cited_context | citing_context	FIN-bench 10	https://doi.org/10.48550/arXiv.2404.01856 (2024)		FIN-bench 10 is used to evaluate and benchmark Finnish language models, focusing on their performance across various tasks. It combines translated and manually corrected tasks from BIG-bench with Finnish-specific tasks, enabling researchers to assess model capabilities at different training stages. The dataset also evaluates English to Finnish and Finnish to English translation performance, particularly in the first 100 sentences of test data, using random sampling from the development set. This comprehensive evaluation helps in understanding the progression and effectiveness of language models in Finnish.; FIN-bench 10 is used to evaluate and benchmark model performance on Finnish language tasks, including translation and specific Finnish tasks. It combines translated and manually corrected tasks from BIG-bench with additional Finnish-specific tasks. The dataset assesses various aspects of model capabilities, such as translation performance and task-specific performance, by comparing larger English-focused models with smaller monolingual Finnish models. It also evaluates model checkpoints at 10% intervals to systematically assess the progression of Finnish capabilities.	
Finnish Corpus	cited_context | citing_context	FinCORE corpus	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	https://www.semanticscholar.org/paper/56dfc5866c1658cbaaf90cb91335cc3a544cb5e3 (2019)	The FinCORE corpus is used to train and evaluate models for identifying machine-translated and generated texts. Researchers focus on lexical features and employ SVM classification methods. This dataset enables the development and testing of algorithms to distinguish between human-written and machine-generated content, enhancing the accuracy of text authentication and detection systems.; The FinCORE corpus is used to train a Support Vector Machine (SVM) classifier with lexical features to filter machine-translated and generated texts. This improves data quality in multilingual register identification, enhancing the accuracy of text classification tasks. The dataset's lexical features are crucial for distinguishing between different text types, enabling more reliable register identification in multilingual contexts.	
Finnish Corpus	cited_context | citing_context	FiNER corpus	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019), https://www.semanticscholar.org/paper/aeb9a63bd0a654b685bc9dff898865ce39447fdd (2020)	https://www.semanticscholar.org/paper/831b942371710e4aa22045874bb621650d337556 (2017)	The FiNER corpus is used to train and evaluate named entity recognition systems specifically for the Finnish language, leveraging its rich morphological and syntactic features. This dataset, derived from Finnish newspaper material, enables researchers to address the challenges of entity recognition in a highly inflected language, enhancing the accuracy and robustness of natural language processing models.; The FiNER corpus is primarily used to train and evaluate named entity recognition (NER) systems in Finnish, with a focus on news texts, including 19th century, modern, and technology news domains. It serves as a reference for annotation guidelines and tagger documentation, aiding in the development of NER models. The dataset supports research on in-domain performance, F-score metrics, and syntactic and semantic analysis, enhancing the accuracy and reliability of NER systems in Finnish.	
Finnish Corpus	citing_context	Finlex	https://www.semanticscholar.org/paper/0ad7fa9d1db5a92666727530ecf3c80ab1bd8bcf (2020)	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	The Finlex dataset is primarily used to train and enhance the FinBERT model, focusing on various types of Finnish text including news articles, web content, online discussions, and legal documents. This training improves the model's ability to process and understand formal, journalistic, colloquial, and legal language. The dataset is also used to evaluate the performance of FinBERT and fastText models using Annif extractor and SVD, specifically in Finnish language processing tasks.	
Finnish Corpus	cited_context	Finnish historical newspaper collection 1771-1910	https://www.semanticscholar.org/paper/831b942371710e4aa22045874bb621650d337556 (2017)	https://www.semanticscholar.org/paper/ca707ae668a3a2dc581e8b72d94a5dd30b3dbee8 (2016)	The Finnish historical newspaper collection 1771-1910 is used as an evaluation dataset to assess the performance of named entity recognition (NER) tools on OCRed texts from Finnish historical newspapers. This dataset enables researchers to evaluate the accuracy and effectiveness of NER tools in processing and extracting entities from historical documents, focusing on the specified time period. The dataset's historical and linguistic characteristics provide a unique challenge for NER tool development and testing.	
Finnish Corpus	cited_context	Finnish Internet ParseBank	https://www.semanticscholar.org/paper/4c4cfb375a59785d4ffbe9a377df50ec5ef5e321 (2018)	https://doi.org/10.3233/978-1-61499-442-8-184 (2014)	The Finnish Internet ParseBank is used to extract syntactic bigram and n-gram data from a large-scale corpus of internet Finnish. Researchers focus on linguistic structures, parsing errors, non-words, and incorrectly tagged or lemmatized words. This dataset enables the construction of databases and the analysis of syntactic patterns, enhancing understanding of internet Finnish and improving natural language processing techniques.	
Finnish Corpus	cited_context	Finnish Internet Parsebank project	https://doi.org/10.1007/s10579-019-09471-7 (2019)	https://doi.org/10.3233/978-1-61499-442-8-184 (2014)	The Finnish Internet Parsebank project dataset is used to initialize distributional word representations with pretrained word embeddings, enhancing model performance on Finnish language tasks such as nested entity recognition. It supports the evaluation of neural models' recall and generalization capabilities on in-domain Finnish text. The dataset, derived from a large-scale corpus of Finnish internet text, including Wikipedia, provides a broad linguistic context, improving model performance across various domains.	
Finnish Corpus	citing_context	Finnish Morphology Test (FMT)	https://doi.org/10.1177/01427237231167301 (2023)		The Finnish Morphology Test (FMT) dataset is used to validate metalinguistic abilities, particularly letter naming skills, in Finnish children as part of the FinCDI III. It also collects data on morphological inflections, assessing the correct usage of inflections in various linguistic contexts. This dataset enables researchers to evaluate and understand specific linguistic skills and developmental milestones in Finnish-speaking children.	
Finnish Corpus	citing_context	Finnish news corpus	https://doi.org/10.18653/V1/2021.NLP4IF-1.6 (2021)	https://doi.org/10.1007/s10579-019-09471-7 (2019)	The Finnish news corpus is used for annotating articles to improve named entity recognition in Finnish news materials. This dataset supports research in Finnish language processing and information extraction, specifically focusing on enhancing the accuracy of identifying and categorizing named entities within news texts. The annotated data enables the development and evaluation of natural language processing models tailored for the Finnish language.	
Finnish Corpus	cited_context	Finnish News Corpus	https://www.semanticscholar.org/paper/aeb9a63bd0a654b685bc9dff898865ce39447fdd (2020)	https://doi.org/10.18653/v1/D18-1309 (2018)	The Finnish News Corpus is used to evaluate deep learning-based Named Entity Recognition (NER) methods, specifically focusing on in-domain performance and F-score metrics. This dataset enables researchers to assess the effectiveness of NER models in accurately identifying entities within news articles, providing a benchmark for model performance in the Finnish language context.	
Finnish Corpus	cited_context	Finnish sentiment data	https://doi.org/10.1007/s10579-023-09644-5 (2023)		The Finnish sentiment data dataset is used to analyze sentiment in Finnish text, specifically for developing and evaluating sentiment analysis models. It was made publicly available in 2020, enabling researchers to focus on improving the accuracy and effectiveness of sentiment analysis tools tailored for the Finnish language.	
Finnish Corpus	cited_context	Finnish sentiment and emotion lexicons	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://doi.org/10.1111/j.1467-8640.2012.00460.x (2013)	The Finnish sentiment and emotion lexicons dataset is used to enhance the accuracy and reliability of sentiment and emotion analysis specifically for the Finnish language. Researchers manually verify the lexicons to ensure their precision, which is crucial for applications requiring high linguistic fidelity. This dataset enables more robust and contextually appropriate analyses in Finnish language studies.	
Finnish Corpus	citing_context	Finnish SQuAD	https://doi.org/10.48550/arXiv.2501.05963 (2025)	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	The Finnish SQuAD dataset is used to train an extractive question-answering (QA) model, specifically focusing on span-detection tasks. Researchers employ the Finnish FinBERT-base model, adhering to the methodology outlined by Devlin et al. (2019). This dataset enables the development and evaluation of QA systems tailored for the Finnish language, enhancing the model's ability to identify and extract precise answer spans from given texts.	
Finnish Corpus	citing_context	Finnish translation of SQuAD2	https://doi.org/10.48550/arXiv.2501.05963 (2025)	https://doi.org/10.48550/arXiv.2211.13794 (2022)	The Finnish translation of SQuAD2 is used to evaluate the performance of question answering models in Finnish. Researchers compare the model's performance against previous results using this dataset, which contains Finnish translations of SQuAD2 questions and passages. This enables the assessment of advancements in Finnish language processing and question answering systems.	
Finnish Corpus	cited_context	Finnish UD	https://www.semanticscholar.org/paper/aeb9a63bd0a654b685bc9dff898865ce39447fdd (2020)	https://www.semanticscholar.org/paper/1bfd03d3cc9d41564002ad32db1793e91f0c5220 (2015)	The Finnish UD dataset is primarily used to create and enhance Finnish Named Entity Recognition (NER) corpora, focusing on dependency parsing and syntactic word definitions. It incorporates Finnish NER annotation guidelines and tools, leveraging existing manual annotations and source texts. This dataset enables the development of broad-coverage corpora for training modern NER methods, aligning tokens with existing linguistic resources and adhering to the Universal Dependencies framework.	
Finnish Corpus	citing_context	Finnish Wikipedia 20221120 dump	https://doi.org/10.48550/arXiv.2311.05640 (2023)		The Finnish Wikipedia 20221120 dump is used to create a dataset of 110 million tokens for text extraction, specifically focusing on the Finnish language. The dataset is processed using WikiExtractor to facilitate linguistic analysis, enabling researchers to conduct detailed examinations of Finnish language structures and usage.	
Finnish Corpus	cited_context	Finnish WordNet	https://www.semanticscholar.org/paper/1b4b539ca43efce3be9d3a3a9ba1cd6aeec8690e (2021)		The Finnish WordNet dataset is used to enhance lexical coverage and synonymy in Finnish language processing tasks by supplementing existing synonym dictionaries. This supplementation improves the richness and accuracy of lexical resources, supporting more effective language processing methodologies. The dataset's comprehensive synonymy data enables researchers to address challenges in natural language understanding and processing, particularly in tasks requiring extensive lexical knowledge.	
Finnish Corpus	cited_context	FinnSentiment	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://www.semanticscholar.org/paper/f591ac054f3dde80d522c556b3e186b3b7af95a5 (2018)	The FinnSentiment dataset is used for sentiment analysis in Finnish social media, employing LSTM recurrent neural networks to classify sentiment. It is also utilized to investigate the relationship between social media sentiment and stakeholder perceptions in Finnish public companies, particularly focusing on the impact on financial reporting. This dataset enables researchers to analyze and understand public sentiment and its implications on corporate financial practices.	
Finnish Corpus	cited_context | citing_context	FinnTreeBank	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019), https://doi.org/10.1007/s10579-013-9244-1 (2013)	https://doi.org/10.18653/v1/K18-2014 (2018)	The FinnTreeBank dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, based on the provided evidence, it cannot be definitively stated how FinnTreeBank is used in research.; The FinnTreeBank dataset is used in research to compare the performance of different methods in POS tagging, particularly in the context of the CoNLL 2018 shared task. It is also utilized to study interannotator agreement in dependency syntax for Finnish, focusing on both single and double-annotated portions of the dataset. This enables researchers to evaluate the reliability and consistency of syntactic annotations in Finnish linguistic studies.	
Finnish Corpus	citing_context	Finsyn	https://www.semanticscholar.org/paper/dfe5398de4e8fc5b41ea2b9902f1391b8bb2f500 (2025)		The Finsyn dataset is used in Finnish language processing research, focusing on syntactic analysis, parsing, and the study of personal pronouns. It is also utilized to develop and evaluate speech synthesis systems, particularly for two female speakers across various speaking styles. Additionally, the dataset is used to extract posteriorgram probabilities from a Kaldi HMM-TDNN-medium model, studying 29 Finnish phonemes and 3 special symbols, which aids in phonetic research.	
Finnish Corpus	citing_context	FLOTA ArXiv-S	https://doi.org/10.48550/arXiv.2401.07923 (2024)	https://doi.org/10.18653/v1/2022.acl-short.43 (2022)	The FLOTA ArXiv-S dataset is used to evaluate and compare the performance of pretrained language models, with a focus on tokenization methods and model performance. Researchers employ this dataset to assess how different tokenization techniques impact the effectiveness of language models, providing insights into optimal model configurations and performance metrics.	
Finnish Corpus	cited_context	FSD corpus	https://doi.org/10.21437/Interspeech.2013-42 (2013)		The FSD corpus is used to assess language proficiency among adults of various linguistic backgrounds, focusing on evaluating linguistic skills and understanding language variation. Researchers employ this dataset to analyze and compare proficiency levels, using it to address questions related to language skills and variations. The dataset's comprehensive coverage of linguistic data enables detailed evaluations and comparisons across different adult populations.	
Finnish Corpus	citing_context	Glosbe	https://www.semanticscholar.org/paper/ce4ed291f4039730746bfcc04de2379be049f123 (2022)	https://www.semanticscholar.org/paper/d61f61fbcf0cb78f4188d6e0b61b031c8ce16ba1 (2016)	The Glosbe dataset is used to retrieve parallel sentences for translation, particularly serving as a fallback when other sources are insufficient. It supports multilingual translation, often using English as a pivot language. This dataset enables researchers to enhance translation quality and coverage by providing additional sentence pairs, facilitating more robust and diverse translation models.	
Finnish Corpus	cited_context	Hel-laSwag	https://doi.org/10.48550/arXiv.2404.01856 (2024)	https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea (2021)	The Hel-laSwag dataset is used to evaluate model performance on multiple-choice commonsense reasoning tasks, specifically focusing on understanding context and selecting correct answers from sets of Finnish sentences. This dataset enables researchers to assess models' abilities to comprehend and reason about natural language in a Finnish context, providing insights into the effectiveness of various NLP approaches in this language.	
Finnish Corpus	cited_context	HENKO(11)	https://www.semanticscholar.org/paper/b2e9fdc30d3c9c62b0be8175672a81922f6a661e (2020)	https://doi.org/10.29007/ZQS5 (2019)	The HENKO(11) dataset is utilized for named entity recognition and linking tasks, specifically to enhance automatic content annotation, data anonymization, and enrich linked data for genealogical network analysis. This dataset supports these tasks by providing a robust foundation for identifying and linking entities, thereby improving the accuracy and utility of genealogical data.	
Finnish Corpus	citing_context	human annotated Finnish data	https://doi.org/10.18653/V1/2020.COLING-MAIN.575 (2020)	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	The human annotated Finnish data dataset is used to evaluate the performance of FinBERT, a language model tailored for Finnish. Specifically, it is employed to compare FinBERT's performance against human annotations, focusing on macro f1 scores to assess model accuracy in Finnish language tasks. This dataset enables researchers to benchmark and refine the model's effectiveness in handling Finnish linguistic nuances.	
Finnish Corpus	citing_context	Jigsaw Toxicity dataset	https://doi.org/10.48550/arXiv.2311.05640 (2023)	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019)	The Jigsaw Toxicity dataset is used to fine-tune a FinBERT model for classifying toxic comments, evaluating the effectiveness of machine-translated data in enhancing model performance. It is also utilized to train a KenLM model for Finnish, specifically to remove lines with high perplexity from documents, thereby improving text quality and coherence. These applications focus on leveraging the dataset to enhance model accuracy and text processing in the Finnish language.	
Finnish Corpus	cited_context	Norssi High School Alumni on the Semantic Web	https://www.semanticscholar.org/paper/b2e9fdc30d3c9c62b0be8175672a81922f6a661e (2020)	https://doi.org/10.5617/dhnbpub.11199 (2020)	The 'Norssi High School Alumni on the Semantic Web' dataset is primarily used to extract and study historical Finnish academic names from 1640 to 1899, focusing on Latin-based names and their linguistic and cultural significance. It is employed to create and enhance ontologies, linking biographical and family names to historical Finnish academic figures. The dataset enriches linked open data services by integrating and connecting biographical information, improving the accuracy and richness of data on Finnish academic people.	
Finnish Corpus	citing_context	dataset of Ehara et al. (2010)	https://www.semanticscholar.org/paper/ce4ed291f4039730746bfcc04de2379be049f123 (2022)	https://doi.org/10.1145/1719970.1719978 (2010)	The dataset of Ehara et al. (2010) is used as the foundational resource for designing TallVocabL2Fi, a system that provides personalized reading support for second-language web documents. This system leverages collective intelligence to enhance vocabulary acquisition and comprehension. The dataset's specific characteristics enable the development of tailored educational tools, focusing on improving the reading experience for second-language learners by integrating user-generated data and feedback.	
Finnish Corpus	citing_context	dataset of Venekoski and Vankka (2017)	https://doi.org/10.48550/arXiv.2311.05640 (2023)	https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f (2013)	The dataset of Venekoski and Vankka (2017) is used to create 130 examples for Finnish language research, focusing on linguistic patterns and structures. It is translated to Finnish and utilized to explore the application of word and phrase representations in the Finnish language, enabling detailed analysis of linguistic features and their representations.	
Finnish Corpus	cited_context | citing_context	OpenSubtitles	https://doi.org/10.48550/arXiv.2310.07081 (2023), https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://www.semanticscholar.org/paper/6a9e5377bbf28b86a1dd8cd5802998c949a34ff3 (2018), https://www.semanticscholar.org/paper/4df321947a2ac4365584a01d78a780913b171cf5 (2016)	The OpenSubtitles dataset is used to create in-domain test sets and build datastores for approximate kNN search in kNN-MT, leveraging its large, noisy parallel corpora. It is also utilized to train models on parallel corpora for multiple language pairs, enhancing sentence alignments in noisy data. This dataset enables researchers to evaluate model performance and improve translation quality in specific domains.; The Opensubtitles corpus is used for cross-lingual translation, specifically translating English sentences into Finnish, French, and Italian. Research focuses on evaluating the performance and quality of these translations. The dataset's large scale and diverse content enable robust testing and improvement of translation models across multiple languages.	Opensubtitles corpus
Finnish Corpus	cited_context	OpenSubtitles1	https://www.semanticscholar.org/paper/1b4b539ca43efce3be9d3a3a9ba1cd6aeec8690e (2021)	https://www.semanticscholar.org/paper/2bb12c4c4ea47e0c564d1568f075e5c3c3f24de4 (2021)	The OpenSubtitles1 dataset is used in research to study paraphrase pairs in Finnish, focusing on manual selection and quality. It serves as a source of data for training and evaluation, particularly for Finnish language content. The dataset supports research into natural language processing, linguistic analysis, and linguistic variation, leveraging its large corpus of subtitles.	
Finnish Corpus	citing_context	OPUS	https://doi.org/10.1017/S1351324923000086 (2023)	https://www.semanticscholar.org/paper/dc66194b0b977aeabafa2f37f7fa8a455a6f2642 (2015)	The OPUS dataset is used for collecting candidate sentences to generate hard negatives in bootstrapping experiments, both for Finnish and multilingual text. It is also utilized alongside the Finnish Internet Parsebank to mine correct target sentences for paraphrase mining, enhancing sentence pool diversity and scale. Additionally, it samples 7500 sentence pairs for linguistic analysis, contributing to balanced datasets for Finnish language processing and large-scale linguistic analysis.	
Finnish Corpus	citing_context	OPUS-100	https://doi.org/10.48550/arXiv.2506.00748 (2025)	https://www.semanticscholar.org/paper/06128690bf2596d0097b7cf0eb1f2cd87de6ec66	The OPUS-100 dataset is used to investigate the trade-off between specialization in pronoun disambiguation and overall translation performance in massively multilingual neural machine translation models. This involves analyzing how these models handle pronoun disambiguation while maintaining general translation quality, leveraging the dataset's multilingual and specialized content.	
Finnish Corpus	cited_context	OPUS Movie Subtitle parallel corpus	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The OPUS Movie Subtitle parallel corpus is used for fine-grained sentiment analysis and the study of sentiment preservation in translated texts, particularly in movie subtitles. It supports cross-lingual sentiment analysis and transfer learning, aiding in the alignment of emotional content across languages. The dataset is also utilized for training and evaluating natural language processing models, focusing on English and Finnish languages for translation and language modeling tasks.	
Finnish Corpus	cited_context	Opusparcus	https://www.semanticscholar.org/paper/1b4b539ca43efce3be9d3a3a9ba1cd6aeec8690e (2021), https://doi.org/10.18653/v1/2020.semeval-1.18 (2020)	https://doi.org/10.1075/BTL.48.08ESK (2004)	The Opusparcus dataset is used in various linguistic and computational research areas, focusing on Finnish. It is employed to study untypical frequencies in translated Finnish literary texts using corpus-based methods. The dataset supports the identification and alignment of paraphrase pairs with English source segments, enhancing automatic alignment techniques. It also investigates subject changes and linguistic patterns in translations between French and Finnish. Additionally, it supplements lexical resources, such as synonym dictionaries, and aids in the construction and evaluation of sentential paraphrases within a multilingual context. The dataset is used to train models for linguistic analysis, particularly for syntactic structures in Finnish, as part of the Universal Dependencies project. It provides high-quality paraphrases for training and multilingual data for projects like GWSC.	
Finnish Corpus	citing_context	ROOTS	https://doi.org/10.48550/arXiv.2311.05640 (2023)	https://doi.org/10.18653/v1/2023.acl-long.891 (2023)	The ROOTS dataset is used to analyze the contamination of Finnish data within multilingual datasets. Researchers employ contamination analysis methods to measure the presence and purity of Finnish content. This dataset enables specific research questions related to linguistic data integrity and the impact of multilingual contamination on Finnish language datasets.	
Finnish Corpus	cited_context | citing_context	Semantic Finlex	https://www.semanticscholar.org/paper/0ad7fa9d1db5a92666727530ecf3c80ab1bd8bcf (2020), https://doi.org/10.3233/FAIA190023 (2018)	https://doi.org/10.3233/FAIA190023 (2018), https://www.semanticscholar.org/paper/1c2bb04db39e7161b0ca0c754378bd2bb8d5e389 (2016)	The Semantic Finlex dataset is used to transform and publish Finnish legislation and case law as Linked Open Data on the web. This involves methodologies for data transformation and semantic web technologies. The dataset enables researchers to enhance the accessibility and interoperability of legal texts, facilitating research in legal informatics and open government data initiatives.; The Semantic Finlex dataset is used to store and provide legal terminology, enhancing the understanding of legal texts through linked data in the CORE contextual reader. This application focuses on improving the accessibility and comprehension of legal documents by integrating semantic web technologies. The dataset's linked data structure enables researchers to link and contextualize legal terms, facilitating more nuanced analysis and interpretation of legal content.	
Finnish Corpus	cited_context	TDT corpus	https://www.semanticscholar.org/paper/dc66194b0b977aeabafa2f37f7fa8a455a6f2642 (2015)	https://www.semanticscholar.org/paper/b519439f0777142c02d675647ca7c04d36cbc41b (2013)	The TDT corpus is used to develop and adapt the extended layer prediction method for the Universal Dependencies (UD) scheme, specifically focusing on dependency parsing and syntactic structure in Finnish text. This dataset enables researchers to enhance parsing accuracy and refine syntactic analysis methodologies, contributing to advancements in natural language processing for the Finnish language.	
Finnish Corpus	cited_context	textual customer feedback from Telia Finland	https://doi.org/10.1007/s10579-023-09644-5 (2023)	https://www.semanticscholar.org/paper/cff1f34a1a17ff871751d79017467b7a3718b80c (2019)	The textual customer feedback from Telia Finland dataset is used to analyze customer sentiment in the Finnish language. Researchers compare machine learning, representation learning, and deep learning models to classify sentiment. This dataset enables the evaluation of different methodologies for sentiment analysis, focusing on their effectiveness in understanding customer feedback in Finnish.	
Finnish Corpus	cited_context | citing_context	Turku Dependency Treebank	https://www.semanticscholar.org/paper/477d66dcd2c08243dcc69822d6da7ec06393773a (2019), https://doi.org/10.3233/978-1-61499-442-8-184 (2014), https://www.semanticscholar.org/paper/aeb9a63bd0a654b685bc9dff898865ce39447fdd (2020)	https://doi.org/10.1007/s10579-013-9244-1 (2013)	The Turku Dependency Treebank is used to construct and evaluate dependency parses in Finnish, focusing on the performance and reliability of human annotators. It facilitates comparisons between model performance and human evaluations, particularly in the context of probabilistic cues in natural speech corpora and double-annotated final annotations. This dataset enables researchers to assess the accuracy and consistency of linguistic annotations and models.; The Turku Dependency Treebank is primarily used for Finnish language research, focusing on syntactic analysis and dependency parsing. It supports the training and evaluation of models such as BERT and Udify, comparing their performance in POS tagging and dependency parsing. The dataset aids in assessing annotation quality and consistency, contributing to the Universal Dependencies collection. It is also used to train and evaluate tools for Finnish language processing, enhancing parsing accuracy and morphological analysis.	
Finnish Corpus	citing_context	Turku NER corpus	https://doi.org/10.18653/V1/2020.COLING-MAIN.575 (2020)	https://www.semanticscholar.org/paper/aeb9a63bd0a654b685bc9dff898865ce39447fdd (2020)	The Turku NER corpus is used for named entity recognition tasks in the Finnish language, specifically to replace names and locations in Finnish data. This dataset enables researchers to enhance the accuracy and robustness of NER models by providing a rich set of annotated entities, facilitating the development and evaluation of natural language processing systems tailored for Finnish.	
Finnish Corpus	citing_context	Turku Paraphrase Corpus	https://doi.org/10.1017/S1351324923000086 (2023)	https://doi.org/10.18653/v1/D19-1410 (2019)	The Turku Paraphrase Corpus is used to train and evaluate paraphrase detection models, particularly SBERT and BERT. It enables researchers to measure the models' performance in ranking paraphrase labels and distinguishing between positive and negative examples. This dataset supports the development and assessment of natural language processing techniques focused on identifying semantic equivalence in text.	
Finnish Corpus	cited_context	UDer (Universal Derivations)	https://doi.org/10.18653/v1/2021.sigmorphon-1.5 (2021)	https://doi.org/10.14712/00326585.003 (2020)	The UDer (Universal Derivations) dataset is used to study derivational morphology across 20 languages, including Finnish, by integrating harmonized word-formation resources. It supports linguistic analysis and computational modeling of morphological processes, enabling researchers to explore word formation and morphological patterns systematically. The dataset's cross-linguistic scope facilitates comparative studies and enhances understanding of derivational processes.	
Finnish Corpus	cited_context	UD Finnish	https://www.semanticscholar.org/paper/dc66194b0b977aeabafa2f37f7fa8a455a6f2642 (2015)	https://www.semanticscholar.org/paper/51cfde9bbc25a4cff1d5666815674c83886d933e (2010)	The UD Finnish dataset is used to train a graph-based dependency parser, specifically aimed at enhancing the accuracy and speed of parsing Finnish language structures. This dataset enables researchers to develop more efficient parsing algorithms, addressing the complexities of the Finnish language through its rich syntactic annotations.	
Finnish Corpus	cited_context	UniMorph	https://doi.org/10.18653/v1/2021.sigmorphon-1.5 (2021)	https://www.semanticscholar.org/paper/5117f10bff3d50aba4619d39be528bf0027e8d09 (2012)	The UniMorph dataset is used as a gold standard for morphological analysis in the NLP community, covering 133 languages, including Finnish. It provides precise and complete linguistic data, enabling researchers to evaluate feature tag compatibility across multiple languages and to assess morphological and syntactic features. The dataset supports the evaluation of systems like UniMorph 2.0 and MorphyNet, ensuring high-quality linguistic annotations and facilitating cross-linguistic comparisons.	
Finnish Corpus	cited_context	Universal Dependencies 5	https://doi.org/10.18653/v1/2021.sigmorphon-1.5 (2021)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The Universal Dependencies 5 dataset is used to provide ground truth for morphological analysis, specifically focusing on inflections of words across a multilingual corpus of hand-annotated sentences. This dataset enables researchers to validate and improve algorithms for morphological parsing and analysis, enhancing the accuracy of natural language processing systems across multiple languages.	
Finnish Corpus	cited_context	Universal Dependencies for Finnish	https://doi.org/10.18653/v1/2021.sigmorphon-1.5 (2021)	https://www.semanticscholar.org/paper/a2ebc3e5026111d835971cb610a6bdca05a215e6 (2019)	The Universal Dependencies for Finnish dataset is used to evaluate morphological and syntactic models like UniMorph 2.0 and MorphyNet, focusing on feature tag compatibility and derivational data. Research specifically investigates Finnish language processing by comparing these features across multiple languages, enhancing understanding and improving computational models for Finnish.	
Finnish Corpus	cited_context	universal dependency tree-banks	https://www.semanticscholar.org/paper/dc66194b0b977aeabafa2f37f7fa8a455a6f2642 (2015)	https://www.semanticscholar.org/paper/16b26c3bfb3feef8f2d1889b4e76c15e3f5b6fb7 (2014)	The Universal Dependency Tree-banks dataset provides linguistic annotations for multiple languages, focusing on syntactic structures and dependencies. It supports cross-linguistic research and the development of universal dependency frameworks and dependency parsing models. This dataset enables researchers to develop and evaluate cross-linguistic typologies, enhancing the understanding of syntactic variations across languages.	
Finnish Corpus	citing_context	word frequency data	https://doi.org/10.1017/S0332586520000013 (2020)	https://doi.org/10.1075/btl.48.07mau (2004)	The word frequency data dataset is used to analyze and compare word frequency differences between Finnish-to-English translations and their corresponding source language-specific subcorpora. This involves examining linguistic interference patterns, employing comparative analysis methods to identify how translation processes affect word usage. The dataset enables researchers to explore specific linguistic phenomena, such as the impact of source language structures on translated texts, providing insights into translation dynamics and language transfer effects.	
Ghanaian Pidgin English Corpus	citing_context	GhaP	https://doi.org/10.1177/13670069211019126 (2021)	https://www.semanticscholar.org/paper/3ee12dec281cc490e6b626076a2fc8402a7dcd24 (1958)	The GhaP dataset is used to analyze the polysemy of 'make-cop.nfact' in Ghanaian Pidgin English, comparing its usage with other languages in the corpus. This involves examining linguistic data to understand semantic variations and cross-linguistic similarities, providing insights into the structure and function of Ghanaian Pidgin English.	
Ghanaian Pidgin English Corpus	cited_context	GloVe	https://www.semanticscholar.org/paper/880c3a51450b9a4c9f3c0de44352b2ae0f8a3e63 (2019)	https://doi.org/10.3115/v1/D14-1162 (2014)	The GloVe dataset is used to initialize word embeddings for Pidgin, providing global context to word vectors before fine-tuning on a Pidgin corpus. This approach helps in enhancing the semantic richness of the embeddings, which are then utilized in natural language processing tasks specific to the Pidgin language.	
Ghanaian Pidgin English Corpus	cited_context	JW300	https://www.semanticscholar.org/paper/880c3a51450b9a4c9f3c0de44352b2ae0f8a3e63 (2019)	https://doi.org/10.18653/v1/P16-1162 (2015)	The JW300 dataset is used to establish a supervised baseline for evaluating models in machine translation, specifically focusing on parallel data for Ghanaian Pidgin English. This dataset provides essential parallel text pairs that enable researchers to train and test machine translation models, facilitating the development and assessment of translation accuracy and performance in this language.	
Gulf Arabic Corpus	citing_context	15-hour QA corpus	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://www.semanticscholar.org/paper/f315ecd89131e4c013840964c1506ab29188b721 (2014)	The 15-hour QA corpus is used to develop a speech recognition system for Gulf Arabic, specifically focusing on TV series and talk show programs. This dataset enhances recognition accuracy by providing a rich source of spoken language data, enabling researchers to train models on diverse and contextually relevant content.	
Gulf Arabic Corpus	citing_context	Arabic Expressive-Receptive Vocabulary Test	https://doi.org/10.1080/21622965.2019.1596113 (2019)	https://doi.org/10.1007/978-3-319-62884-4_4 (2018)	The 'Arabic Expressive-Receptive Vocabulary Test' dataset is mentioned in research citations but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular study.	
Gulf Arabic Corpus	cited_context	CALIMA GLF	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)	https://www.semanticscholar.org/paper/fcc5f168c5466e2cc788bbdc2fdac743b3b6367e (2018)	The CALIMA GLF dataset is used to provide annotated dialectal Arabic data, specifically for Gulf Arabic dialects, supporting morphological analysis and spelling correction. It extends the coverage of MADAMIRA's databases, enhancing accuracy in morphological analysis and disambiguation of Gulf Arabic dialects. This dataset enables researchers to improve the precision and reliability of linguistic tools and analyses focused on Gulf Arabic.	
Gulf Arabic Corpus	citing_context	Callhome	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://doi.org/10.21437/Interspeech.2016-1541 (2016)	The Callhome dataset is primarily used for developing and refining transcription methodologies for Gulf Arabic language data, focusing on linguistic features, code-switching, and dialectal orthography. It serves as a foundational reference for general and conversational speech transcription rules, aiding in the creation of consistent and accurate transcription standards. The dataset supports research on linguistic challenges in spoken language, particularly in bilingual contexts, and contributes to the development of robust speech-based systems through collaborative learning.	
Gulf Arabic Corpus	cited_context	clinical database	https://doi.org/10.1017/S0305000912000499 (2012)		The clinical database is used to compare linguistic performance metrics (MLU, errors per utterance, lexical diversity) of participants with Specific Language Impairment (SLI) to age-matched typically developing Kuwaiti Arabic-speaking children. This involves analyzing speech data to identify and quantify differences in language abilities, enabling researchers to better understand the linguistic challenges faced by children with SLI.	
Gulf Arabic Corpus	citing_context	EGY corpus	https://www.semanticscholar.org/paper/061ff64f071ffded321bf78d198d19e582f79ed2 (2016)		The EGY corpus is used to collect and annotate Gulf Arabic language data, focusing on linguistic features and dialectal variations in both spoken and written forms. This dataset enables researchers to analyze and document the specific characteristics of Gulf Arabic, supporting studies on linguistic diversity and dialectology.	
Gulf Arabic Corpus	cited_context	Emirati Arabic Corpus (EAC)	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)		The Emirati Arabic Corpus (EAC) is used to study phonological, morphosyntactic, and linguistic features of Emirati Arabic through annotated data. It supports corpus-based analysis of both spoken and written texts, facilitating research into language acquisition, particularly developmental patterns and linguistic structures in child speech. The dataset's annotations enable detailed linguistic analysis, enhancing understanding of Emirati Arabic.	
Gulf Arabic Corpus	citing_context	Emirati Arabic Language Acquisition Corpus (EMALAC)	https://www.semanticscholar.org/paper/061ff64f071ffded321bf78d198d19e582f79ed2 (2016)	https://doi.org/10.4324/9781315805672 (2000)	The Emirati Arabic Language Acquisition Corpus (EMALAC) is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how EMALAC is used in research based on the provided evidence.	
Gulf Arabic Corpus	citing_context	ESCWA corpus	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://doi.org/10.21437/interspeech.2021-1809 (2021)	The ESCWA corpus is used to develop multilingual strategies for dialectal code-switching Arabic Automatic Speech Recognition (ASR), specifically focusing on UN meeting transcripts. This dataset enables researchers to improve recognition accuracy by leveraging its multilingual and dialectal content, addressing the challenges of code-switching in Arabic speech.	
Gulf Arabic Corpus	cited_context	Gulf Arabic Corpus	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)	https://www.semanticscholar.org/paper/a7b24d1d636321b3e57031b54f0511774d8051d8 (2016)	The Gulf Arabic Corpus is used to study linguistic patterns and structures in Gulf Arabic, particularly focusing on phonological and morphosyntactic phenomena in Emirati Arabic. It supports large-scale linguistic analysis through a corpus of over 100 million words, including a manually annotated subset of about 200,000 words. This dataset enables detailed linguistic annotation and analysis, facilitating research into Gulf Arabic varieties and resource development.	
Gulf Arabic Corpus	citing_context	Gumar corpus	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)	The Gumar corpus is used to collect and analyze Gulf Arabic language data from forum novels, with a focus on extending subsets through morphological annotations and orthographic modifications following CODA guidelines. It is also utilized in large-scale Arabic dialect projects, though specific details on its application in Gulf Arabic research are limited. The dataset's annotated content supports linguistic analysis and dialectal studies.	
Gulf Arabic Corpus	citing_context	MADAR Corpus	https://doi.org/10.48550/arXiv.2312.06926 (2023)	https://www.semanticscholar.org/paper/f315ecd89131e4c013840964c1506ab29188b721 (2014)	The MADAR Corpus is used for machine translation from English to various Arabic dialects, including Gulf, Levantine, Egyptian, and North African Arabic. It emphasizes cross-dialectal comparisons and the unique linguistic characteristics of each dialect. The dataset is employed to evaluate translation accuracy and fluency, addressing the challenges of translating into multiple dialects. This enables researchers to develop and refine machine translation systems tailored to the complexities of dialectal Arabic.	
Gulf Arabic Corpus	citing_context	MGB-2	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://doi.org/10.1109/SLT.2016.7846277 (2016)	The MGB-2 dataset is used for Arabic multi-dialect broadcast media recognition, specifically focusing on dialectal Arabic speech from Aljazeera Arabic TV channel. It contains 1,200 hours of audio data and is employed to develop and test speech recognition systems capable of handling the variability of Arabic dialects. This dataset enables researchers to address challenges in recognizing and processing spoken dialectal Arabic, enhancing the accuracy and robustness of speech recognition technologies.	
Gulf Arabic Corpus	citing_context	object and action naming battery for Saudi/Gulf Arabic	https://doi.org/10.1080/02687038.2020.1765303 (2020)	https://doi.org/10.3389/CONF.FPSYG.2016.68.00026 (2016)	The 'object and action naming battery for Saudi/Gulf Arabic' is used to assess various aspects of aphasia in Qatari/Gulf Arabic speakers. It provides a comprehensive tool for linguistic and cognitive evaluations, enabling researchers to conduct detailed assessments of language disorders. The dataset's specific focus on Gulf Arabic makes it particularly useful for studies targeting this linguistic region.	
Gulf Arabic Corpus	citing_context	QAraC	https://doi.org/10.48550/arXiv.2312.06926 (2023)	https://doi.org/10.1184/R1/6373130.V1 (2014)	The QAraC dataset is used for English-Gulf, Levantine, and Egyptian-North African machine translation, supporting multidialectal Arabic research. It is specifically designed to facilitate Gulf Arabic dialectal studies, providing a rich resource for linguistic and computational analyses. This dataset enables researchers to develop and evaluate machine translation models across multiple Arabic dialects, enhancing the accuracy and applicability of these systems in diverse linguistic contexts.	
Gulf Arabic Corpus	citing_context	SDR Speech-based Collaborative Learning Corpus (SB-CLC)	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://www.semanticscholar.org/paper/9e4a6174e02ed2f0877ff93298cd76c02858fe95 (2020)	The SDR Speech-based Collaborative Learning Corpus (SB-CLC) is used for developing and refining speech transcription rules and models, particularly in the context of general and conversational speech. It serves as a foundational reference for speech processing and supports collaborative learning in speech-based systems, enhancing the robustness of transcription models.	
Gulf Arabic Corpus	citing_context	speech containing Saudi Arabic-English code-switching	https://doi.org/10.48550/arXiv.2403.18182 (2024)	https://doi.org/10.5539/IJEL.V5N5P99 (2015)	The dataset 'speech containing Saudi Arabic-English code-switching' is mentioned in research citations but lacks detailed descriptions of its usage. Therefore, there is no explicit evidence to describe specific methodologies, research questions, or applications. The dataset's relevance and how it enables research remain unspecified in the provided contexts.	
Gulf Arabic Corpus	citing_context	Tharwa multi-dialectal lexicon	https://www.semanticscholar.org/paper/061ff64f071ffded321bf78d198d19e582f79ed2 (2016)	https://www.semanticscholar.org/paper/15ab21643e74de81c5c36895eb7a1874e971d445 (2011)	The Tharwa multi-dialectal lexicon is used to enhance lexical resources for Gulf Arabic, providing a comprehensive list of dialectal terms and their meanings. It supports research into informal Arabic with high dialectal content, particularly in online commentary, by analyzing linguistic features of user-generated content. The dataset is also employed to compare and contrast Gulf Arabic with other Arabic dialects, aiding in the development of dialectal language models.	
Hebrew Corpus	citing_context	al-Ṯurayyā Project	https://doi.org/10.1007/s10579-025-09812-9 (2025)		The al-Ṯurayyā Project dataset is used to source an Arabic dataset, focusing on linguistic features and structures relevant to Semitic languages, including Hebrew. It is employed to train and evaluate syntactic parsers for modern Hebrew texts, emphasizing morphological and syntactic structures. Additionally, the dataset supports historical and linguistic research by compiling and standardizing names from medieval Middle Eastern sources.	
Hebrew Corpus	citing_context	annotated comments written in Hebrew	https://www.semanticscholar.org/paper/d328662303ebbd50a9532ed33eb955ebdf621693 (2022)	https://doi.org/10.18653/v1/S19-2010 (2019)	The annotated comments written in Hebrew dataset is used to identify and categorize offensive language in social media, specifically focusing on Hebrew content. Researchers employ this dataset to develop and evaluate machine learning models for detecting offensive language, leveraging its annotated nature to train and test these models effectively.	
Hebrew Corpus	citing_context	annotated Facebook comments written in Hebrew	https://www.semanticscholar.org/paper/d328662303ebbd50a9532ed33eb955ebdf621693 (2022)		The annotated Facebook comments dataset in Hebrew is used to study linguistic features, patterns, and sentiment in online comments. It supports the development and evaluation of models for detecting offensive language and analyzing linguistic and social aspects. The dataset addresses the shortage of Hebrew resources, enhancing model performance and accuracy in processing Hebrew text. It also focuses on annotation quality and public availability for further research.	
Hebrew Corpus	citing_context	annotated Knesset data	https://doi.org/10.1007/s10579-025-09833-4 (2024)	https://doi.org/10.1007/s10579-020-09510-8 (2020)	The annotated Knesset data dataset is used to evaluate the impact of including or excluding Knesset data in training models. Researchers plot learning curves to assess performance differences, focusing on how the inclusion of this dataset affects model performance. This approach helps in understanding the dataset's contribution to model accuracy and reliability.	
Hebrew Corpus	citing_context	Ben-Yehuda corpus	https://doi.org/10.1145/3479159 (2022)	https://doi.org/10.3115/v1/D14-1179 (2014)	The Ben-Yehuda corpus is used for training LSTM-based neural networks, specifically incorporating period-specific artificial OCR errors to simulate historical text conditions. This approach helps researchers address challenges in processing and understanding historical documents, enhancing the accuracy of text recognition and analysis in historical contexts.	
Hebrew Corpus	citing_context	BGU-version of the lexicon	https://doi.org/10.18653/V1/2021.CLPSYCH-1.6 (2021)	https://www.semanticscholar.org/paper/d4f86a14dfcafd6008e13a5ffdaa6a2e06c89a11 (2016)	The BGU-version of the lexicon is used as a lexical resource for the YAP Hebrew parser, providing essential morphological information to support data-driven morphological analysis and disambiguation. This dataset enhances linguistic processing by integrating detailed morphological data, enabling more accurate parsing and disambiguation of Hebrew text. It is specifically utilized to improve the performance of the YAP parser in handling complex Hebrew morphology.	
Hebrew Corpus	citing_context	Biblia Hebraica Stuttgartensia (BHS)	https://www.semanticscholar.org/paper/63ffa6a574bc494b8b5c2f913955b168986beafa (2020)	https://doi.org/10.1177/0142064X8100401203 (1981)	The Biblia Hebraica Stuttgartensia (BHS) dataset is primarily used to obtain the Hebrew Bible text for source texts in research projects, emphasizing the accuracy and scholarly reliability of the Hebrew text. It serves as a foundational resource for textual analysis and critical studies, enabling researchers to work with a trusted version of the Hebrew Bible.	
Hebrew Corpus	citing_context	BMC corpus	https://www.semanticscholar.org/paper/f57f45978ec082d6c35e27f9c17143d4acef60b7 (2021)		The BMC corpus is primarily used for Hebrew language research, focusing on morphological, syntactic, and named entity recognition (NER) tasks. It supports both token-based and morpheme-based NER evaluations, enhancing entity identification accuracy. The dataset also facilitates sentiment analysis and provides syntactic annotations, enabling comprehensive linguistic and textual studies, including the analysis of Hebrew Wikipedia content.	
Hebrew Corpus	citing_context	BYP	https://doi.org/10.1145/3479159 (2022)	https://doi.org/10.1007/978-3-540-78135-6_53 (2008)	The BYP dataset is used to study the impact of artificially generated errors on network performance. Research focuses on specific questions, such as the effect of a 20% noise ratio. Methodology involves introducing controlled errors to assess performance degradation, enabling analysis of network robustness under error conditions.	
Hebrew Corpus	citing_context	Camoni corpus	https://doi.org/10.18653/v1/2022.findings-acl.266 (2022)	https://doi.org/10.1093/jamia/ocaa150 (2020)	The Camoni corpus is primarily used for evaluating and improving entity linking performance in noisy text from online health communities, with a focus on both Hebrew and English languages. It is employed to compare performance against baseline approaches, measure F1 score gains, and test systems like MDTEL for cross-lingual and transliteration entity linking. This dataset enables researchers to address specific challenges in processing and linking entities in multilingual, noisy online health community data.	
Hebrew Corpus	citing_context	FLEURS	https://doi.org/10.21437/interspeech.2023-430 (2023), https://doi.org/10.48550/arXiv.2407.07566 (2024)	https://doi.org/10.1109/SLT54892.2023.10023141 (2022)	The FLEURS dataset is used to evaluate few-shot learning of universal speech representations, particularly for Hebrew sentences read from Wikipedia by multiple speakers. It focuses on assessing model performance in few-shot scenarios, enabling researchers to understand how well these models generalize to new data with limited training examples.	
Hebrew Corpus	citing_context	free association norms in Hebrew	https://doi.org/10.1371/journal.pone.0023912 (2011)		The 'free association norms in Hebrew' dataset is used to analyze semantic relationships in Hebrew, focusing on free association norms to understand lexical connections and cognitive processes. Researchers employ this dataset to explore how words are associated in the Hebrew language, providing insights into the mental lexicon and cognitive structures underlying language use. This dataset enables detailed examination of lexical networks and the cognitive mechanisms involved in word association.	
Hebrew Corpus	citing_context	French Street Name Signs (FSNS)	https://doi.org/10.1109/WACV45572.2020.9093597 (2020)	https://doi.org/10.1007/978-3-319-46604-0_30 (2016)	The French Street Name Signs (FSNS) dataset is used in multilingual research settings, particularly to enhance model performance in Hebrew language tasks. It is leveraged for multi-task learning, where the dataset's public availability facilitates the improvement of models through shared tasks, demonstrating the effectiveness of the approach in a multilingual context.	
Hebrew Corpus	cited_context | citing_context	Google Books	https://doi.org/10.1007/s10579-019-09458-4 (2019)	https://www.semanticscholar.org/paper/ba3f84c45807ef82ee096868159974066600761d (2012)	The Google Books dataset is used to compile a massive corpus of Hebrew texts from mid-nineteenth-century publications. Researchers analyze linguistic features and historical trends within these texts, employing corpus linguistics methods to explore changes over time. This dataset enables detailed historical and linguistic analysis by providing access to a large volume of period-specific textual data.; The Google Books dataset is used to compile a large corpus of Hebrew texts from mid-nineteenth century publications, enabling historical linguistic analysis. Researchers employ this dataset to study the evolution of the Hebrew language over time, leveraging its extensive collection of period-specific texts. This corpus supports in-depth examination of linguistic changes and patterns, providing valuable insights into historical language use.	
Hebrew Corpus	cited_context	Goralnik Screening Test for Hebrew	https://doi.org/10.3389/fpsyg.2018.01953 (2018)		The Goralnik Screening Test for Hebrew is used to assess children's vocabulary size and overall language performance in Hebrew, particularly in bilingual contexts. It employs standardized tasks to screen and evaluate language skills, focusing on language development and specific linguistic contexts. This dataset enables researchers to conduct detailed evaluations of children's language abilities, supporting studies on bilingualism and language acquisition.	
Hebrew Corpus	cited_context	Haifa Lexicon of Contemporary Hebrew	https://doi.org/10.1007/s10579-007-9050-8 (2008)	https://www.semanticscholar.org/paper/370c1787954a0134e9b3833fe52ecd75b62d4ffe (2006)	The Haifa Lexicon of Contemporary Hebrew is used to provide a broad-coverage lexicon of over 22,000 entries for computational linguistic research and applications. It serves as a foundational resource, enabling researchers to develop and test algorithms and models that require a comprehensive understanding of contemporary Hebrew vocabulary. This dataset supports the creation of linguistic tools and resources, enhancing the accuracy and breadth of computational analyses in Hebrew language processing.	
Hebrew Corpus	cited_context	HebDB	https://doi.org/10.48550/arXiv.2410.21502 (2024)	https://doi.org/10.48550/arXiv.2407.07566 (2024)	The HebDB dataset is used for Hebrew speech processing, specifically to train and evaluate both weakly supervised and standard models. It contains approximately 4500 hours of data, enabling researchers to develop and test speech recognition systems. This dataset facilitates advancements in Hebrew language technology by providing a large-scale resource for model training and evaluation.	
Hebrew Corpus	citing_context	Heb-FDA-2	https://doi.org/10.1111/1460-6984.12737 (2022)	https://doi.org/10.2307/2285908 (1967)	The Heb-FDA-2 dataset is used to measure reliability in Hebrew language sentence subtests. Researchers compare it with a comparison sentence task to assess statistical significance. This dataset enables the evaluation of linguistic tasks and helps in understanding the reliability of sentence subtests in Hebrew.	
Hebrew Corpus	cited_context	HEBLEX	https://doi.org/10.18653/v1/2020.findings-emnlp.391 (2020)	https://doi.org/10.3115/1220175.1220259 (2006)	The HEBLEX dataset is used for Hebrew morphological disambiguation, employing a morpheme-based Hidden Markov Model (HMM) for unsupervised learning. This approach facilitates the analysis of Hebrew language structures, enabling researchers to address specific challenges in morphological disambiguation and improve the accuracy of linguistic models. The dataset's morpheme-level annotations are crucial for training and evaluating these models.	
Hebrew Corpus	citing_context	Hebrew PropBank	https://www.semanticscholar.org/paper/e65e3fd8069ce1fc19c382411711ebaf837f244a (2020)	https://doi.org/10.1162/0891201053630264 (2005)	The Hebrew PropBank dataset is used for semantic role labeling in Hebrew, providing annotated examples for training and evaluation. It supports cross-lingual NLP tasks by integrating morphological, dependency syntax, and semantic role annotations in both English and Hebrew, enabling researchers to analyze and compare linguistic structures across languages.	
Hebrew Corpus	cited_context	Hebrew SPMRL treebank	https://doi.org/10.18653/v1/2020.findings-emnlp.391 (2020)	https://www.semanticscholar.org/paper/c79ce664a42d7867846ee468180bf9ad8c9e17b2 (2018)	The Hebrew SPMRL treebank is used to provide static lattice files for the CoNLL18 UD shared task, focusing on universal morphological lattices for dependency parsing. It is also utilized to evaluate PtrNetMD on joint segmentation-and-tagging tasks, where its performance is compared against state-of-the-art results using aligned mset scores. This dataset enables researchers to benchmark and improve models for morphological analysis and dependency parsing in Hebrew.	
Hebrew Corpus	cited_context | citing_context	Hebrew Treebank	https://doi.org/10.48550/arXiv.2210.07873 (2022), https://doi.org/10.1162/tacl_a_00404 (2020)	https://doi.org/10.18653/v1/W16-1709 (2016), https://doi.org/10.1162/tacl_a_00253 (2019)	The Hebrew Treebank is used to evaluate Hebrew NLP performance, particularly in assessing degradation outside its represented genre. It has been converted to the Universal Dependencies framework to standardize syntactic annotations, despite following outdated UD V2 guidelines. The dataset is also utilized for evaluating legacy tokenization methods and creating a dependency treebank, focusing on syntactic structures and annotations in Hebrew.; The Hebrew Treebank is used for developing and evaluating natural language processing (NLP) models, particularly in Hebrew Named Entity Recognition (NER) and morpho-syntactic parsing. It provides 6,143 morpho-syntactically analyzed sentences from the HAARETZ corpus, enabling the creation of token-level and morpheme-level annotations. Researchers use it to pre-train embeddings and assess NER modeling strategies, focusing on the challenges of tokenization and morphological analysis in Modern Hebrew.	
Hebrew Corpus	cited_context	Hebrew treebank Ver.2	https://www.semanticscholar.org/paper/92de46eae233b296d242379785514943e75dcced (2011)	https://www.semanticscholar.org/paper/dc0a309c8cadb3397f1b2a42cf110ae5df08c00c (2008)	The Hebrew treebank Ver.2 is used to conduct experiments on morpho-syntactic dependencies in Modern Hebrew. Researchers convert the tagset to align with the MILA morphological analyzer, enabling detailed analysis of linguistic structures. This dataset facilitates the study of syntactic relationships and morphological tagging, enhancing the accuracy and consistency of linguistic annotations in Modern Hebrew.	
Hebrew Corpus	cited_context	Hebrew Unified-SD version	https://doi.org/10.1162/tacl_a_00253 (2019)	https://www.semanticscholar.org/paper/b7a0d10ad580366c91a5f391fcd78a94c8c75747 (2013)	The Hebrew Unified-SD version dataset is used to evaluate morpho-syntactic parsing in Modern Hebrew, specifically focusing on dependency structures and linguistic annotations. It serves as the source for the SPMRL shared task, providing a unified morpho-syntactic scheme that facilitates consistent Hebrew language processing. This dataset enables researchers to assess and improve parsing accuracy and linguistic annotation quality in Modern Hebrew.	
Hebrew Corpus	citing_context	Hebrew Web MB-CDI WG questionnaire	https://doi.org/10.1017/S0305000921000179 (2021)		The Hebrew Web MB-CDI WG questionnaire is used to collect data from Hebrew-speaking parents regarding their toddlers' language development, specifically focusing on vocabulary and grammar acquisition. This dataset employs a survey methodology to gather detailed information on children's linguistic milestones. It enables researchers to address questions related to the developmental patterns and stages of language acquisition in young Hebrew speakers, providing insights into early language development.	
Hebrew Corpus	cited_context | citing_context	Hebrew Wikipedia	https://doi.org/10.18653/v1/2022.acl-long.4 (2022), https://doi.org/10.1093/jamia/ocaa150 (2020)		The Hebrew Wikipedia dataset is used for linguistic analysis and language model training, focusing on the content and structure of Hebrew articles. Researchers extract texts using the Attardi (2015) methodology and train models on a corpus of 3.8 million sentences, split into 80% training and 20% validation sets. This enables detailed linguistic studies and the development of robust Hebrew language models.; The Hebrew Wikipedia dataset is used for linguistic analysis and natural language processing tasks, focusing on the content and structure of Hebrew articles. It provides a large corpus of 3.8 million sentences, split into 80% training and 20% validation sets, enabling the training and evaluation of Hebrew language models. This dataset supports research in understanding Hebrew linguistic patterns and developing robust NLP models for the language.	
Hebrew Corpus	cited_context	Hebrew WordNet	https://doi.org/10.1007/s10579-007-9050-8 (2008)		The Hebrew WordNet dataset is used to analyze the structure and content of the Hebrew language, particularly focusing on the distribution and frequency of nouns and other parts of speech. It is also utilized to expand verb datasets by adding 3500 verbs through root and inflection base entries, enhancing the coverage of Hebrew verb paradigms. Additionally, the dataset supports the analysis of Hebrew lexical relations, examining semantic connections and word senses. This comprehensive approach enables detailed linguistic research and enhances the understanding of Hebrew morphology and semantics.	
Hebrew Corpus	cited_context	HeDC4 corpus	https://doi.org/10.48550/arXiv.2308.16687 (2023)	https://doi.org/10.48550/arXiv.2304.11077 (2023)	The HeDC4 corpus is used for refining and improving the quality and usability of a Hebrew language dataset through further cleaning processes. This involves methodologies focused on data refinement to enhance dataset integrity, supporting research that requires high-quality Hebrew language data.	
Hebrew Corpus	citing_context	HeGeL	https://doi.org/10.48550/arXiv.2406.03897 (2024)	https://doi.org/10.48550/arXiv.2307.00509 (2023)	The HeGeL dataset is used to evaluate text-based geolocation from Hebrew text, specifically focusing on the accuracy of location prediction. Researchers employ natural language processing techniques to analyze the dataset, addressing the research question of how effectively locations can be predicted from textual content. This dataset enables the assessment of geolocation models' performance in the context of Hebrew language data.	
Hebrew Corpus	cited_context | citing_context	HeQ	https://doi.org/10.18653/v1/2023.findings-emnlp.915 (2023), https://doi.org/10.48550/arXiv.2308.16687 (2023)	https://doi.org/10.18653/v1/D16-1264 (2016), https://doi.org/10.18653/v1/2023.findings-emnlp.915 (2023)	The HeQ dataset is used to develop a benchmark for machine reading comprehension in Hebrew, specifically designed to mirror the SQuAD format. It focuses on question answering tasks, enabling researchers to evaluate and improve models' ability to understand and respond to queries in the Hebrew language. This dataset facilitates the creation and testing of algorithms tailored for Hebrew, advancing natural language processing in this specific linguistic context.; The HeQ dataset is used to evaluate question answering models, specifically focusing on Hebrew reading comprehension. It contains 30,000 high-quality samples sourced from the GeekTime newsfeed and Wikipedia. Researchers employ this dataset to assess model performance in understanding and answering questions based on complex textual content, enhancing the evaluation of natural language processing techniques in the Hebrew language.	
Hebrew Corpus	cited_context | citing_context	historical corpus of medieval Judeo-Arabic	https://doi.org/10.1007/s10579-019-09458-4 (2019)	https://doi.org/10.1163/22134638-06021122 (2018)	The 'historical corpus of medieval Judeo-Arabic' dataset is primarily cited in research contexts but lacks detailed descriptions of its specific usage, methodology, or application in published studies. Therefore, there is insufficient evidence to provide a comprehensive account of how it is actually used in research.; The historical corpus of medieval Judeo-Arabic is used to study language contact between Hebrew and Judeo-Arabic during the Middle Ages. Researchers focus on the evolution of linguistic elements over time, employing a comparative analysis to understand how these languages influenced each other. This dataset enables detailed examination of linguistic changes and cultural interactions, providing insights into the historical development of these languages.	
Hebrew Corpus	citing_context	Historical Index of the Medieval Middle East	https://doi.org/10.1007/s10579-025-09812-9 (2025)	https://doi.org/10.1145/3486187.3490203 (2021)	The Historical Index of the Medieval Middle East is used to compile and standardize names of people and places from medieval Middle Eastern sources, supporting historical and linguistic research. It is also utilized to train and evaluate syntactic parsers for modern Hebrew, focusing on morphological and syntactic structures. This dual use enhances the accuracy and reliability of both historical and linguistic analyses.	
Hebrew Corpus	citing_context	historical Polish training dataset	https://doi.org/10.1145/3479159 (2022)	https://doi.org/10.1109/ICDAR.2019.00255 (2017)	The historical Polish training dataset is used to evaluate and compare the performance of an optimized model against a top model from ICDAR 2019, specifically focusing on the morphological richness of the Hebrew language. This involves employing the dataset in benchmarking tasks to assess model accuracy and efficiency in handling complex linguistic structures.	
Hebrew Corpus	citing_context	HTB	https://doi.org/10.1007/s10579-025-09833-4 (2024)	https://www.semanticscholar.org/paper/424559e89b6923f379e930e67d465df8b481ddbf (2001)	The HTB dataset is used to contribute to Hebrew linguistic resources, specifically focusing on modern Hebrew text and annotated Hebrew Wikipedia articles. It is employed to enhance linguistic resources but is limited by specific registers and timeframes. This dataset supports research in modern Hebrew linguistics, aiding in the development of annotated corpora and linguistic studies within these constraints.	
Hebrew Corpus	citing_context	Hyperpartisan news detection dataset	https://doi.org/10.48550/arXiv.2304.11077 (2023)	https://doi.org/10.18653/v1/S19-2145 (2019)	The Hyperpartisan news detection dataset is used to evaluate the performance of RoBERTa and Longformer models in detecting hyperpartisan bias in news articles. Specifically, the dataset is translated into Hebrew to assess these models' effectiveness in a non-English context. This enables researchers to explore the cross-linguistic applicability of hyperpartisan bias detection techniques.	
Hebrew Corpus	cited_context | citing_context	JEMH’s BYP subcorpus	https://doi.org/10.1007/s10579-019-09458-4 (2019)	https://doi.org/10.1163/22134638-12340052 (2015)	The JEMH’s BYP subcorpus is used to study the development of the Hebrew language, particularly focusing on superfluous negation and the evolution of discourse markers. Linguistic analysis is the primary methodology employed, enabling researchers to examine historical changes and patterns in Hebrew usage. This dataset provides valuable insights into linguistic evolution, supporting detailed analyses of specific grammatical and discursive features.; The JEMH’s BYP subcorpus is used to trace the development of superfluous negation and the evolution of discourse markers in Middle Hebrew. Researchers employ corpus linguistics methods to analyze grammatical and discourse features, enabling detailed examination of linguistic changes over time. This dataset provides a rich resource for understanding historical linguistic phenomena through empirical analysis.	
Hebrew Corpus	citing_context	Jerusalem Corpus of Emergent Modern Hebrew	https://doi.org/10.1007/s10579-025-09833-4 (2024)	https://doi.org/10.1007/s10579-019-09458-4 (2019)	The Jerusalem Corpus of Emergent Modern Hebrew is used to study Hebrew during its formative years (1830-1970), focusing on literary texts. Researchers employ this dataset to analyze the evolution of modern Hebrew through historical literary works, leveraging the corpus's extensive collection of texts available from the Hebrew equivalent of Project Gutenberg. This enables detailed linguistic and literary analysis, providing insights into the development and standardization of modern Hebrew.	
Hebrew Corpus	cited_context | citing_context	JPress	https://doi.org/10.48550/arXiv.2307.16220 (2023)	https://www.semanticscholar.org/paper/6bc0ff5ce439ea2a60b2ef300f5ae3a911bd2ce4 (2018)	The JPress dataset is used to create an evaluation dataset (JP_CE) by randomly selecting 150 OCRed historical Hebrew newspaper articles from the period 1800-2015. This dataset facilitates the assessment of OCR accuracy and performance on historical Hebrew texts, enabling researchers to evaluate and improve OCR technologies for digitized historical documents.; The JPress dataset is used to create an evaluation dataset (JP_CE) comprising 150 OCRed historical Hebrew newspaper articles from 1800-2015. It focuses on assessing OCR accuracy and historical text processing methodologies. This dataset enables researchers to evaluate the effectiveness of OCR technologies and improve the processing of historical texts, specifically addressing challenges related to Hebrew language newspapers.	
Hebrew Corpus	cited_context | citing_context	labeled Israeli newspaper dataset	https://doi.org/10.1287/ijds.2022.0016 (2021)	https://www.semanticscholar.org/paper/424559e89b6923f379e930e67d465df8b481ddbf (2001)	The labeled Israeli newspaper dataset is used to test and evaluate models for classifying grammatical roles in Hebrew text. Researchers focus on performance metrics such as F1-score to assess the model's accuracy. This dataset enables the development and refinement of natural language processing techniques specific to Hebrew, enhancing the understanding and processing of Hebrew grammatical structures.; The labeled Israeli newspaper dataset is used to test and evaluate models for classifying grammatical roles in Hebrew text. Researchers focus on performance metrics such as F1-score to assess the model's accuracy. This dataset enables the development and refinement of natural language processing techniques specifically tailored for Hebrew, enhancing the understanding and processing of Hebrew grammatical structures.	
Hebrew Corpus	citing_context	Lampeter Corpus of Early Modern English Tracts	https://doi.org/10.1007/s10579-019-09458-4 (2019)	https://www.semanticscholar.org/paper/f0b411d21b694a2c5532c6312c72d7293fe63df4 (1997)	The Lampeter Corpus of Early Modern English Tracts is used to study diachronic linguistic features, particularly focusing on changes in language over time. Researchers employ this corpus to analyze shifts in linguistic patterns within early modern tracts, using it to explore historical language evolution. This dataset enables detailed textual analysis, providing insights into the development of Early Modern English.	
Hebrew Corpus	citing_context	LJSpeech	https://doi.org/10.21437/interspeech.2023-430 (2023)		The LJSpeech dataset is used as a reference for designing a Hebrew corpus, focusing on replicating its scale and structure. Researchers employ the dataset's design principles to create a comparable Hebrew dataset, ensuring similar methodological standards and characteristics. This approach aids in developing a robust Hebrew speech corpus for various linguistic and speech technology applications.	
Hebrew Corpus	citing_context	mC4	https://doi.org/10.48550/arXiv.2304.11077 (2023)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The mC4 dataset includes the Hebrew language, but its specific usage in research is not detailed in the provided descriptions. Therefore, there is no explicit evidence to describe its application, methodology, or research questions in any particular study.	
Hebrew Corpus	citing_context	NEMO	https://doi.org/10.48550/arXiv.2304.11077 (2023), https://www.semanticscholar.org/paper/f57f45978ec082d6c35e27f9c17143d4acef60b7 (2021)	https://doi.org/10.1162/tacl_a_00404 (2020)	The NEMO dataset is primarily used for linguistic and sentiment analysis in Modern Hebrew. It supports neural modeling techniques for named entity recognition and morphology, as well as training and evaluating sentiment analysis models. Researchers use it to develop and test neural architectures, including BERT-based models, for tasks such as sentiment classification and named entity recognition, leveraging its token-based and morpheme-based annotations.	
Hebrew Corpus	citing_context	offensive language corpus in Hebrew	https://doi.org/10.18653/v1/2024.woah-1.8 (2024)		The offensive language corpus in Hebrew is used to classify offensive language in Hebrew Twitter posts. The dataset focuses on categorizing posts into types such as abuse, hate, violence, pornography, or non-offensive. Labeled by Arabic-Hebrew bilingual speakers, it enables researchers to develop and evaluate classification models for detecting offensive content in social media.	
Hebrew Corpus	cited_context	corpus of Sima’an et al. (2001	https://doi.org/10.1007/s10579-007-9050-8 (2008)	https://www.semanticscholar.org/paper/424559e89b6923f379e930e67d465df8b481ddbf (2001)	The dataset 'corpus of Sima’an et al. (2001)' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	
Hebrew Corpus	citing_context	OpenSubtitles 2016	https://www.semanticscholar.org/paper/e65e3fd8069ce1fc19c382411711ebaf837f244a (2020)	https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The OpenSubtitles 2016 dataset is used to align 7 million sentences across multiple languages, focusing on the extraction of parallel corpora from movie and TV subtitles. This dataset facilitates translation and language modeling by providing large-scale, naturally occurring text data. Researchers employ it to develop and evaluate algorithms for cross-lingual alignment and to enhance the performance of machine translation systems.	
Hebrew Corpus	cited_context | citing_context	OSCAR corpus	https://doi.org/10.1109/AICCSA59173.2023.10479258 (2023), https://doi.org/10.18653/v1/2021.mrqa-1.11 (2021), https://doi.org/10.48550/arXiv.2402.16065 (2024), https://doi.org/10.48550/arXiv.2210.07873 (2022), https://www.semanticscholar.org/paper/f57f45978ec082d6c35e27f9c17143d4acef60b7 (2021)	https://doi.org/10.18653/v1/2020.acl-main.156 (2020)	The OSCAR corpus is primarily used to train and enhance language models such as AlephBERT and HeBERT, focusing on Hebrew and other mid-resource languages. It provides a large, multilingual, and structured dataset, including web-crawled and de-duplicated text, which improves the models' understanding of formal, informal, and colloquial Hebrew. This corpus enables researchers to develop more robust and contextually aware language models, specifically tailored for Hebrew and similar languages.; The OSCAR corpus is used for pre-training and training language models, particularly HeBERT, a Hebrew adaptation of BERT. It focuses on the Hebrew portion extracted from Common Crawl, emphasizing language classification, filtering, and cleaning. The dataset provides structured and curated content, enhancing performance on Hebrew language tasks and improving multilingual text understanding from web-crawled data.	
Hebrew Corpus	citing_context	OSCAR 2019	https://doi.org/10.48550/arXiv.2304.11077 (2023)	https://doi.org/10.18653/v1/2020.acl-main.156 (2020)	The OSCAR 2019 dataset is primarily used for pre-training language models, specifically AlephBERT and HeBERT, focusing on enhancing contextualized word embeddings for mid-resource languages like Hebrew. It leverages line-based and deduplicated data to improve model performance, making it a crucial resource for developing robust Hebrew language models.	
Hebrew Corpus	cited_context | citing_context	PARASHOOT	https://doi.org/10.48550/arXiv.2403.09719 (2024), https://doi.org/10.18653/v1/2021.mrqa-1.11 (2021)	https://doi.org/10.18653/v1/2021.mrqa-1.11 (2021), https://doi.org/10.18653/v1/D16-1264 (2016)	The PARASHOOT dataset is used to develop and fine-tune Hebrew question answering systems, addressing the scarcity of semantic datasets in Hebrew. It provides a SQuAD-style dataset for machine comprehension research, focusing on improving performance on Hebrew-specific linguistic challenges. This dataset enables researchers to enhance the ability of models to understand and answer questions from Hebrew text, contributing to advancements in Hebrew language processing.; The PARASHOOT dataset is used to address the scarcity of semantic datasets in Hebrew, specifically for developing Hebrew question answering systems. It follows the format of SQuAD and focuses on machine comprehension of Hebrew text, enabling researchers to build and evaluate systems that can understand and answer questions from Hebrew passages.	
Hebrew Corpus	cited_context	parental diary studies	https://doi.org/10.1017/S0305000903005750 (2003)	https://www.semanticscholar.org/paper/8354582089af3222ccb5bf7b10655e85585223fd (1987)	The 'parental diary studies' dataset is used to study early lexical development in Hebrew-speaking children, particularly focusing on the one-word phase before word combinations emerge. This research employs detailed parental observations to document and analyze children's early vocabulary acquisition. The dataset enables researchers to explore specific linguistic milestones and patterns in early language development, providing insights into the cognitive processes involved in learning to speak.	
Hebrew Corpus	citing_context	Pre-Classical Piyyut	https://doi.org/10.48550/arXiv.2402.17371 (2024)	https://doi.org/10.1162/COLI_a_00124 (2013)	The 'Pre-Classical Piyyut' dataset is used to assess inter-annotator agreement in Hebrew metaphor annotation by calculating Cohen’s kappa scores. This focuses on the reliability and consistency of annotations, ensuring that the dataset supports robust and reproducible research in linguistic and literary analysis.	
Hebrew Corpus	citing_context	QALD-9plus	https://doi.org/10.1162/tacl_a_00499 (2021)	https://doi.org/10.3233/SW-140134 (2015)	The QALD-9plus dataset is used to evaluate question answering systems over DBpedia and Wikidata, focusing on performance and translation quality across multiple languages, including Hebrew. It enables researchers to assess system accuracy and effectiveness in handling multilingual queries, providing a benchmark for comparing different approaches and methodologies.	
Hebrew Corpus	cited_context	raw data from the Hebrew speakers	https://doi.org/10.1080/17549507.2020.1808701 (2020)	https://doi.org/10.1177/0023830917708808 (2018)	The dataset 'raw data from the Hebrew speakers' is used to compare oral diadochokinetic (DDK) rates among Hebrew speakers, controlling for age and gender. Researchers employ this dataset to investigate the effects of practice and visual feedback on DDK rates, utilizing statistical methods to control for demographic variables. This enables a focused analysis on the impact of these factors on speech motor performance.	
Hebrew Corpus	citing_context	SASPEECH	https://doi.org/10.48550/arXiv.2506.12311 (2025)	https://doi.org/10.48550/arXiv.2410.21502 (2024)	The SASPEECH dataset is used to evaluate model performance on out-of-distribution Hebrew speech data, specifically focusing on the stability of text-to-speech synthesis. This dataset enables researchers to assess how well models generalize to unseen Hebrew speech, providing insights into model robustness and reliability in real-world applications.	
Hebrew Corpus	citing_context	SFT dataset	https://doi.org/10.48550/arXiv.2407.07080 (2024)	https://doi.org/10.48550/arXiv.2305.18290 (2023)	The SFT dataset is used to sample entries for creating a multilingual (English and Hebrew) DPO corpus, specifically to optimize preferences in language models. This involves selecting and integrating data to enhance model performance, focusing on preference optimization methodologies. The dataset's multilingual nature is crucial for developing and evaluating language models that can handle both English and Hebrew effectively.	
Hebrew Corpus	citing_context	Slim-Pajama corpus	https://doi.org/10.48550/arXiv.2407.07080 (2024)		The Slim-Pajama corpus is used to source English data, specifically sampled to match the quantity of Hebrew data available for comparative analysis. This enables researchers to conduct balanced and controlled comparative studies, ensuring that the volume of data from each language is equivalent, which is crucial for valid cross-linguistic comparisons.	
Hebrew Corpus	cited_context | citing_context	SPMRL shared task	https://www.semanticscholar.org/paper/f57f45978ec082d6c35e27f9c17143d4acef60b7 (2021)	https://doi.org/10.18653/v1/w13-4917 (2013)	The SPMRL shared task dataset is used to evaluate parsing and morphological analysis in morphologically rich languages, particularly Hebrew. It supports cross-framework assessments of model performance in morphology, POS tagging, and syntactic structure parsing. Researchers use it to test and compare the accuracy and performance of models across multiple epochs, focusing on universal dependency parsing and morphological labeling.; The SPMRL shared task dataset is used to evaluate parsing and morphological analysis in Hebrew, a morphologically rich language. It supports cross-framework assessments of model performance in parsing, morphology, and POS tagging. Research focuses on accuracy, performance, and comparative evaluations across different frameworks, enhancing understanding of linguistic structures and model capabilities.	
Hebrew Corpus	citing_context	TCR	https://www.semanticscholar.org/paper/4154c57cfc2c9ab271a1967cdbacd7cb1d5b58ab	https://doi.org/10.3115/1621474.1621488 (2007)	The TCR dataset is used to classify temporal relations in Hebrew text, focusing on the linguistic and semantic aspects of event sequences. Researchers employ this dataset to analyze and model the temporal dynamics within texts, enabling a deeper understanding of how events are sequenced and related in Hebrew language contexts.	
Hebrew Corpus	cited_context	treebank of 4500 sentences	https://doi.org/10.1007/s10579-007-9050-8 (2008)	https://doi.org/10.1017/S135132490700455X (2008)	The treebank of 4500 sentences is used to train models for Hebrew language processing, specifically focusing on segmentation and part-of-speech tagging. This dataset enables researchers to achieve high accuracy in these tasks by providing a structured set of annotated sentences, facilitating the development and evaluation of natural language processing models.	
Hebrew Corpus	citing_context	T-RES	https://doi.org/10.1044/2021_JSLHR-21-00205 (2022)	https://doi.org/10.21437/speechprosody.2018-145 (2018)	The T-RES dataset is used to study emotional speech perception in Hebrew, specifically focusing on the emotions of anger, happiness, sadness, and neutrality. It provides semantically validated sentences for these emotions, enabling researchers to analyze how these emotions are perceived in spoken Hebrew. The dataset's validated content supports methodologies that require accurate emotional labeling for perceptual studies.	
Hebrew Corpus	citing_context	TypeCraft	https://www.semanticscholar.org/paper/72e1485fd871b3e5ea2df74800df08355381bee6 (2016)	https://doi.org/10.1007/s10579-013-9257-9 (2013)	The TypeCraft dataset is utilized for collaborative databasing and resource sharing among linguists, primarily for encoding and sharing morphological data in XML format. It facilitates the sharing of annotated linguistic resources, enhancing collaborative research and enabling the analysis of morphological structures across different languages.	
Hebrew Corpus	cited_context	UD data collection	https://doi.org/10.18653/v1/2020.findings-emnlp.391 (2020)	https://doi.org/10.1162/tacl_a_00033 (2018)	The UD data collection is used to apply character-level sequence labeling for word segmentation, specifically through the implementation and interpretation of end-to-end neural models. This dataset enables researchers to develop and evaluate neural architectures that can accurately segment words, enhancing natural language processing tasks.	
Hebrew Corpus	cited_context | citing_context	UD Hebrew corpus	https://www.semanticscholar.org/paper/f57f45978ec082d6c35e27f9c17143d4acef60b7 (2021)	https://doi.org/10.1162/tacl_a_00253 (2019)	The UD Hebrew corpus is used to evaluate parsing models in Hebrew, focusing on morpho-syntactic parsing strategies. Researchers compare different evaluation metrics, such as Aligned MultiSet F1 Scores and Aligned F1 scores, to assess trends and performance in Hebrew language parsing. This dataset enables detailed analysis and comparison of parsing techniques, enhancing the accuracy and reliability of morpho-syntactic parsing models.; The UD Hebrew corpus is used to evaluate parsing models in Hebrew, focusing on the comparison of different evaluation metrics such as Aligned MultiSet F1 Scores and Aligned F1 scores. This dataset enables researchers to assess the performance and accuracy of syntactic parsers, contributing to advancements in Hebrew language processing and computational linguistics.	
Hebrew Corpus	cited_context	UD Treebank	https://doi.org/10.48550/arXiv.2308.16687 (2023)	https://doi.org/10.18653/v1/W18-6016 (2018)	The UD Treebank dataset is used to train models on syntactic parsing and part-of-speech tagging for Hebrew, utilizing 5K tagged sentences. It is also expanded with 35K additional tagged sentences to enhance linguistic annotation and syntactic structure analysis. This dataset supports research in improving the accuracy of syntactic parsing and linguistic annotation in Hebrew.	
Hebrew Corpus	citing_context	WordMill database	https://doi.org/10.1037/0278-7393.30.6.1271 (2004)		The WordMill database is used to analyze Hebrew language data, specifically focusing on morphological family-size counts and identifying potential family members among 7 million word forms. Researchers often use a dictionary to aid in these analyses. This dataset enables detailed linguistic studies by providing extensive word form data, facilitating the exploration of morphological structures and relationships within the Hebrew language.	
Hebrew Corpus	citing_context	YAEL essays	https://www.semanticscholar.org/paper/7834c0a14af9b276aa64cbac8e16214598e259c7 (2022)	https://www.semanticscholar.org/paper/4ae16f85abeaee528e21a37a56539d19e2705d70 (2015)	The YAEL essays dataset is used to study orthographic and morphological errors in Hebrew essays. Researchers tokenize the essays and compare them with corrected versions to analyze linguistic improvements. Half of the essays are reviewed and corrected by NITE, and the Child Phonology Analyzer is used for tokenization. This dataset enables detailed analysis of error patterns and linguistic development in written Hebrew.	
Hungarian Corpus	citing_context	2012 Magyar Ifjusag database	https://doi.org/10.1556/063.9.2019.1.26 (2019)		The 2012 Magyar Ifjusag database is used to study the social background of language learning among young Hungarian students aged 15-29. Researchers focus on demographic and educational factors influencing language acquisition, employing methodologies that analyze these variables to understand their impact on language skills. This dataset enables detailed examination of how social and educational contexts shape language learning outcomes in this age group.	
Hungarian Corpus	citing_context	collection carried out in 2013	https://doi.org/10.1007/978-3-031-40498-6_9 (2023)	https://www.semanticscholar.org/paper/d61f61fbcf0cb78f4188d6e0b61b031c8ce16ba1 (2016)	The dataset collected in 2013 is used for text summarization tasks, focusing on Hungarian language data. It enables researchers to analyze and process large volumes of Hungarian text, facilitating the development and evaluation of summarization algorithms. The dataset's specific focus on 2013 data provides a consistent temporal context for these linguistic analyses.	
Hungarian Corpus	citing_context	Common Voice	https://www.semanticscholar.org/paper/5e691962003082913bdcb0aa3c80a9def55b6858 (2022)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Common Voice dataset is used to collect and verify Hungarian speech data, specifically to expand multilingual speech corpora. It provides 19 hours of training data, which researchers use to enhance the diversity and robustness of speech recognition systems, particularly in under-resourced languages like Hungarian. This dataset enables the development and improvement of multilingual speech technologies by providing a substantial amount of verified speech data.	
Hungarian Corpus	cited_context	Czech MALACH corpora	https://doi.org/10.1109/TASL.2009.2038807 (2010)	https://doi.org/10.3115/1610075.1610130 (2006)	The Czech MALACH corpora is used to train and evaluate models for recognizing Hungarian, focusing on morphological and phonological features. This dataset enables researchers to develop and test algorithms that can accurately identify and process Hungarian language elements, enhancing natural language processing capabilities.	
Hungarian Corpus	cited_context	e-Magyar beszédarchívum	https://www.semanticscholar.org/paper/c26c12a76df67ab2675bddd91eedad87e49e6b78 (2017)	https://www.semanticscholar.org/paper/af04ad1ceda1ca7c8d53809bc4167ac9aaa82218 (2017)	The dataset 'e-Magyar beszédarchívum' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. Its role in enabling research is unclear based on the provided evidence.	
Hungarian Corpus	cited_context | citing_context	EUROM1	https://doi.org/10.1007/978-3-319-10816-2_51 (2014)	https://doi.org/10.21437/Eurospeech.1995-198 (1995)	The EUROM1 dataset is used to compare phonetic structures across languages, serving as a multilingual spoken language resource with standardized materials and recording protocols. It enables researchers to conduct cross-linguistic analyses, facilitating the study of phonetic similarities and differences among European languages. This dataset supports research questions related to phonetic structure and linguistic diversity, leveraging its consistent methodological approach to ensure comparability across languages.; The EUROM1 dataset is used to analyze the phonetic structures of various languages, with a focus on comparing Hungarian with other languages. It employs similar materials and recording protocols to ensure comparability. This dataset enables researchers to explore linguistic differences and similarities, contributing to phonetic and phonological studies.	
Hungarian Corpus	citing_context	Europarl parallel corpus	https://doi.org/10.48550/arXiv.2502.20552 (2025)	https://www.semanticscholar.org/paper/694b3c58712deefb59502847ba1b52b192c413e5 (2005)	The Europarl parallel corpus is used to generate silver training data for machine translation, specifically focusing on creating resources for training models on parallel texts. This dataset enables researchers to develop and improve machine translation systems by providing large volumes of aligned sentences in multiple languages, enhancing the quality and accuracy of translations.	
Hungarian Corpus	citing_context	EXAMS-V	https://doi.org/10.48550/arXiv.2505.23008 (2025)	https://doi.org/10.18653/v1/2020.acl-main.691 (2019)	The EXAMS-V dataset is used to evaluate vision-language models across various disciplines and languages. It involves manual curation and quality checking by native speakers, ensuring high data reliability. This dataset enables researchers to assess model performance in complex, multilingual environments, focusing on the integration of visual and textual information.	
Hungarian Corpus	citing_context	FLORES-200	https://doi.org/10.1109/CITDS62610.2024.10791389 (2024)	https://doi.org/10.48550/arXiv.2207.04672 (2022)	The FLORES-200 dataset is used to train machine translation models, specifically focusing on the English-Hungarian subset. This dataset enhances translation quality and coverage by providing a robust resource for model training. The dataset's comprehensive and diverse content enables researchers to address challenges in translating between these languages, improving the accuracy and fluency of machine-translated text.	
Hungarian Corpus	citing_context	Historical Corpus of Personal Texts	https://www.semanticscholar.org/paper/aef1ca540acb54b6e9e7f7cd96c2e6d7cd27336c (2020)	https://doi.org/10.1007/s10579-017-9393-8 (2018)	The Historical Corpus of Personal Texts is used to analyze informal language use in Old and Middle Hungarian, primarily through private letters and trial testimonies. Researchers employ this dataset to conduct linguistic analyses, focusing on patterns and variations in personal texts. This corpus enables detailed examination of historical language practices, providing insights into the evolution of informal Hungarian language over time.	
Hungarian Corpus	citing_context	HuAMR	https://doi.org/10.48550/arXiv.2502.20552 (2025)	https://www.semanticscholar.org/paper/ebe23ed507014dc012c15dea0e1b538538cd8505 (2013)	The HuAMR dataset is used to evaluate model performance on Hungarian Abstract Meaning Representation (AMR) parsing. Researchers focus on assessing semantic feature structures and use Smatch scores to measure accuracy. This dataset enables the evaluation and improvement of AMR parsing models specifically for the Hungarian language, addressing the challenges of semantic representation in this linguistic context.	
Hungarian Corpus	citing_context	HuComTech Multimodal Database	https://www.semanticscholar.org/paper/5e691962003082913bdcb0aa3c80a9def55b6858 (2022)		The HuComTech Multimodal Database is used to analyze audio-visual recordings of 121 young adult university students, focusing on multimodal communication patterns and behaviors. Researchers employ this dataset to study how verbal and non-verbal cues interact in communication, using methodologies that involve detailed analysis of both audio and visual data. This enables investigations into the nuances of human interaction, providing insights into communication dynamics and behavioral patterns.	
Hungarian Corpus	citing_context	HuDocVQA-manual	https://doi.org/10.48550/arXiv.2505.23008 (2025)		The HuDocVQA-manual dataset is used to address the shortage of high-quality Hungarian datasets for OCR and visual question answering. It includes manually annotated document images, automatically generated document images, and PDFs filtered from Common Crawl. This dataset enables researchers to develop and evaluate models for OCR and VQA tasks, enhancing the accuracy and robustness of these systems in processing Hungarian documents.	
Hungarian Corpus	cited_context	Hu-mor	https://www.semanticscholar.org/paper/d61f61fbcf0cb78f4188d6e0b61b031c8ce16ba1 (2016)	https://doi.org/10.3115/1034678.1034723 (1999)	The 'Hu-mor' dataset is used to develop and evaluate computational morphology for Hungarian, specifically focusing on morphological analysis and database development. It enables researchers to enhance and test algorithms for processing Hungarian language data, contributing to the advancement of natural language processing techniques for morphologically rich languages.	
Hungarian Corpus	citing_context	HU-News	https://doi.org/10.1007/s10579-021-09568-y (2022)	https://doi.org/10.5281/ZENODO.3770924 (2020)	The HU-News dataset is used to evaluate the performance of multilingual and monolingual BERT models on Hungarian news articles. Researchers focus on ROUGE scores to assess model effectiveness. This dataset enables the comparison of different BERT variants, providing insights into their capabilities in processing and summarizing Hungarian news content.	
Hungarian Corpus	citing_context	Hungarian	https://doi.org/10.18653/v1/2021.eacl-main.11 (2020)	https://www.semanticscholar.org/paper/ea45438193cd724445d08cf3a1fa9137ffed54f6 (2011)	The 'Hungarian' dataset is used to analyze linguistic patterns in Hungarian language posts and movie subtitles, focusing on thread content, user interactions, and colloquial usage. It is auto-extracted from large corpora, including 730M Reddit threads and movie subtitles, and serves as a source for building larger datasets of conversational data. However, it has limitations, notably lacking dialogue and turn segmentation, which can introduce non-dialogue text and errors.	
Hungarian Corpus	cited_context	Hungarian, and Italian corpora	https://doi.org/10.1007/978-3-030-28577-7_26 (2019)		The dataset 'Hungarian, and Italian corpora' is mentioned in research citations but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe how it is employed in methodologies, research questions, or applications. Its role and relevance in actual research remain unspecified based on the provided information.	
Hungarian Corpus	citing_context	Hungarian BEA database	https://www.semanticscholar.org/paper/5e691962003082913bdcb0aa3c80a9def55b6858 (2022)	https://doi.org/10.1007/978-3-319-10816-2_51 (2014)	The Hungarian BEA database is used to study spontaneous speech in Hungarian, focusing on linguistic patterns and acoustic features. It is employed to develop and evaluate models and multifunctional applications for Hungarian spoken language. Researchers use subsets of the database to ensure consistency and comparability, particularly with the WSJ dataset, for training, evaluation, and large-scale linguistic analysis.	
Hungarian Corpus	citing_context	Hungarian Choice of Plausible Alternatives Corpus	https://doi.org/10.36244/icj.2023.5.10 (2023)	https://doi.org/10.18653/v1/d13-1170 (2013)	The Hungarian Choice of Plausible Alternatives Corpus is used to evaluate various linguistic and cognitive tasks in Hungarian, including commonsense reasoning, commitment and belief in discourse, reading comprehension, textual entailment, sentiment analysis, coreference resolution, and grammatical acceptability. It employs methodologies such as translation from existing datasets and design based on established frameworks to address specific research questions like logical consistency, inference, and syntactic correctness. This dataset enables researchers to assess the performance of models and human subjects in these areas, providing insights into the nuances of Hungarian language processing and understanding.	
Hungarian Corpus	citing_context	Hungarian HuLU	https://doi.org/10.1109/CITDS62610.2024.10791364 (2024)	https://doi.org/10.18653/v1/W18-5446 (2018)	The Hungarian HuLU dataset is used to create subcorpora by translating English GLUE and Super-GLUE subcorpora, enabling natural language understanding tasks in Hungarian. This involves adapting established benchmarks to the Hungarian language, facilitating research into the performance and capabilities of NLP models in this specific linguistic context.	
Hungarian Corpus	cited_context	Hungarian National Corpus	https://www.semanticscholar.org/paper/3ce4bf7b10c0d6a067ccf5b866ce3d330de0e2ba (2013), https://www.semanticscholar.org/paper/7bb79a8c174a17a7d494478f81147212720ca387 (2014)	https://www.semanticscholar.org/paper/7bf5de63d9ef5a1ef4827ff84118b7e687ed71ff (2002)	The Hungarian National Corpus is used in linguistic research to enhance POS-tagging and annotation precision, focusing on the morphological and structural properties of the Hungarian language. It supports studies on corpus representativity, the challenges of collecting and processing electronic texts, and the analysis of language variations within and outside Hungary. The dataset's extensive size (187 million words) and diverse content enable robust training and evaluation of linguistic models, addressing specific research questions related to linguistic diversity and regional differences.	
Hungarian Corpus	citing_context	Hungarian nominal WordNet	https://www.semanticscholar.org/paper/cc231da613cd09556578d28bc7659ac113ba2e15 (2004)	https://www.semanticscholar.org/paper/019b66cdec5dbe67cd32fb8abfa9a4723c1d4b67 (2002)	The Hungarian nominal WordNet dataset is primarily used for mining collocations and other linguistic purposes, providing a structured vocabulary of Hungarian nouns. This resource enables researchers to analyze noun usage and collocation patterns, supporting linguistic studies and enhancing natural language processing tasks.	
Hungarian Corpus	cited_context	Hungarian version of Wikipedia	https://doi.org/10.1017/S1351324918000281 (2018)	https://doi.org/10.1162/tacl_a_00051 (2016)	The Hungarian version of Wikipedia is used to train word embeddings with a focus on the Hungarian language, employing the fastText algorithm to incorporate subword information. This methodology allows researchers to analyze linguistic features and patterns, enhancing understanding of the Hungarian language through computational linguistics.	
Hungarian Corpus	cited_context	Hunmorph/morphdb.hu	https://www.semanticscholar.org/paper/d61f61fbcf0cb78f4188d6e0b61b031c8ce16ba1 (2016)	https://www.semanticscholar.org/paper/13167f9cd8c7906ca808b01d28dca6dd951da8a5	The dataset 'Hunmorph/morphdb.hu' is mentioned in the citation context but lacks detailed descriptions of its usage in specific research studies. Therefore, there is no explicit evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.	
Hungarian Corpus	citing_context	huTenTen	https://doi.org/10.1007/978-3-031-40498-6_9 (2023)	https://www.semanticscholar.org/paper/fcc7e0c241c456997a1f2452b77e1b20397ec305 (2013)	The huTenTen dataset serves as the Hungarian reference corpus for the SketchEngine platform, enabling linguistic research on the Hungarian language. It is utilized to study linguistic patterns and features specific to Hungarian, providing researchers with a comprehensive resource for corpus-based analysis. This dataset supports the exploration of morphological, syntactic, and semantic aspects of the language, facilitating detailed linguistic investigations.	
Hungarian Corpus	cited_context	ICD10 Coding of Death Certificates in French, Hungarian and Italian	https://www.semanticscholar.org/paper/badce54b2b9e7fe92e35036b7aac8b667c432b5f (2018)	https://www.semanticscholar.org/paper/13db484cd8cee4d99ab3f7ee9c00e06f3c74e0c1 (2018)	The dataset 'ICD10 Coding of Death Certificates in French, Hungarian and Italian' is mentioned in research contexts but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided in the available literature. Therefore, it is unclear how this dataset is actually utilized in research studies.	
Hungarian Corpus	citing_context	MALACH	https://www.semanticscholar.org/paper/5e691962003082913bdcb0aa3c80a9def55b6858 (2022)	https://doi.org/10.1007/978-3-319-43958-7_16 (2016)	The MALACH dataset is used for recognizing spontaneous speech in Hungarian, with a focus on morphological and acoustic modeling techniques. It addresses the challenges of spontaneous speech recognition in a less-resourced language setting, enabling researchers to improve speech recognition systems specifically tailored for Hungarian.	
Hungarian Corpus	citing_context	MASSIVE-AMR	https://doi.org/10.48550/arXiv.2502.20552 (2025)	https://doi.org/10.48550/arXiv.2405.19285 (2024)	The MASSIVE-AMR dataset is used to create a cross-lingual AMR dataset for question answering, supporting research in multilingual natural language processing. It includes Hungarian and other languages, enabling the development and evaluation of models that can handle multiple languages. This dataset facilitates the exploration of cross-lingual transfer and the improvement of question answering systems across different linguistic contexts.	
Hungarian Corpus	citing_context	MNSZ2	https://doi.org/10.1007/978-3-031-40498-6_9 (2023)	https://www.semanticscholar.org/paper/7bb79a8c174a17a7d494478f81147212720ca387 (2014)	The MNSZ2 dataset is used to provide a large-scale corpus for Hungarian language research, containing over 1 billion words. It is employed for linguistic analysis and natural language processing tasks, enabling researchers to explore complex language structures and develop more accurate computational models for the Hungarian language.	
Hungarian Corpus	cited_context	Named Entity Corpus for Hungarian	https://doi.org/10.1017/S1351324918000281 (2018)	https://www.semanticscholar.org/paper/67744cb2f60f325c41addce8b7187f716050420c (2006)	The Named Entity Corpus for Hungarian is used to train and evaluate named entity recognition systems specifically for the Hungarian language. Containing approximately 14,400 tagged phrases, this dataset supports research in improving the accuracy of identifying and categorizing named entities in Hungarian text. It enables researchers to develop and test algorithms that can effectively recognize entities such as persons, organizations, and locations in Hungarian-language documents.	
Hungarian Corpus	cited_context	corpus of death reports in Hungarian	https://doi.org/10.1007/978-3-030-28577-7_26 (2019)	https://www.semanticscholar.org/paper/fdc85ab0343c8e7b996d955d44f45e0cc3a959a4 (2019)	The corpus of death reports in Hungarian is used to train and evaluate models for identifying causes of death, focusing on language and coding issues specific to Hungarian. This dataset enables researchers to address the complexities of death report analysis in the Hungarian language, enhancing the accuracy of cause-of-death identification through computational models.	
Hungarian Corpus	citing_context	opensubtitles	https://doi.org/10.18653/v1/2021.eacl-main.11 (2020), https://doi.org/10.1109/CITDS62610.2024.10791389 (2024)	https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The OpenSubtitles dataset is primarily used for pre-training language models and neural conversation models, often compared against Gutenberg pre-training. It leverages large parallel corpora from movie and TV subtitles to enhance multilingual representation learning and evaluate the effectiveness of different pre-training datasets. This dataset supports research in improving model performance and understanding the impact of diverse training data sources.	
Hungarian Corpus	cited_context	orthographic dictionary of Hungarian	https://www.semanticscholar.org/paper/d61f61fbcf0cb78f4188d6e0b61b031c8ce16ba1 (2016)		The orthographic dictionary of Hungarian is used to create a list of correct forms and multiword expressions, ensuring orthographic accuracy and comprehensiveness. It is also utilized to train and evaluate models, focusing on the morphological and syntactic features of Hungarian. This dataset enables researchers to enhance the accuracy and reliability of computational models in processing Hungarian language data.	
Hungarian Corpus	citing_context	Semantic Textual Similarity dataset	https://doi.org/10.1109/CITDS62610.2024.10791389 (2024)		The Semantic Textual Similarity dataset is used to evaluate Hungarian natural language processing systems, specifically for semantic textual similarity tasks. It contributes to the HuLU benchmark, aiding in the development and assessment of models that understand and measure the semantic similarity between texts in the Hungarian language.	
Hungarian Corpus	citing_context	SZAK Corpus	https://www.semanticscholar.org/paper/cc231da613cd09556578d28bc7659ac113ba2e15 (2004)		The SZAK Corpus is used to study technical texts in Hungarian, specifically focusing on the unannotated Hungarian component of a parallel corpus containing approximately 1.2 million words. This dataset enables researchers to analyze linguistic features and patterns in technical writing, contributing to the understanding of specialized language use in the Hungarian context.	
Hungarian Corpus	citing_context	Szeged Corpus	https://www.semanticscholar.org/paper/68b37cf1d2c990c800fddc673568981df725670f (2014)	https://doi.org/10.1007/11551874_16 (2005)	The Szeged Corpus is used to provide comprehensive morphological analyses and lemmas for Hungarian texts, supporting linguistic research and natural language processing tasks. It is specifically employed for part-of-speech tagging, focusing on the annotation with MSD codes to facilitate detailed linguistic analysis. This dataset enables researchers to study the morphological structure and syntactic properties of Hungarian, enhancing the accuracy and depth of NLP applications and linguistic studies.	
Hungarian Corpus	citing_context	Szeged Corpus 2.5	https://www.semanticscholar.org/paper/68b37cf1d2c990c800fddc673568981df725670f (2014)	https://www.semanticscholar.org/paper/3ce4bf7b10c0d6a067ccf5b866ce3d330de0e2ba (2013)	The Szeged Corpus 2.5 is primarily used for training and evaluating the magyarlanc linguistic preprocessing toolkit for Hungarian, specifically focusing on part-of-speech tagging and morphological analysis. It provides baseline accuracy results for the POS-tagger module and supports the evaluation of its performance with new harmonized codes. This dataset enables researchers to enhance and validate the accuracy of linguistic tools for the Hungarian language.	
Hungarian Corpus	cited_context	Szeged Dependency Treebank	https://www.semanticscholar.org/paper/3ce4bf7b10c0d6a067ccf5b866ce3d330de0e2ba (2013)	https://www.semanticscholar.org/paper/c95b073cb9f89d11d50cdf5bc53cbdcaa444c3fe (2003)	The Szeged Dependency Treebank is used to analyze Hungarian language corpora, focusing on morphological features and tagging accuracy. It is employed in studying POS tagging and annotation with MSD codes, as well as examining morphological and syntactic structures. This dataset enables researchers to evaluate and improve the accuracy of linguistic annotations and understand the complexities of Hungarian morphology and syntax.	
Hungarian Corpus	citing_context	Szeged NER corpus	https://doi.org/10.26615/978-954-452-072-4_119 (2021)	https://www.semanticscholar.org/paper/0a1cf8dbb859c13cbdb40788d7e69060155f9d77 (2014)	The Szeged NER corpus is used to enhance and evaluate named entity recognition (NER) systems in Hungarian, particularly in business news contexts. It enriches named entity annotations, improves NER quality and coverage, and provides a manually annotated resource for training and testing. The dataset also includes entity types not covered by the OntoNotes NER tagset, such as names of securities and stock exchange indexes. Derived from Wikipedia, it serves as a large-scale silver-standard corpus for NER tasks.	
Hungarian Corpus	citing_context	SZ ´OT´AR lexical database	https://www.semanticscholar.org/paper/9cf2606e10c1e8743dab1aca062ff70b586d47fe (2004)		The SZ ´OT´AR lexical database is used to preserve positional information data from FK89, employing sentence-level chunking to maintain contextual details in the Hungarian language. This methodology ensures that the dataset retains the necessary structural and contextual integrity, enabling research focused on linguistic analysis and the preservation of specific linguistic features.	
Hungarian Corpus	citing_context	TR-News	https://doi.org/10.1007/s10579-021-09568-y (2022)	https://www.semanticscholar.org/paper/adcfef04625c2763028815759750d47c7c3fe689 (2015)	The TR-News dataset is used in research to compare summarization performance and dataset sizes, specifically focusing on Hungarian news articles. It enables researchers to evaluate and benchmark extreme summarization tasks by providing a substantial number of instances, facilitating the analysis of summarization techniques in the Hungarian language.	
Hungarian Corpus	citing_context	Webcorpus 2.0	https://doi.org/10.1007/978-3-031-40498-6_9 (2023)	https://doi.org/10.15476/elte.2020.066 (2021)	Webcorpus 2.0 is used to compile a corpus of Hungarian texts, specifically focusing on web-based content. This dataset supports language modeling and natural language processing tasks by providing a large, diverse set of textual data. Researchers utilize this corpus to develop and evaluate models that can process and understand Hungarian language content from the web, enhancing the accuracy and applicability of NLP systems in this context.	
Indian Corpus	cited_context	1000 web text documents from five domains	https://doi.org/10.1109/ICISET.2018.8745621 (2018)	https://doi.org/10.1109/ICACCAF.2017.8344721 (2017)	The dataset of 1000 web text documents from five domains is used to evaluate similarity measures for categorizing Bangla text documents. Researchers focus on TF-IDF weighting schemes, achieving high accuracy in document categorization. This dataset enables the assessment of different weighting methods to improve text classification performance in the Bangla language.	
Indian Corpus	cited_context	184 documents	https://doi.org/10.1109/ICISET.2018.8745621 (2018)	https://www.semanticscholar.org/paper/d53267c1580935465df7ab6a93fa534a2fbf3b3c (2012)	The '184 documents' dataset is used for classifying Punjabi text documents, employing a hybrid approach that combines Naive Bayes and Ontological Based classification. This methodology focuses on enhancing the accuracy of text classification. The dataset's specific characteristics and its size enable researchers to test and refine classification algorithms, addressing the challenge of accurately categorizing Punjabi text.	
Indian Corpus	citing_context	dataset [28]	https://doi.org/10.1109/PCEMS58491.2023.10136062 (2023)	https://doi.org/10.1109/TISC.2011.6169079 (2011)	Dataset [28] is used to train and evaluate finger-spelling recognition models for Indian Sign Language. The dataset focuses on capturing and analyzing shape, texture, and local movement features of hand gestures. This enables researchers to develop more accurate and robust models for recognizing and interpreting sign language, enhancing communication technologies for the deaf community.	
Indian Corpus	cited_context	5251 sarcasm tweets in Hinglish	https://www.semanticscholar.org/paper/5fd2099e9f4c6d9f748685a3bd11460ce6043914 (2020)	https://doi.org/10.18653/v1/W18-3504 (2018)	The '5251 sarcasm tweets in Hinglish' dataset is used to detect sarcasm in Hinglish tweets, focusing on linguistic patterns and context-specific cues in code-switched language. Researchers employ this dataset to analyze and identify the unique features of sarcasm in Hinglish, enhancing understanding of code-switching and contextual nuances in social media communication.	
Indian Corpus	cited_context	7 Indian languages test-set	https://doi.org/10.1109/ICIIP.2017.8313738 (2017)	https://doi.org/10.1145/1577802.1577804 (2009)	The '7 Indian languages test-set' is used to evaluate and compare the performance of different OCR architectures, including Tesseract OCR, by measuring character error rates across seven Indian languages. This dataset enables researchers to assess the accuracy and effectiveness of OCR technologies specifically tailored for Indian scripts, providing insights into their strengths and weaknesses.	
Indian Corpus	citing_context	ACTSA	https://doi.org/10.48550/arXiv.2401.02254 (2024)	https://doi.org/10.18653/v1/W17-5408 (2017)	The ACTSA dataset is used to build a gold-standard annotated corpus of Telugu sentences, specifically supporting sentiment analysis research. It provides labeled data for training and evaluating machine learning models, enabling researchers to develop and refine algorithms that can accurately analyze sentiment in Telugu text.	
Indian Corpus	cited_context	ACTSA Sentiment Analysis corpus	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	https://doi.org/10.18653/v1/W17-5408 (2017)	The ACTSA Sentiment Analysis corpus is used for training and evaluating models in natural language processing tasks, specifically for headline and news article classification in Indian languages such as Bengali and Telugu. It employs supervised learning methods to categorize headlines and articles into predefined topics and sentiments, leveraging annotated data to enhance model accuracy and performance.	
Indian Corpus	cited_context	Affect WordNet	https://doi.org/10.1145/3383330 (2020)	https://www.semanticscholar.org/paper/52b3d9fcc8d0d3beb0357c598358f0435784d517 (2004)	The Affect WordNet dataset is used to create lexicons that focus on emotional and affective word associations, particularly in Indian languages. It serves as an affective extension of WordNet, enabling researchers to develop resources that capture nuanced emotional content. This dataset facilitates the study of affective language use, enhancing applications in sentiment analysis and emotion recognition.	
Indian Corpus	cited_context | citing_context	Aggression-annotated Corpus of Hindi-English Code-mixed Data	https://doi.org/10.48550/arXiv.2306.14764 (2023), https://doi.org/10.1007/978-3-031-41682-8_9 (2024)	https://www.semanticscholar.org/paper/45094ab1ae50337033514e48213397d35b821e43 (2018)	The Aggression-annotated Corpus of Hindi-English Code-mixed Data is used to study aggression in multilingual social media content, focusing on Hindi, English, and mixed-language tweets. Researchers analyze linguistic markers of aggression in code-mixed data, employing the dataset to investigate how language mixing influences the expression and detection of aggressive behavior online. This dataset enables detailed analysis of specific linguistic features that correlate with aggression in multilingual contexts.; The Aggression-annotated Corpus of Hindi-English Code-mixed Data is used to detect aggression in social media posts that mix Hindi and English. Researchers employ this dataset to identify and analyze aggressive content, focusing on the linguistic patterns and context-specific indicators of aggression in code-mixed data. This enables the development and evaluation of models for automated aggression detection in multilingual online environments.	
Indian Corpus	cited_context	A Hindi-English code-switching corpus	https://doi.org/10.21437/Interspeech.2021-1339 (2021)	https://www.semanticscholar.org/paper/a61eaafc0d58f7e9cac424658ac900c5fe2aabc8 (2014)	The Hindi-English code-switching corpus is primarily used to evaluate and improve Word Error Rates (WERs) in Automatic Speech Recognition (ASR) systems for mixed-language speech processing. It supports the development and evaluation of ASR models, particularly focusing on hybrid DNN-HMM architectures. The dataset is also utilized to analyze the linguistic integration of English in Hindi and Bengali contexts, providing annotated data for studying code-switching phenomena and building robust ASR systems for real-world scenarios.	
Indian Corpus	cited_context	AI4Bharat IndicCorp	https://doi.org/10.18653/v1/2021.eacl-main.303 (2021)	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	The AI4Bharat IndicCorp dataset is used to mine Indian named entities from English newspapers in India, focusing on the identification and extraction of culturally relevant information. This involves employing natural language processing techniques to extract and analyze names and entities specific to Indian contexts, enhancing the cultural relevance and accuracy of the data.	
Indian Corpus	cited_context	AI4Bharat IndicNLP dataset	https://doi.org/10.18653/v1/2021.eacl-main.303 (2021)		The AI4Bharat IndicNLP dataset is used to enhance monolingual vocabulary in Indic languages, supporting natural language processing tasks and linguistic analysis. It provides essential linguistic resources that contribute to the development of language models, improving their performance and accuracy in handling Indic languages. This dataset enables researchers to build more robust and contextually aware models, facilitating advancements in NLP for these languages.	
Indian Corpus	cited_context	AI4Bharat-StoryWeaver corpus	https://doi.org/10.48550/arXiv.2205.03018 (2022)		The AI4Bharat-StoryWeaver corpus is primarily used to train and evaluate machine translation and transliteration systems for Indian languages, particularly focusing on Telugu. It supports research in improving the accuracy, fluency, and cultural relevance of translations and transliterations, especially in low-resource settings and educational content. The dataset is also utilized to develop and assess natural language processing models for story generation and language understanding tasks, as well as to enhance transliteration search performance in information retrieval. Crowd-sourced transliterations and parallel transliteration pairs are key features that improve the dataset's accuracy and coverage.	
Indian Corpus	citing_context	Aksharantar	https://www.semanticscholar.org/paper/8ca047003dc2a9114b4f6571998d1619d2f4991b (2025), https://doi.org/10.1145/3639565 (2024)	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)	The Aksharantar dataset is used to train and evaluate multilingual transliteration models, specifically for converting native scripts into Roman scripts across 21 to 22 Indian languages. This enhances the efficiency of Language Identification systems and achieves state-of-the-art results using a transformer-based approach. The dataset's comprehensive coverage of Indic languages supports robust transliteration and cross-lingual consistency.	
Indian Corpus	citing_context	a large parallel corpus	https://doi.org/10.1145/3639565 (2024)	https://doi.org/10.3115/v1/N15-3017 (2015)	The large parallel corpus is used to develop machine learning methods for transliteration, specifically focusing on improving transliteration accuracy for Indian languages. This dataset enables researchers to create and train models that can accurately convert names and words between different scripts, enhancing the performance of transliteration systems. The corpus's extensive size and parallel nature are crucial for training robust machine learning algorithms.	
Indian Corpus	citing_context	Alpaca dataset	https://doi.org/10.48550/arXiv.2311.05845 (2023)		The Alpaca dataset is primarily used for translation tasks, employing a meticulous methodology to ensure accuracy and completeness. This approach is consistently applied across studies to maintain consistency. The dataset enables researchers to address specific challenges in translation, though its use is not limited to Indian languages as per the provided descriptions.	
Indian Corpus	cited_context	ALT	https://doi.org/10.1162/tacl_a_00452 (2021)		The ALT dataset is primarily used to provide parallel data for Indian language translation, focusing on bilingual text alignment and corpus construction. It supports the training of neural machine translation models and enhances the performance of multilingual translation systems, particularly for under-resourced languages like Odia, Bengali, and Gujarati. The dataset contributes to the development of robust machine translation models and cross-lingual evaluation tasks, emphasizing high-accuracy translations and linguistic annotation.	
Indian Corpus	cited_context	AmaderCAT corpus	https://doi.org/10.1017/nlp.2024.26 (2023)	https://doi.org/10.1007/978-981-15-9774-9_35 (2021)	The AmaderCAT corpus is used to train a Bengali-English neural machine translation model using a seq2seq architecture with a global attention mechanism. This dataset enables researchers to achieve a BLEU score of 22.3, facilitating advancements in bilingual translation quality between Bengali and English. The corpus's specific focus on these languages supports research in cross-lingual communication and translation technology.	
Indian Corpus	cited_context	Amritha paraphrase dataset	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)		The Amritha paraphrase dataset is used to study paraphrasing in four Indian languages: Hindi, Punjabi, Tamil, and Malayalam. Researchers focus on linguistic variations and semantic equivalence, employing the dataset to analyze how different expressions convey the same meaning across these languages. This enables a deeper understanding of linguistic structures and semantic relationships, supporting research in natural language processing and computational linguistics.	
Indian Corpus	cited_context	An Crúbadán	https://doi.org/10.48550/arXiv.2205.03983 (2022)	https://www.semanticscholar.org/paper/ef9f81a662aa9618173d033e95d46d74e14aeafc (2007)	An Crúbadán is used to build corpora for under-resourced languages by leveraging web data in approximately 2,000 languages. This dataset enhances language coverage, enabling researchers to develop linguistic resources and tools for languages with limited digital presence. The methodology involves collecting and processing web data to create comprehensive language corpora, which supports research in natural language processing, language documentation, and computational linguistics.	
Indian Corpus	cited_context	annotated dataset	https://doi.org/10.1007/978-3-319-64283-3_26 (2017)	https://www.semanticscholar.org/paper/03541b4b3006b75f206e855892ba4b0d36e49c7c (2010)	The annotated dataset is used to evaluate the accuracy of transliteration methods among Indian languages, specifically focusing on UTF-WX conversion. It serves as a benchmark for transliteration systems, enabling researchers to assess and compare the performance of different transliteration approaches. The dataset's annotated nature facilitates precise evaluation and validation of transliteration algorithms.	
Indian Corpus	citing_context	Assamese caption dataset	https://doi.org/10.1109/IALP61005.2023.10337310 (2023)	https://www.semanticscholar.org/paper/b51ce2126eadf25586b118b284cc5dfa0065b557 (2022)	The Assamese caption dataset is used to generate image captions in Assamese, addressing low-resource language processing challenges. Created by translating Flickr30K and MSCOCO using Microsoft Azure, this dataset supports research in natural language generation and machine translation, specifically for under-resourced languages. It enables the development and evaluation of models that can produce accurate and contextually appropriate captions in Assamese.	
Indian Corpus	citing_context	Babel dataset	https://doi.org/10.1109/ICASSP39728.2021.9414349 (2020)		The Babel dataset is used as an example of multi-lingual data that lacks detailed speaker profiling information. This highlights the need for more comprehensive datasets in the field, particularly to address the limitations in current multi-lingual datasets. The dataset's lack of detailed speaker information underscores gaps in research methodologies and emphasizes the importance of developing richer, more detailed linguistic datasets.	
Indian Corpus	cited_context	Bangla speech corpora	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)	https://doi.org/10.21437/SLTU.2018-11 (2018)	The Bangla speech corpora dataset is used to develop and evaluate speech processing systems for Bangla, focusing on the linguistic and phonetic characteristics of the language. Researchers employ this dataset to enhance system accuracy and robustness, addressing specific challenges related to Bangla's unique phonetic structure. This enables more effective speech recognition and synthesis applications for Bangla.	
Indian Corpus	cited_context | citing_context	BBC corpus	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1109/ICDSE.2014.6974627 (2014)	The BBC corpus is used to develop an abstractive text summarization system for Malayalam, specifically for translating and summarizing news articles. This involves employing natural language processing techniques to generate concise summaries while preserving key information. The dataset's relevance lies in its extensive collection of news articles, which facilitates the training and evaluation of summarization models tailored for the Malayalam language.	
Indian Corpus	citing_context	Bengali dataset containing 35,000 hate comments	https://doi.org/10.1109/GCON58516.2023.10183497 (2023)	https://doi.org/10.1007/978-981-16-0586-4_37 (2020)	The Bengali dataset containing 35,000 hate comments is primarily used for hate speech detection and classification in Bengali. It is employed to train and evaluate mono and multilingual transformer models, focusing on performance and accuracy. The dataset supports binary classification (hate vs. non-hate) and is used to enhance model understanding of Bengali language structures through pretraining techniques like mask language modeling. Additionally, it facilitates the detection of hate speech in various contexts, including political, personal, geopolitical, and religious comments, using methods such as multichannel CNN and LSTM-based techniques.	
Indian Corpus	cited_context	Bengali Hate Speech Dataset	https://www.semanticscholar.org/paper/34d652c3dd30a09a5db3205a2aa2a3fccb35cf8a (2022)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The Bengali Hate Speech Dataset is used to study hate speech in Bengali, specifically focusing on identifying and classifying offensive language in social media posts. Researchers employ this dataset to develop and evaluate machine learning models for detecting hate speech, addressing the challenge of offensive content in online platforms. The dataset's relevance lies in its application to improve content moderation and enhance understanding of linguistic patterns in hate speech.	
Indian Corpus	citing_context	Bengali news corpus	https://doi.org/10.1145/3584861 (2023)	https://www.semanticscholar.org/paper/4964224643572654da4dd92b4af3555782ec5d7f (2010)	The Bengali news corpus is used to train and evaluate machine learning models for sentiment classification, specifically focusing on opinion polarity identification in Bengali news text. Researchers employ support vector machine classifiers to form decision boundaries for sentiment analysis, enabling the accurate identification of positive and negative opinions in the dataset.	
Indian Corpus	cited_context	Bengali news portals	https://doi.org/10.1109/SMART46866.2019.9117554 (2019)	https://www.semanticscholar.org/paper/91fa6f6dfb09d3a38e500c2eafa3606b87924aa4 (2004)	The 'Bengali news portals' dataset is used to collect Bengali news articles for summarization tasks, focusing on the linguistic and stylistic aspects of Bengali news content. Researchers employ this dataset to analyze and develop summarization techniques tailored to the unique characteristics of the Bengali language, enhancing the accuracy and relevance of news summaries.	
Indian Corpus	cited_context	Bengali SentiWordnet	https://doi.org/10.1109/BigData.2016.7840818 (2016), https://www.semanticscholar.org/paper/bba6fd95578997b7d090277b21f1e4e290c8c7e2 (2010)	https://www.semanticscholar.org/paper/df9804cef3d63c9ca401639d13a2b5f6e58822fb (2010)	The Bengali SentiWordnet dataset is used to create a dictionary with sentiment scores for Bengali words, facilitating sentiment analysis and emotion labeling in Bengali text. It aids in recognizing enthusiastic expressions through fine-grained sentence-level tagging and supports the development of a polarity classifier by incorporating linguistic features to enhance the reliability of polarity scores.	
Indian Corpus	citing_context	Bhasha-Abhijnaanam	https://doi.org/10.1109/ACCESS.2024.3396290 (2024)		The Bhasha-Abhijnaanam dataset is used to fine-tune BERT models for Malayalam-English code-mixed data, enhancing performance in code-switching scenarios. It also sources romanized data for Indian languages, aiding in linguistic and transliteration research. This dataset enables researchers to address specific challenges in multilingual and transliteration contexts, leveraging its rich code-mixed and romanized content.	
Indian Corpus	citing_context	Bhashaverse	https://www.semanticscholar.org/paper/8ca047003dc2a9114b4f6571998d1619d2f4991b (2025)		The Bhashaverse dataset is used to sample sentences for Indian languages, primarily focusing on monolingual and parallel corpora. It supports language processing tasks by providing extensive monolingual corpora for training and evaluating language models, enhancing their performance in various Indian languages. This dataset enables researchers to improve the accuracy and robustness of language models through diverse linguistic data.	
Indian Corpus	cited_context	Bible and Quran transcriptions	https://doi.org/10.18653/v1/D18-1269 (2018)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The 'Bible and Quran transcriptions' dataset is primarily used to create parallel sentences in Urdu, enhancing translation models and quality. It provides linguistic data for both religious texts and movie subtitles, supporting the development and evaluation of natural language processing models, particularly for Indian languages like Hindi and Urdu. This dataset enables researchers to improve translation accuracy and model performance in specific contexts.	
Indian Corpus	citing_context	BioASQ	https://doi.org/10.48550/arXiv.2506.01615 (2025)		The BioASQ dataset is primarily used to evaluate and advance cross-lingual capabilities in question answering, information retrieval, and multilingual systems across diverse languages. It focuses on extractive QA, multilingual information retrieval, and neural machine translation. The dataset highlights the lack of Indian language coverage in specialized domains such as biomedical, scientific fact verification, and financial question answering, underscoring the need for more inclusive multilingual resources.	
Indian Corpus	cited_context	BIS Tagset	https://doi.org/10.18653/v1/2021.acl-long.105 (2021)	https://www.semanticscholar.org/paper/488ffec639da631ef22aa643144a9085f43e38ec (2012)	The BIS Tagset is used to annotate part-of-speech tags in the Konkani language, ensuring linguistic consistency and standardization in the tagging process. This dataset facilitates research focused on improving the accuracy and reliability of linguistic annotations, which is crucial for developing robust natural language processing tools and resources for the Konkani language.	
Indian Corpus	cited_context	BNLPC7	https://doi.org/10.18653/v1/2021.eacl-main.224 (2021)	https://doi.org/10.1109/ICCITECHN.2015.7488060 (2015)	The BNLPC7 dataset is used for experimenting with a framework for Bengali news document summarization. Researchers employ sentence frequency and clustering techniques to analyze the dataset. This approach helps in addressing the specific research question of improving summarization accuracy for Bengali news documents, leveraging the dataset's focus on Bengali language content.	
Indian Corpus	citing_context	Brahminet	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)	https://www.semanticscholar.org/paper/c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c (2020)	The Brahminet dataset is primarily used for linguistic processing and analysis of South Asian languages written in the Latin script. It focuses on transliteration tasks, evaluating model performance in converting Roman script to Indian scripts, and improving top-1 accuracy across 12 languages. The dataset also supports historical linguistics by studying ancient Indian scripts like Brahmi, and it aids in the compilation and representation of native words in these languages, though it may not be fully representative of all native words. Research using Brahminet emphasizes computational methods for transliteration, normalization, and accuracy improvements.	
Indian Corpus	citing_context	BullyExplain	https://doi.org/10.13053/cys-28-3-4989 (2024)	https://doi.org/10.18653/v1/2023.emnlp-main.1035 (2023)	The BullyExplain dataset is used to develop and evaluate explainable models for detecting cyberbullying in code-mixed languages, particularly focusing on the linguistic challenges and cultural nuances of Indian languages. This dataset enables researchers to address the complexities of mixed-language content, enhancing the accuracy and interpretability of cyberbullying detection systems.	
Indian Corpus	citing_context	BUSUM-BNLP	https://doi.org/10.1109/CEC60901.2024.10612059 (2024)	https://doi.org/10.1109/HORA58378.2023.10156794 (2023)	The BUSUM-BNLP dataset is used for implementing Bangla Extractive Update Summarization, specifically focusing on multi-document update summarization in the Bangla language. This dataset enables researchers to develop and evaluate algorithms that can generate concise summaries from multiple documents, updating existing summaries with new information. The dataset's relevance lies in its application to the Bangla language, addressing the need for effective summarization tools in this linguistic context.	
Indian Corpus	cited_context	CALCS data	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020)	https://doi.org/10.18653/v1/W18-3219 (2018)	The CALCS data is used to extend the dataset for sentiment analysis, building upon a 2018 shared task focused on named entity recognition in code-switched data. This extension enhances the dataset's utility for analyzing sentiment in multilingual contexts, particularly where code-switching is prevalent. The dataset enables researchers to develop and evaluate models that can accurately recognize and analyze named entities and sentiments in mixed-language texts.	
Indian Corpus	cited_context	CCMatrix	https://doi.org/10.1162/tacl_a_00452 (2021)	https://doi.org/10.18653/v1/2021.acl-long.507 (2019)	The CCMatrix dataset is used to analyze the limited representation of Indic languages in parallel data, focusing on the coverage of only 6 Indic languages. This analysis highlights gaps in linguistic resources, informing research on language diversity and resource scarcity in natural language processing.	
Indian Corpus	citing_context	CFILT-HiNER	https://doi.org/10.48550/arXiv.2212.10168 (2022)	https://doi.org/10.48550/arXiv.2204.13743 (2022)	The CFILT-HiNER dataset is used for Named Entity Recognition (NER) in Hindi, specifically for identifying and classifying named entities. It provides a large, human-annotated dataset with over 100k sentences, all annotated by a single expert. This dataset supports the development and evaluation of NER models, focusing on improving F1 scores through its extensive and diverse content.	
Indian Corpus	cited_context	chaii	https://doi.org/10.18653/v1/2022.emnlp-main.360 (2022)	https://doi.org/10.18653/v1/2020.acl-main.653 (2019)	The 'chaii' dataset is used to evaluate the performance of fine-tuned question generation (QG) models on cross-lingual tasks, particularly focusing on Indian languages. It assesses cross-lingual generalization and accuracy, testing models across multiple typologically diverse languages, including Indian languages. The dataset enables researchers to evaluate and improve multilingual robustness in QG and question answering models.	
Indian Corpus	cited_context	character segmentation from Bangla handwritten isolated words	https://doi.org/10.1109/ICDAR.2003.1227832 (2003)	https://doi.org/10.1109/ICDAR.1999.791809 (1999)	The dataset 'character segmentation from Bangla handwritten isolated words' is used to develop and evaluate character segmentation techniques for Bangla handwritten text. Researchers focus on isolating individual characters from words using recursive contour following. This methodology helps in improving the accuracy of character recognition in Bangla handwriting, addressing specific challenges in segmenting complex scripts. The dataset's relevance lies in its application to enhancing the performance of optical character recognition (OCR) systems for Bangla.	
Indian Corpus	citing_context	CISLR	https://doi.org/10.1109/ICCDS60734.2024.10560428 (2024)	https://doi.org/10.18653/v1/2022.emnlp-main.707 (2022)	The CISLR dataset is used to develop tools for Indian Sign Language (ISL) recognition, specifically to facilitate communication between deaf communities and the general population. Research employs this dataset to process ISL, enhancing the accuracy and effectiveness of sign language recognition systems. This enables the creation of more inclusive communication technologies.	
Indian Corpus	cited_context	code-mixed data obtained from online social media	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020), https://doi.org/10.1109/ACCESS.2021.3104106 (2021)	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020)	The code-mixed data obtained from online social media is used for sentiment analysis and shallow parsing of Hindi-English and Spanish-English tweets. Research focuses on achieving high F1-scores for sentiment classification and word-level identification in social media text, leveraging labeled tags to enhance accuracy. This dataset enables detailed analysis of multilingual content, facilitating the development of more effective natural language processing techniques for code-mixed data.	
Indian Corpus	citing_context	Common Crawl Oscar Corpus	https://doi.org/10.18653/v1/2024.findings-acl.857 (2024)	https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	The Common Crawl Oscar Corpus is used to address the scarcity of multilingual corpora, particularly for low-resource Indian languages like Angika. It promotes multilinguality and linguistic diversity by providing a large-scale parallel corpus for training and evaluation. This dataset supports multilingual research and development in Indian languages, enhancing capabilities through extensive parallel data, and highlights the underrepresentation of low-resource languages in existing corpora.	
Indian Corpus	citing_context	Common Voice	https://doi.org/10.48550/arXiv.2208.11761 (2022)	https://doi.org/10.1109/CVPR42600.2020.00737 (2020)	The Common Voice dataset is used for Automatic Speech Recognition (ASR) research, providing a diverse set of voice samples from global volunteers. This diversity supports the development and testing of ASR systems, enhancing their performance across various accents and speech patterns. The dataset's broad representation enables researchers to address challenges in speech recognition accuracy and robustness, particularly in multilingual environments.	
Indian Corpus	citing_context	CONSTRAINT 2021	https://doi.org/10.48550/arXiv.2210.04267 (2022)	https://doi.org/10.18653/v1/2021.woah-1.3 (2020)	The CONSTRAINT 2021 dataset is used to fine-tune and evaluate BERT variants for detecting abusive language in Indian languages, particularly focusing on hate speech and offensive content in social media. It enables researchers to address the nuanced challenges of identifying such content, enhancing model performance and reliability in real-world applications.	
Indian Corpus	citing_context	continuous Indian Sign Language dataset	https://doi.org/10.1145/3610661.3616550 (2023)	https://doi.org/10.21437/interspeech.2021-1094 (2021)	The continuous Indian Sign Language dataset is used to train and evaluate models for automatic speech-to-sign language generation. Research focuses on translating spoken words into sign language gestures, assessing accuracy, fluency, and the importance of prosody and facial action units. Models are compared using metrics like DTW and Probability of Correct Keypoints scores, emphasizing the dataset's role in advancing sign language generation methods.	
Indian Corpus	cited_context	Corpora Collection at Leipzig University	https://doi.org/10.48550/arXiv.2205.03983 (2022)	https://doi.org/10.18653/v1/2020.emnlp-main.480 (2019)	The Corpora Collection at Leipzig University is used to provide large-scale multilingual text corpora, including Indian languages, for cross-lingual research and natural language processing (NLP) tasks. It facilitates the creation of aligned web pages, Wikipedia articles, and parallel web-document pairs, enabling machine translation, training, and evaluation of language models. The dataset supports diverse and extensive language resources, enhancing multilingual NLP applications and linguistic research.	
Indian Corpus	cited_context	Corpus Asm	https://doi.org/10.3115/1667583.1667595 (2009)	https://doi.org/10.1145/1386869.1386871 (2008)	The Corpus Asm dataset is used to train and test a morphological tagger for Assamese, focusing on genre consistency within corpora derived from the same daily newspaper. It is also utilized to acquire and analyze the morphology of the Assamese language from a large text corpus of nearly 300,000 words, specifically from the Assamese daily Asomiya Pratidin. This dataset enables detailed morphological analysis and the development of linguistic tools for Assamese.	
Indian Corpus	cited_context	Cross-Script Code-Mixed Question Answering Corpus	https://doi.org/10.1007/978-3-319-73606-8_3 (2016)	https://www.semanticscholar.org/paper/1430bef50f67c743406f3dd909398931b0178508 (2016)	The Cross-Script Code-Mixed Question Answering Corpus is used to prepare datasets and evaluate research in code-mixed cross-script question answering, particularly for English–Bengali interactions. It focuses on the unique challenges of mixed-script queries in the sports and tourism domains, addressing multilingual and multilingual language processing challenges. This dataset enables researchers to study and develop methods for handling code-mixed data, enhancing the performance of question answering systems in diverse linguistic contexts.	
Indian Corpus	citing_context	CrossSum	https://doi.org/10.48550/arXiv.2310.18600 (2023)	https://doi.org/10.18653/v1/2023.acl-long.143 (2021)	The CrossSum dataset is used as a benchmark for multilingual and cross-lingual abstractive summarization, evaluating models' performance across over 1,500 language pairs, including Indian languages. It contains 1.7M article-summary samples, enabling researchers to develop and test summarization models that extend beyond English-centric approaches, focusing on generating accurate summaries in multiple languages.	
Indian Corpus	citing_context	Crowdsourced high-quality Tamil multi-speaker speech data set	https://doi.org/10.1109/ICACCS57279.2023.10112880 (2023)	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)	The Crowdsourced high-quality Tamil multi-speaker speech data set is used to train speech synthesis and recognition systems, enhancing model robustness and accuracy. It supports deep learning models for speech-to-text translation, specifically improving Tamil speech recognition. The multi-speaker nature of the dataset aids in developing more generalized and accurate models.	
Indian Corpus	citing_context	CrowS-Pairs	https://doi.org/10.1145/3677525.3678666 (2023), https://doi.org/10.48550/arXiv.2403.20147 (2024)	https://doi.org/10.18653/v1/2020.emnlp-main.154 (2020)	The CrowS-Pairs dataset is used to measure and address social biases in language models, particularly focusing on religion and caste categories. It is employed to create sentence pairs that help assess biases in masked language models. Additionally, it is used in conjunction with another dataset to compare US-centric biases related to race and gender with biases in Indian languages. This dataset enables researchers to quantitatively evaluate and understand the nature of biases in different cultural contexts.	
Indian Corpus	citing_context	CST5	https://doi.org/10.48550/arXiv.2304.13005 (2023)	https://doi.org/10.48550/arXiv.2211.07514 (2022)	The CST5 dataset is used for code-switched semantic parsing tasks, specifically focusing on Hindi-English utterances. It enhances data augmentation techniques by providing a rich set of code-switched examples, which improves the performance and robustness of natural language processing models in understanding and parsing mixed-language inputs.	
Indian Corpus	citing_context	custom dataset of 1500 questions over Wikipedia and news articles	https://doi.org/10.48550/arXiv.2309.15779 (2023)	https://doi.org/10.1109/iccica52458.2021.9697268 (2021)	The custom dataset of 1500 questions over Wikipedia and news articles is used to fine-tune Multilingual BERT for a Marathi language question answering system. This dataset focuses on enhancing performance in low-resource languages by providing a diverse set of questions, enabling researchers to improve the model's accuracy and robustness in understanding and answering queries in Marathi.	
Indian Corpus	citing_context	cvit	https://doi.org/10.48550/arXiv.2212.10180 (2022)	https://doi.org/10.18653/v1/D19-5215 (2019)	The cvit dataset is used to evaluate machine translation systems, particularly for Indian languages, in the context of the WAT-2019 competition. It focuses on assessing the performance and accuracy of these systems, providing a benchmark for comparing different translation models. The dataset's relevance lies in its specific application to Indian languages, enabling researchers to address the unique challenges and nuances of these languages in machine translation tasks.	
Indian Corpus	cited_context	Dakshina test set	https://doi.org/10.48550/arXiv.2305.15814 (2023), https://doi.org/10.48550/arXiv.2205.03018 (2022), https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)	https://www.semanticscholar.org/paper/c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c (2020)	The Dakshina test set is used to evaluate and improve transliteration, normalization, and machine translation systems for Indian languages, particularly those written in the Latin script. It supports linguistic analysis, script conversion, and cross-lingual tasks, including named entity recognition, information retrieval, and parallel text analysis. The dataset encompasses multiple Indian languages and scripts, enabling researchers to focus on accuracy, cross-lingual alignment, and domain-specific vocabulary. It highlights issues such as limited language coverage and ambiguous named entities, and is used to enhance cross-script communication and document relevance in information retrieval systems.	
Indian Corpus	cited_context | citing_context	Dakshina Dataset	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022), https://www.semanticscholar.org/paper/96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61 (2021)	https://www.semanticscholar.org/paper/c45d9d930543934efef0b7501596c5effd1857dd (2010), https://www.semanticscholar.org/paper/c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c (2020)	The Dakshina Dataset is used in research to process South Asian languages written in the Latin script, primarily for transliteration tasks. Researchers employ this dataset to evaluate the effectiveness of various transliteration methods, focusing on accuracy and performance. The dataset's relevance lies in its ability to support the development and testing of algorithms designed to handle the complexities of South Asian languages in the Latin script.; The Dakshina Dataset is used for processing South Asian languages written in the Latin script, specifically focusing on transliteration and cross-script processing. It contains 10,000 sentence pairs for 12 Indian languages, enabling researchers to address tasks such as transliteration and normalization. This dataset facilitates the development and evaluation of algorithms for handling linguistic data across different scripts, enhancing the accuracy and efficiency of language processing systems.	
Indian Corpus	citing_context	dataset	https://doi.org/10.11591/EEI.V10I5.3173 (2021)		The dataset is used to compile a multilingual corpus encompassing 9 Indian languages, specifically 5 Indo-Aryan and 4 Dravidian languages. It supports linguistic and computational analysis, enabling researchers to explore language structure, evolution, and computational processing across these diverse linguistic families. The dataset's comprehensive coverage of multiple Indian languages facilitates comparative studies and the development of language-specific computational tools.	
Indian Corpus	cited_context	DeitY Datasets	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)	https://www.semanticscholar.org/paper/1d480a2ae449ee625aa543610ed2574e3c87643e (2016)	The DeitY Datasets are used to provide resources for Indian languages, supporting linguistic research and development efforts in India. These datasets enable researchers to conduct studies focused on the linguistic characteristics and development of Indian languages, facilitating the creation of language-specific tools and resources.	
Indian Corpus	cited_context	corpus described in Arivazhagan et al. (2019)	https://doi.org/10.48550/arXiv.2205.03983 (2022)	https://www.semanticscholar.org/paper/c17985a669522e7e85ae3d34754c7df49c7187d1 (2019)	The corpus described in Arivazhagan et al. (2019) is extended to include 25 billion sentence pairs across 112 languages. It is primarily used for massively multilingual neural machine translation, enhancing the coverage and diversity of language pairs. This dataset enables researchers to develop more robust and inclusive translation models by providing a vast and varied linguistic resource.	
Indian Corpus	cited_context	Devanagari script recognition data resources	https://doi.org/10.1109/TSMCC.2010.2095841 (2011)	https://doi.org/10.1109/RIDE.2003.1249846 (2003)	The Devanagari script recognition data resources are used to develop and assess recognition systems, specifically focusing on character-level accuracy and performance in recognizing mixed numerals. This dataset enables researchers to evaluate the effectiveness of their models in accurately identifying Devanagari characters and numerals, contributing to advancements in optical character recognition technology for this script.	
Indian Corpus	citing_context	document summary pairs from printed NCTB books	https://www.semanticscholar.org/paper/693e0ff7b621e4d6d7a5e1738be13021214248ca (2022)	https://doi.org/10.18653/v1/2021.eacl-main.224 (2021)	The dataset of document summary pairs from printed NCTB books is used for unsupervised abstractive summarization of Bengali text documents. Researchers focus on generating concise summaries from these printed books, leveraging the dataset to develop and refine summarization models. Despite its non-public availability, the dataset enables advancements in natural language processing for Bengali, specifically in the domain of educational texts.	
Indian Corpus	citing_context	DOSA	https://doi.org/10.48550/arXiv.2506.15355 (2025)	https://doi.org/10.48550/arXiv.2406.05967 (2024)	The DOSA dataset is used to create a culturally-diverse multilingual visual question answering benchmark, emphasizing the role of local input in capturing diverse insights. It captures cultural artifacts through local collaboration, highlighting the importance of diverse perspectives in social artifacts. This dataset enables researchers to develop and evaluate models that better understand and represent cultural diversity.	
Indian Corpus	cited_context	Dravidian-CodeMix-FIRE 2020	https://www.semanticscholar.org/paper/ca0a072d666c43e2218ee11e6b81226a78d058f7 (2020)	https://www.semanticscholar.org/paper/16c9d493f59f05cebc57f87d14b4506e4350e28a (2020)	The Dravidian-CodeMix-FIRE 2020 dataset is used for sentiment analysis in Dravidian code-mixed social media texts. Researchers employ this dataset to enhance machine translation for under-resourced languages, focusing on improving the accuracy and effectiveness of sentiment analysis in multilingual contexts. This dataset enables the development and evaluation of models that can better handle the complexities of code-mixed data, thereby advancing natural language processing in these languages.	
Indian Corpus	cited_context	Dravidian-CodeMix-FIRE 2021	https://doi.org/10.18653/v1/2023.findings-emnlp.252 (2023)	https://doi.org/10.3390/BDCC3030037 (2019)	The Dravidian-CodeMix-FIRE 2021 dataset is primarily used to evaluate and improve the processing of code-mixed text, particularly focusing on Dravidian languages mixed with English. It is employed in sentiment analysis of Hinglish comments from YouTube, using semi-supervised approaches to understand viewer reactions. The dataset also supports the development and benchmarking of classifiers for multilingual text classification tasks, incorporating a wide range of Indian languages, including Manipuri, Sindhi, Sinhala, and Urdu. This enables researchers to assess performance across diverse linguistic contexts and enhance the understanding of mixed-language content.	
Indian Corpus	cited_context	EMILLE corpus	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020), https://www.semanticscholar.org/paper/bd3d1ef6eaec411c103d0f7d13b054ba3700ccd5 (2014)	https://www.semanticscholar.org/paper/6569c9b81b7721fff588ee58c7607367045e9af0 (2003)	The EMILLE corpus is used to support minority language engineering, particularly for South Asian languages, by providing linguistic data for language processing and computational linguistics. It enhances named entity recognition in Indian languages and improves English-to-Hindi machine translation, especially in domain-specific contexts like agriculture, addressing terminological and contextual challenges.	
Indian Corpus	cited_context	EMILLE/CIIL corpus	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	https://www.semanticscholar.org/paper/aaabc925599c264966388d79b24a28d2c458ae79 (2000)	The EMILLE/CIIL corpus is used to build a comprehensive corpus for South Asian languages, focusing on linguistic diversity across 14 languages with 92 million words. This dataset enables researchers to analyze and document the rich linguistic landscape of South Asia, supporting studies in language documentation, linguistic variation, and corpus linguistics.	
Indian Corpus	cited_context | citing_context	e-newspaper data of Telugu	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1007/978-981-10-5544-7_54 (2018)	The e-newspaper data of Telugu is used to train and evaluate text summarization models, specifically focusing on automatic keyword extraction in Telugu e-newspapers. This dataset helps improve the accuracy and relevance of generated summaries and keywords, making it valuable for enhancing natural language processing tasks in the Telugu language. The dataset's accessibility on Kaggle facilitates its use in these research areas.	
Indian Corpus	cited_context	English-Hindi dataset	https://doi.org/10.3115/1613692.1613697 (2006)	https://doi.org/10.3115/1119282.1119294 (2003)	The English-Hindi dataset is used to evaluate alignment error rates in machine translation. Researchers compare the performance of their proposed models against state-of-the-art models using this dataset. It enables the assessment of translation accuracy and alignment quality, providing a benchmark for model improvement and evaluation.	
Indian Corpus	cited_context	Europarl	https://doi.org/10.18653/v1/W15-3912 (2015)	https://www.semanticscholar.org/paper/694b3c58712deefb59502847ba1b52b192c413e5 (2005)	The Europarl dataset is used to provide parallel translation data for statistical machine translation, enabling the evaluation of translation quality across multiple language pairs. It supports multilingual applications, including those involving Indian languages, by facilitating the development and assessment of translation systems. This dataset's parallel nature is crucial for training and testing translation models.	
Indian Corpus	cited_context	Facebook Hindi word analogy dataset	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The Facebook Hindi word analogy dataset is used to evaluate Hindi word embeddings by assessing their performance on semantic and syntactic analogies. Researchers employ this dataset to measure the quality and effectiveness of word embeddings, focusing on how well they capture linguistic relationships. This evaluation helps in refining embedding models and improving their applicability in natural language processing tasks.	
Indian Corpus	cited_context	fake news dataset	https://www.semanticscholar.org/paper/b5673532a3c39d7d8410f9e3d67b4a4a4ba26df4 (2020)	https://doi.org/10.1016/j.marpolbul.2018.01.042 (2018)	The fake news dataset is used to generate new annotated data in Urdu by translating the original English dataset. This process focuses on creating multilingual resources for fake news detection, enhancing the availability of annotated data in non-English languages. The dataset enables researchers to develop and test algorithms for identifying fake news in multilingual contexts, specifically addressing the need for robust fake news detection tools in Urdu.	
Indian Corpus	citing_context	FDMSE-ISL	https://doi.org/10.48550/arXiv.2407.14224 (2024)	https://doi.org/10.1109/ACCESS.2020.3028072 (2020)	The FDMSE-ISL dataset is used to test five different models for Indian sign language recognition using multi-modal data. This dataset enables researchers to evaluate model performance in recognizing signs from various modalities, contributing to the development of more accurate and robust sign language recognition systems.	
Indian Corpus	cited_context	FinalNepaliCorpus	https://doi.org/10.1145/3606696 (2023)	https://doi.org/10.5120/12217-8374 (2013)	The FinalNepaliCorpus dataset is used to develop a Support Vector Machine-based Part-of-Speech (POS) tagger for the Nepali language. It provides a dictionary of words and their corresponding parts of speech, enabling researchers to accurately tag text data. This methodology supports linguistic analysis and natural language processing tasks specific to Nepali.	
Indian Corpus	cited_context | citing_context	FIRE dataset	https://www.semanticscholar.org/paper/05a2c02a917c839e3d00b9bf2012ad6ef316fd52 (2023), https://doi.org/10.48550/arXiv.2312.09508 (2023)	https://doi.org/10.1145/3476415.3476418 (2021), https://www.semanticscholar.org/paper/43f2ad297941db230c089ba353efc3f281ab678c (2020)	The FIRE dataset is used to build reusable, open-source test collections for information retrieval, serving as benchmark datasets to evaluate retrieval systems. It is also utilized to train models on domain-specific text from newspaper articles, addressing the challenge of generalizing these models to other domains. This dataset enables researchers to assess and improve the performance of information retrieval systems in specific contexts.; The FIRE dataset is used to train models on domain-specific text from newspaper articles, focusing on the challenges of generalizing these models to other domains. This dataset enables researchers to explore and address issues related to model adaptability and domain specificity in natural language processing tasks.	
Indian Corpus	cited_context | citing_context	FIRE 2014 data	https://doi.org/10.48550/arXiv.2204.13743 (2022)	https://www.semanticscholar.org/paper/d294182e2b5b1a91eb2ce9b148d0b15f9becaa47 (2014)	The FIRE 2014 dataset is primarily used for Named Entity Recognition (NER) in multilingual contexts, specifically focusing on Hindi, Tamil, Malayalam, and English. It is employed to train and evaluate NER systems, enhancing cross-lingual performance and entity identification accuracy. The dataset supports research on multilingual and code-switched data, demonstrating promising results in Hindi-English code-switching with a 75.96% F1 score. Key entities include Person, Location, and Organization, which are crucial for improving information extraction systems.; The FIRE 2014 data is primarily used for Named Entity Recognition (NER) in multilingual settings, specifically focusing on Hindi, Tamil, Malayalam, and English. It is employed to train and evaluate NER systems, enhancing cross-lingual performance and entity identification accuracy. The dataset supports supervised learning methods and evaluates performance metrics like F1 scores, particularly in Hindi-English code-switched data, achieving notable results.	FIRE 2014
Indian Corpus	cited_context	FIRE 2014 workshop for NER	https://www.semanticscholar.org/paper/d938d15e64f8a1d9ca2935faeb36b9ecdcefbfc8 (2020)	https://doi.org/10.1145/3238797 (2018)	The FIRE 2014 workshop for NER dataset is used to train and evaluate Named Entity Recognition (NER) models for five Indian languages: Hindi, Tamil, Bengali, Malayalam, and Marathi. It focuses on enhancing tagging performance in low-resource settings, employing methodologies that address the challenges of limited annotated data. This dataset enables researchers to develop and test NER systems that can accurately identify named entities in these languages, contributing to advancements in multilingual NLP.	
Indian Corpus	cited_context | citing_context	FIRE-2015	https://doi.org/10.3390/bdcc4010003 (2020), https://doi.org/10.1007/978-3-319-73606-8_3 (2016)	https://doi.org/10.1109/ICACCI.2016.7732099 (2016), https://doi.org/10.1145/2600428.2609622 (2014)	The FIRE 2015 dataset is used for sentiment analysis tasks on mixed script Indic languages. Researchers focus on evaluating the effectiveness of sentiment analysis models, employing methodologies that assess model performance on these specific linguistic challenges. This dataset enables detailed analysis of sentiment in multilingual contexts, providing insights into the nuances of mixed script processing and model accuracy.; The FIRE-2015 dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on its application, methodology, research questions, or specific characteristics. Therefore, based on the provided evidence, it cannot be accurately described how this dataset is used in research.	FIRE 2015
Indian Corpus	cited_context	FIRE-2015 Mixed Script Information Retrieval	https://www.semanticscholar.org/paper/5bb962568be873c59217f01c4ad8e072f9cfa3fc (2018)	https://doi.org/10.1007/978-3-319-26832-3_61 (2015)	The FIRE-2015 Mixed Script Information Retrieval dataset is used to evaluate information retrieval systems for code-mixed Indian languages, including Bengali, Hindi, Tamil, and others mixed with English. It supports research into multilingual and mixed-script processing, focusing on system performance in understanding and classifying sentiments in social media data like tweets.	
Indian Corpus	cited_context	FIRE-2016 MSIR	https://doi.org/10.1145/3574318.3574328 (2022)	https://doi.org/10.1007/978-3-319-73606-8_3 (2016)	The FIRE-2016 MSIR dataset is used for hate speech and offensive content identification in English and Indo-Aryan languages, enhancing detection accuracy. It is also employed for mixed script information retrieval tasks, facilitating cross-script search and retrieval in multilingual environments. This dataset supports research in improving the effectiveness of content moderation and multilingual information access systems.	
Indian Corpus	cited_context	FIRE-2022	https://www.semanticscholar.org/paper/12a5c581814d9f0163b686f075386715789af492 (2022)	https://www.semanticscholar.org/paper/693e0ff7b621e4d6d7a5e1738be13021214248ca (2022)	The FIRE-2022 dataset is used to examine and validate summarization approaches in Indian languages, particularly Hindi, English, and Gujarati. It provides datasets for the ILSUM-2022 shared task, focusing on the challenges and methodologies in Indian language summarization. Researchers use this dataset to test and evaluate summarization models, assessing their performance across various Indian languages and addressing specific summarization challenges.	
Indian Corpus	citing_context	FIRE 2022 ILSUM Track	https://www.semanticscholar.org/paper/05a2c02a917c839e3d00b9bf2012ad6ef316fd52 (2023)	https://doi.org/10.18653/v1/P16-3015 (2019)	The FIRE 2022 ILSUM Track dataset is used to develop automatic summarization systems for resource-poor Indian languages, addressing the lack of available resources. Researchers employ this dataset to create and evaluate summarization models, focusing on improving the generation of concise and coherent summaries in under-resourced linguistic contexts. This dataset enables the advancement of natural language processing techniques specifically tailored for Indian languages.	
Indian Corpus	citing_context	Flickscore	https://doi.org/10.1109/TCE.2023.3324009 (2024)	https://www.semanticscholar.org/paper/a456a694a9efeee8984542c66650def1c9fd696a (2018)	The Flickscore dataset is used to build a classification-based movie recommendation system for Indian regional Hindi language movies. It focuses on user ratings and movie attributes to enhance recommendation accuracy. This dataset enables researchers to develop and test algorithms that improve personalized movie recommendations, specifically tailored to the preferences of Hindi-speaking audiences.	
Indian Corpus	cited_context	FLORES	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021)	https://www.semanticscholar.org/paper/a70fd36953d654cd585046c43815fa9ce5187172 (2020)	The FLORES dataset is primarily used for evaluating machine translation systems in low-resource settings, particularly for Nepali-English and Sinhala-English translations. It is also employed to assess multilingual task-oriented semantic parsing, cross-lingual transfer performance, and cross-lingual generalization in natural language understanding tasks. These evaluations focus on robustness and performance across multiple languages, including some Indian languages, enhancing the reliability of NLP models in diverse linguistic contexts.	
Indian Corpus	cited_context	FLORES101/200	https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	https://doi.org/10.1162/tacl_a_00474 (2021)	The FLORES101/200 dataset is used to evaluate the retrieval capabilities of models, particularly focusing on the Indic parts to assess multilingual machine translation performance. It enables researchers to test and improve the accuracy of translations involving Indian languages, providing a benchmark for model performance in multilingual contexts.	
Indian Corpus	citing_context	Flores-200	https://doi.org/10.18653/v1/2024.naacl-long.425 (2024)	https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	The Flores-200 dataset is primarily used for evaluating the performance of machine translation and sentiment analysis models in Indian languages. It provides parallel corpora for 12 to 24 Indian languages, enabling researchers to assess Google Translate's accuracy and the BLEU scores of both Google-translated and human-translated datasets. The dataset is also utilized for training and evaluating sentiment analysis models, focusing on the correctness and preservation of sentiment in translated text. Specific methodologies include comparing manual and automated translations, tabulating translation statistics, and benchmarking model performance on sentiment analysis tasks.	
Indian Corpus	cited_context	FM-IQA	https://doi.org/10.48550/arXiv.2406.05967 (2024)	https://doi.org/10.18653/v1/2020.aacl-main.90 (2020)	The FM-IQA dataset is used to benchmark and evaluate multilingual visual question answering and reasoning, focusing on the system's ability to integrate and understand textual and visual information across different languages. It is applied to assess cross-lingual performance, handle questions in multiple languages, and study multilingual and code-mixed visual question answering, emphasizing cross-lingual and cross-cultural understanding.	
Indian Corpus	cited_context	dataset for aspect based sentiment analysis (ABSA) in Hindi	https://www.semanticscholar.org/paper/8ffefa0acd2d23e9a3b8315aea0aa6d7a3292205 (2016)	https://doi.org/10.3115/v1/S14-2004 (2014)	The dataset for aspect-based sentiment analysis (ABSA) in Hindi is used to train and evaluate models that identify and classify sentiments towards specific aspects of entities. This involves methodologies focused on sentiment extraction and classification, enabling researchers to address questions related to the nuanced understanding of opinions in Hindi text. The dataset's relevance lies in its application to Indian language research, specifically enhancing the accuracy and reliability of ABSA models in Hindi.	
Indian Corpus	cited_context	FT-WC	https://doi.org/10.18653/v1/2023.findings-emnlp.252 (2023)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The FT-WC dataset is used to evaluate and compare pre-trained word embeddings, specifically those from the IndicNLPSuite and Wiki+CommonCrawl, against the authors' IndiSocialFT embeddings. Research focuses on performance in the native script setting, assessing embedding quality and effectiveness in Indian languages. This dataset enables direct comparison and benchmarking of different embedding models, facilitating advancements in natural language processing for Indian languages.	
Indian Corpus	cited_context	Gujarati (Google, 2019a)	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)		The Gujarati (Google, 2019a) dataset is used to develop and evaluate natural language processing (NLP) models for various Indian languages, including Gujarati. It focuses on addressing language-specific challenges and resource availability. The dataset enables researchers to train and test NLP models, enhancing their performance and adaptability to the unique linguistic features of these languages.	
Indian Corpus	cited_context	GupShup	https://doi.org/10.48550/arXiv.2211.07514 (2022)	https://doi.org/10.18653/v1/2021.eacl-main.87 (2021)	The GupShup dataset is used to develop and test Hinglish conversation summarization systems, focusing on natural language processing in mixed-language contexts. It is also employed alongside TOPv2 to evaluate the effectiveness of code-switching in task-oriented semantic parsing, demonstrating significant gains in exact match accuracy. This dataset enables research by providing a rich resource for testing and improving systems that handle linguistic diversity and code-switching.	
Indian Corpus	citing_context	Hand gestures for emergency situations	https://doi.org/10.1007/s13369-022-06843-0 (2022)	https://doi.org/10.1016/j.dib.2020.106016 (2020)	The 'Hand gestures for emergency situations' dataset is used to study dynamic hand gestures in Indian sign language, specifically focusing on video data of emergency-related words. Researchers employ this dataset to enhance the understanding and recognition of these gestures, using video analysis methods to improve communication in emergency scenarios. The dataset's video format and focus on specific emergency terms enable detailed gesture recognition studies.	
Indian Corpus	cited_context	HASOC 2020	https://www.semanticscholar.org/paper/71f727f5c3efbed6aefecff4660ebf76d48efbae (2021)	https://www.semanticscholar.org/paper/1034b822874263b1fd7388cab447253362b95c73 (2020)	The HASOC 2020 dataset is used to identify trends in offensive language in Dravidian languages, particularly focusing on code-mixed data. Researchers employ this dataset to evaluate models designed to detect offensive content. The dataset's code-mixed characteristic is crucial for assessing model performance in multilingual contexts. This enables more nuanced understanding and improvement of offensive language detection systems in Dravidian languages.	
Indian Corpus	citing_context	HASOC Hindi	https://doi.org/10.1109/GCON58516.2023.10183497 (2023)	https://www.semanticscholar.org/paper/34d652c3dd30a09a5db3205a2aa2a3fccb35cf8a (2022)	The HASOC Hindi dataset is used to evaluate and compare mono and multilingual transformer models in hate speech detection. Research focuses on model performance and cross-language evaluation, leveraging the dataset's annotated content to assess effectiveness in identifying hate speech across different languages. This enables researchers to understand the strengths and limitations of multilingual models in handling hate speech in Hindi.	
Indian Corpus	cited_context	Hateful speech detection in public facebook pages for the bengali language	https://doi.org/10.1007/978-981-16-0586-4_37 (2020)	https://doi.org/10.1109/ICMLA.2019.00104 (2019)	The dataset is used to detect various forms of harmful speech in Bengali social media, including hate speech, toxicity, and abusive language. Researchers annotate and classify texts into multiple categories (ranging from 5 to 7 classes) to evaluate supervised models and deep learning methods. The dataset's size (4,700 to 10,219 samples) and diverse classification schemes enable robust performance comparisons across different datasets and methodologies, addressing the challenge of insufficient data in this domain.	
Indian Corpus	cited_context	Hate Speech detection in the Bengali language: A dataset and its baseline evaluation	https://doi.org/10.1007/978-981-16-0586-4_37 (2020)	https://doi.org/10.1109/ICISET.2018.8745565 (2018)	The dataset is used to detect hate speech in the Bengali language, particularly focusing on abusive comments in social media. It contains 30,000 annotated samples, which enable researchers to evaluate baseline models using deep learning techniques. Additionally, the dataset has been utilized to translate an English hate speech dataset into Bengali, enhancing the study of linguistic features and improving model performance.	
Indian Corpus	citing_context	hate speech in Hindi language dataset	https://doi.org/10.48550/arXiv.2311.09086 (2023)	https://doi.org/10.1145/3441501.3441517 (2020)	The hate speech in Hindi language dataset is used to identify and classify hate speech and offensive content in Hindi, primarily sourced from Twitter. It consists of 5,000 to 6,000 posts and focuses on the linguistic and contextual aspects of hate speech. Research methodologies include classification and detection techniques, enabling studies to analyze the nuances and patterns of offensive language in social media contexts.	
Indian Corpus	citing_context	Hellwig’s SanskritTagger corpus	https://doi.org/10.1109/ICICT57646.2023.10134024 (2023)	https://doi.org/10.1007/978-3-642-00155-0_11 (2009)	The Hellwig’s SanskritTagger corpus is used for training and re-analyzing sentences to achieve gold-standard accuracy in lexical and part-of-speech (POS) tagging for Sanskrit. This dataset enables researchers to focus on improving the precision of tagging algorithms, enhancing the accuracy of linguistic analyses in Sanskrit texts. Its specific characteristics, such as annotated lexical and POS data, facilitate robust training and evaluation of tagging models.	
Indian Corpus	cited_context	HH-RLHF-T RANSLATED	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	https://doi.org/10.48550/arXiv.2204.05862 (2022)	The HH-RLHF-T RANSLATED dataset is used to translate and transliterate toxic prompts and non-toxic answers into 14 Indian languages. It focuses on reinforcing helpful and harmless responses in multilingual settings, employing methodologies that ensure the translation maintains the intent and tone of the original content. This dataset enables research on improving the safety and effectiveness of AI models in diverse linguistic environments.	
Indian Corpus	cited_context	HindEnCorp	https://doi.org/10.3115/v1/W14-3309 (2014)	https://www.semanticscholar.org/paper/bd3d1ef6eaec411c103d0f7d13b054ba3700ccd5 (2014)	The HindEnCorp dataset is used to train phrase translation tables for enhancing machine translation performance, specifically for the Hindi-English and Urdu-English language pairs. This involves using the dataset to improve the quality of translations between these Indian languages and English. The dataset's focus on these language pairs enables researchers to address specific challenges in translating Indian languages, thereby improving overall translation accuracy and fluency.	
Indian Corpus	cited_context	Hindi dataset (Modha et al., 2021)	https://www.semanticscholar.org/paper/34d652c3dd30a09a5db3205a2aa2a3fccb35cf8a (2022)	https://doi.org/10.1145/3503162.3503176 (2021)	The Hindi dataset (Modha et al., 2021) is primarily used for identifying hate speech and offensive content in Hindi. It supports sub-tasks A and B for classification and detection. Researchers employ this dataset to fine-tune and evaluate the performance of the XLM-R Large model, focusing on improving classification accuracy for hate speech and offensive content. The dataset's relevance lies in its application to natural language processing tasks, particularly in the context of Indian languages.	
Indian Corpus	cited_context	Hindi-Bhojpuri word cognate pairs	https://www.semanticscholar.org/paper/12276816a1f94c1ec817a4b381de2eb8bcb96ef3 (2018)	https://doi.org/10.18653/v1/W17-4009 (2017)	The Hindi-Bhojpuri word cognate pairs dataset is used to address the out-of-vocabulary problem in machine translation for resource-scarce languages, specifically focusing on Hindi and Bhojpuri. Researchers employ this dataset to compare their methods with state-of-the-art techniques, enhancing word transduction accuracy. This dataset facilitates the development and evaluation of translation models by providing cognate pairs that help in handling unknown words effectively.	
Indian Corpus	cited_context	Hindi-Bhojpuri word pairs	https://www.semanticscholar.org/paper/12276816a1f94c1ec817a4b381de2eb8bcb96ef3 (2018)	https://doi.org/10.3115/1073336.1073356 (2001)	The Hindi-Bhojpuri word pairs dataset is used to train models for translation lexicon induction, specifically focusing on word pairs with the same meaning and similar pronunciations in both languages. This dataset enables researchers to develop more accurate and contextually relevant translation models by leveraging the phonetic and semantic similarities between Hindi and Bhojpuri.	
Indian Corpus	cited_context | citing_context	Hindi Dataset	https://doi.org/10.1109/GCON58516.2023.10183497 (2023), https://www.semanticscholar.org/paper/34d652c3dd30a09a5db3205a2aa2a3fccb35cf8a (2022)	https://www.semanticscholar.org/paper/a6e640ef8db9f18a9c6bd7266842a5558762d651 (2019)	The Hindi Dataset is used to evaluate hate speech identification models, particularly focusing on subtask B. Researchers employ metrics such as Macro-F1 and Weighted-F1 to assess model performance. This dataset enables the development and testing of algorithms designed to detect and classify hate speech in Hindi, contributing to the broader field of natural language processing and content moderation.; The Hindi Dataset is used to evaluate model performance on hate speech identification in Hindi, focusing on metrics such as Marco-F1 and Weighted-F1 scores. Specifically, it is employed to assess BERT models on sub-task C of hate speech detection. This dataset enables researchers to measure and compare the effectiveness of different models in identifying hate speech within the Hindi language.	
Indian Corpus	cited_context | citing_context	Hindi EmotionNet	https://doi.org/10.1145/3461764 (2021), https://doi.org/10.1145/3383330 (2020)	https://doi.org/10.1145/3383330 (2020), https://doi.org/10.1097/00005053-198205000-00017 (1982)	The Hindi EmotionNet dataset is used for sentiment analysis in Hindi, focusing on classifying emotional content in text. It is employed to train and evaluate machine learning models for sentiment analysis tasks, enabling researchers to assess the accuracy and effectiveness of these models in understanding and categorizing emotional expressions in Hindi text.; The Hindi EmotionNet dataset is used to examine inconsistencies and coverage of emotions in Hindi, specifically comparing it with established emotion models like those by Ekman and Plutchik. This involves analyzing the dataset to assess how well it aligns with these models, thereby providing insights into the representation and categorization of emotions in the Hindi language.	
Indian Corpus	cited_context	Hindi-English code mixed dataset	https://www.semanticscholar.org/paper/82200516b244b785cfd2f08ef990fc54e41f4020 (2021)	https://www.semanticscholar.org/paper/5ef6d4204258fc5bfa5575d4ec81b968f8b4dcdc (2018)	The Hindi-English code mixed dataset is used to evaluate deep learning models for hate speech detection. Researchers focus on the performance of 1D CNN, LSTM, and BiLSTM models using domain-specific word embeddings. This dataset enables the assessment of these models' effectiveness in identifying hate speech within code-mixed content, providing insights into model performance and potential improvements.	
Indian Corpus	citing_context	Hindi-English code-mixed social media text	https://www.semanticscholar.org/paper/3287e77baac0c7fca4255eabe5184910bdc93b80 (2021)	https://doi.org/10.18653/v1/W18-1105 (2018)	The Hindi-English code-mixed social media text dataset is primarily used to detect hate speech in multilingual online discourse. Researchers focus on identifying linguistic patterns and contextual cues specific to code-mixed content. This dataset enables the development and evaluation of models that can accurately recognize and classify hate speech, contributing to the broader understanding of multilingual online communication and its challenges.	
Indian Corpus	cited_context | citing_context	Hindi-English code-mixed tweets	https://doi.org/10.48550/arXiv.2306.14764 (2023), https://doi.org/10.18653/v1/W18-1105 (2018)	https://doi.org/10.18653/v1/W18-1105 (2018), https://doi.org/10.1007/978-3-319-73606-8_3 (2016)	The Hindi-English code-mixed tweets dataset is used to develop and evaluate hate speech detection models. Researchers focus on annotating code-mixed social media text to distinguish between hate speech and normal speech. This dataset enables the creation of more nuanced and context-aware models by leveraging the unique linguistic features of code-mixed content.; The Hindi-English code-mixed tweets dataset is used for information retrieval tasks, particularly to retrieve the top k tweets for queries containing Hindi terms in Roman transliterated form. This dataset enables researchers to develop and test algorithms that can effectively handle mixed-language content, enhancing the accuracy of search results in multilingual environments.	
Indian Corpus	citing_context	Hindi HASOC Hate Speech Dataset 2021	https://doi.org/10.48550/arXiv.2306.14764 (2023)	https://www.semanticscholar.org/paper/82200516b244b785cfd2f08ef990fc54e41f4020 (2021)	The Hindi HASOC Hate Speech Dataset 2021 is used to fine-tune the RoBERTa model for detecting hate speech in Hindi, specifically targeting abusive and offensive content on social media. This dataset enables researchers to improve the accuracy of hate speech detection algorithms by providing annotated data that reflects the nuances of Hindi language usage in online platforms.	
Indian Corpus	citing_context	Hindi language on offensive speech	https://doi.org/10.48550/arXiv.2311.09086 (2023)	https://www.semanticscholar.org/paper/a459724d08d68687f91f707e4608869aa0f9c801 (2020)	The Hindi language offensive speech dataset is used to analyze linguistic patterns and context of offensive content in social media, specifically examining 2,000 posts from Twitter and Facebook. Researchers employ this dataset to study the nature and prevalence of offensive speech, focusing on how such content is expressed and contextualized within Hindi-language online communities. This analysis helps in understanding the sociolinguistic dynamics of offensive language and its impact.	
Indian Corpus	citing_context	Hindi news corpora	https://doi.org/10.1145/3624013 (2023)	https://doi.org/10.1109/CICT.2013.6558271 (2013)	The Hindi news corpora dataset is used to evaluate an extractive summarization approach for Hindi news articles. Researchers focus on linguistic and statistical features to rank sentences, enabling the development and assessment of summarization techniques tailored to the Hindi language. This dataset supports the analysis of sentence importance and coherence in news articles, facilitating advancements in natural language processing for Indian languages.	
Indian Corpus	citing_context	Hindi sentiment analysis dataset	https://doi.org/10.18653/v1/2023.wassa-1.12 (2023)	https://www.semanticscholar.org/paper/8ffefa0acd2d23e9a3b8315aea0aa6d7a3292205 (2016)	The Hindi sentiment analysis dataset is primarily used for sentiment analysis in various Indian languages, including Telugu, Tamil, Malayalam, and Bengali, as well as Hindi. It provides annotated data for training and evaluating machine learning models. This dataset enables researchers to develop and test algorithms that can accurately classify sentiments in text, facilitating advancements in natural language processing for Indian languages.	
Indian Corpus	cited_context	Hindi-SentiWordNet (H-SWN)	https://www.semanticscholar.org/paper/267e59a7cdf328f3ba14e7295ca68d4a7839b984 (2010)	https://doi.org/10.1109/ICDEW.2008.4498370 (2008)	The Hindi-SentiWordNet (H-SWN) dataset is used to develop and evaluate sentiment analysis models, particularly for Hindi movie reviews, focusing on classifying positive and negative sentiments. It is constructed as a lexical resource, leveraging sentiment scores and word associations from the English SentiWordNet to adapt and create a Hindi version. This adaptation facilitates multilingual sentiment analysis by linking English and Hindi words, enabling more accurate sentiment classification in Hindi text.	
Indian Corpus	cited_context	Hindi translations of the semantically neutral templates	https://doi.org/10.18653/v1/2022.naacl-main.76 (2021)	https://doi.org/10.18653/v1/N19-1063 (2019)	The dataset 'Hindi translations of the semantically neutral templates' is used to measure social biases in sentence encoders, focusing on the presence of biased language in Indian language contexts. Researchers employ this dataset to analyze and quantify biases, using the translated templates to evaluate encoder performance and identify specific instances of bias. This enables a deeper understanding of how language models handle and propagate social biases in Hindi.	
Indian Corpus	citing_context	Hindi tweets for the political domain	https://doi.org/10.3390/bdcc4010003 (2020)	https://doi.org/10.1109/BigData.2016.7840818 (2016)	The 'Hindi tweets for the political domain' dataset is used to analyze sentiment in tweets related to the Indian election. Researchers employ this dataset to study Indian languages in the political domain, focusing on understanding public opinion and sentiment expressed in Hindi tweets. This dataset enables detailed sentiment analysis, providing insights into electoral dynamics and public discourse.	
Indian Corpus	cited_context	Hindi web texts (HWT)	https://www.semanticscholar.org/paper/bd3d1ef6eaec411c103d0f7d13b054ba3700ccd5 (2014)	https://www.semanticscholar.org/paper/5da91f47d49556c946fe759c46c9e08a8c26b50c (2008)	The Hindi web texts (HWT) dataset is used to compile a monolingual corpus of Hindi news articles, which supports research in Hindi language processing and translation. This dataset enables researchers to develop and evaluate natural language processing techniques specific to Hindi, enhancing translation models and language understanding systems.	
Indian Corpus	cited_context	Hindi WordNet	https://www.semanticscholar.org/paper/ce9125c04985e5be724709d453bdcfd74cd9bdf3 (2008), https://doi.org/10.29007/MXDH (2018)	https://www.semanticscholar.org/paper/8c4fdc361997a98f58a03b874c568f8972ff2079 (2011)	The Hindi WordNet dataset is used as a lexical resource for Hindi, providing word meanings and relationships. It is employed to verify antonymy relations, focusing on lexical semantics in Hindi language processing. Additionally, it serves as a foundational resource to build WordNets for 16 other Indian languages through an expansion approach, leveraging its existing lexical structures.	
Indian Corpus	cited_context | citing_context	HiNER	https://doi.org/10.48550/arXiv.2204.13743 (2022)	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	The HiNER dataset is used to evaluate pre-trained language models on named entity recognition tasks, specifically focusing on identifying Person, Location, and Organization entities. It serves as a benchmark for assessing zero-shot performance, enabling researchers to compare the effectiveness of different models in recognizing named entities without task-specific training.; The HiNER dataset is used to evaluate pre-trained language models on named entity recognition tasks, specifically focusing on identifying Person, Location, and Organization tags. It serves as a benchmark for assessing zero-shot performance, enabling researchers to compare the effectiveness of different models in recognizing entities without prior training on the dataset.	
Indian Corpus	citing_context	HiNER Dataset	https://doi.org/10.48550/arXiv.2405.04829 (2024)	https://www.semanticscholar.org/paper/887d8d4bae488941d9a3a26ec39122abc84fad79 (2010)	The HiNER Dataset is used for Named Entity Recognition (NER) in Hindi, integrating annotated data from the ILCI tourism domain and a subset of the news domain corpus. It focuses on enhancing NER performance in Hindi by providing a rich, domain-diverse dataset that supports the development and evaluation of NER models.	
Indian Corpus	cited_context	Hinglish	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020)		The Hinglish dataset is used to compile a corpus of 20K Hindi-English tweets, specifically focusing on code-switching patterns and linguistic features in social media. Researchers employ this dataset to analyze how users switch between Hindi and English within the same tweet, providing insights into the linguistic behavior and cultural dynamics of multilingual online communities.	
Indian Corpus	cited_context	Hinglish sarcasm tweets	https://www.semanticscholar.org/paper/5fd2099e9f4c6d9f748685a3bd11460ce6043914 (2020)	https://www.semanticscholar.org/paper/f696d60b22e0336472cb84210ad930d8b5fca460 (2018)	The Hinglish sarcasm tweets dataset is used for pretraining models to detect sarcasm in Hinglish tweets, specifically focusing on the linguistic nuances of Hindi written in Roman script. This dataset enables researchers to develop and refine algorithms that can accurately identify sarcastic content, addressing the unique challenges posed by the mixed-language nature of Hinglish.	
Indian Corpus	cited_context	HopeEDI	https://www.semanticscholar.org/paper/3ca6eb7b69da71899ff2d20d6a65f3e668b97312 (2021)	https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c (2020)	The HopeEDI dataset is used to detect hope speech in multilingual YouTube comments, specifically focusing on themes of equality, diversity, and inclusion. Comprising 59,354 comments in English, Tamil, and Malayalam, the dataset supports research into the linguistic and contextual nuances of positive, supportive messages across these languages. This enables the development and evaluation of natural language processing models tailored to identify and promote inclusive content.	
Indian Corpus	citing_context	Hostility Detection Dataset in Hindi	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	https://www.semanticscholar.org/paper/5256e019b6d5cc8ec9fb5fa351cde55fce9c9412 (2020)	The Hostility Detection Dataset in Hindi is used to identify aggressive or threatening language in social media posts. Researchers employ this dataset to develop and test algorithms that can detect hostile content, focusing on the nuances of Hindi language usage. This enables the study of online hostility and the development of tools to mitigate harmful interactions.	
Indian Corpus	cited_context	HSWN	https://www.semanticscholar.org/paper/bdf63f60d81ecf46d030467fac1daf6d0d97c072 (2013)	https://doi.org/10.1561/1500000011 (2008)	The HSWN dataset is used for sentiment analysis in Hindi reviews, specifically focusing on discourse and negation to enhance the accuracy of opinion mining. Researchers employ this dataset to develop and refine algorithms that better capture nuanced sentiments in text, improving the overall performance of sentiment analysis systems in Hindi.	
Indian Corpus	cited_context	ICDAR	https://doi.org/10.1109/ICDAR.2019.00033 (2019)	https://doi.org/10.1109/ICDAR.2013.221 (2013)	The ICDAR dataset is used to train and evaluate the DeepTextSpotter model, specifically for robust reading competition tasks. It enables researchers to assess the model's performance in recognizing and processing text in challenging scenarios, contributing to advancements in text detection and recognition technologies.	
Indian Corpus	citing_context	ICE-Corpora	https://doi.org/10.48550/arXiv.2412.04726 (2024)	https://doi.org/10.1111/J.1467-971X.1996.TB00088.X (1996)	The ICE-Corpora dataset is used to validate the representation of language varieties through manual annotation and automatic variety identification. Researchers focus on assessing the accuracy of variety classification, employing methods that combine human annotation with automated techniques. This dataset enables detailed analysis and evaluation of language variety recognition systems, ensuring their reliability and precision.	
Indian Corpus	citing_context	IE-mTOP	https://doi.org/10.48550/arXiv.2304.13005 (2023)	https://doi.org/10.18653/v1/2021.starsem-1.17 (2021)	The IE-mTOP dataset is used to create interbilingual semantic datasets by machine translating English utterances into eleven Indian languages, with a focus on supporting low-resourced languages. This methodology enhances cross-lingual research, enabling the development and evaluation of multilingual natural language processing models. The dataset's comprehensive coverage of Indian languages facilitates the exploration of linguistic diversity and the improvement of translation quality for underrepresented languages.	
Indian Corpus	citing_context	IE-multilingualTOP	https://doi.org/10.48550/arXiv.2304.13005 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.410 (2020)	The IE-multilingualTOP dataset is used to enhance multilingual semantic parsing and cross-lingual natural language understanding, particularly in task-oriented dialog systems. It supports the creation and comparison of multilingual datasets, focusing on slot alignment and recognition. The dataset covers a wide range of languages and tasks, enabling researchers to improve the performance of multilingual dialog systems through annotated conversations and logical form generation.	
Indian Corpus	cited_context	IIITH corpus	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.21437/Interspeech.2014-483 (2014)	The IIITH corpus is extensively used in research focused on Indian languages, particularly for speech and language technology applications. It supports the development and evaluation of LSTM-based language identification (LID) classifiers, speech recognition systems, and linguistic analysis. The dataset is utilized to study linguistic patterns, phonetic features, and gender dependency in LID performance. It covers a wide range of Indian languages, enabling comprehensive research on multilingual and low-resource scenarios.	
Indian Corpus	cited_context	IIIT-H Indic Speech Database	https://doi.org/10.1109/ACCESS.2020.3028241 (2020)	https://doi.org/10.1007/978-981-13-8687-9_12 (2019)	The IIIT-H Indic Speech Database is used to model 7 Indian languages, employing line spectral frequency-based features. It focuses on language identification and acoustic modeling, enabling researchers to develop and test algorithms for distinguishing between these languages. The dataset's comprehensive coverage of multiple Indian languages and its feature set support robust linguistic analysis and machine learning applications in speech processing.	
Indian Corpus	citing_context	IIIT-H Telugu Corpus	https://doi.org/10.48550/arXiv.2208.11761 (2022)	https://www.semanticscholar.org/paper/c673912882ebbb22d3a60a5523d3eceb060af5a8 (2021)	The IIIT-H Telugu Corpus is primarily used for collecting Automatic Speech Recognition (ASR) data, focusing on large-scale speech data collection through a crowd-sourced approach. It has been utilized to develop and enhance ASR systems for multiple Indian languages, including Telugu, Marathi, and Malayalam. This dataset enables researchers to address the challenge of limited speech data by providing a robust resource, thereby improving the accuracy and performance of speech recognition systems in these languages.	
Indian Corpus	cited_context	IIIT-Hyderabad word similarity dataset	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020), https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	https://doi.org/10.18653/v1/W17-0811 (2017)	The IIIT-Hyderabad word similarity dataset is used to evaluate word embeddings across 7 Indian languages, primarily focusing on linguistic similarity tasks. It assesses the quality of embeddings through word-pair similarities and word analogies, enabling researchers to evaluate syntactic and semantic relationships. This dataset facilitates intrinsic evaluation of embedding models, providing a benchmark for linguistic tasks in Indian languages.	
Indian Corpus	cited_context	IIIT-ILST Devanagari Dataset	https://doi.org/10.1109/ICDAR.2019.00033 (2019)	https://doi.org/10.1109/ICDAR.2017.364 (2017)	The IIIT-ILST Devanagari Dataset is used to benchmark scene text recognition models specifically for the Devanagari script. Researchers focus on evaluating the accuracy and robustness of these models, employing the dataset to test and improve recognition performance in real-world scenarios. This dataset enables detailed analysis and comparison of different recognition algorithms, contributing to advancements in Devanagari text recognition technology.	
Indian Corpus	citing_context	IIIT-INDIC-HW	https://doi.org/10.1109/SPIN57001.2023.10117106 (2023)	https://doi.org/10.1007/978-3-030-86337-1_30 (2021)	The IIIT-INDIC-HW dataset is used for Indic handwritten text recognition, focusing on 10 Indian scripts such as Hindi, Telugu, and Bengali. It supports research in multilingual handwriting recognition by providing a large, annotated dataset. Researchers use it to train CNN and transformer-based models, enabling advancements in recognizing diverse regional languages.	
Indian Corpus	citing_context	IIIT_Ranchi	https://www.semanticscholar.org/paper/693e0ff7b621e4d6d7a5e1738be13021214248ca (2022)	https://doi.org/10.1016/j.eswa.2020.113725 (2020)	The IIIT_Ranchi dataset is used for extractive summarization of Indian languages, employing K-means clustering to develop and evaluate automated summarization techniques. Research focuses on clustering methods to enhance summarization accuracy and efficiency, addressing the need for effective summarization tools in Indian languages. The dataset's relevance lies in its application to automated summarization, specifically through clustering algorithms.	
Indian Corpus	cited_context	IIIT-tagset	https://doi.org/10.18653/v1/P16-3006 (2016)	https://www.semanticscholar.org/paper/c08ba14f573e797cb42e9215f21164312acce9bf (2008)	The IIIT-tagset, a corpus of 70k words annotated for part-of-speech tagging, is used to train and evaluate models for Indian language processing. It incorporates chunk information, enhancing linguistic analysis. This dataset supports research in developing more accurate and contextually rich models for Indian languages, focusing on improving part-of-speech tagging and chunking methodologies.	
Indian Corpus	citing_context	IISC-MILE Corpus	https://doi.org/10.48550/arXiv.2208.11761 (2022)	https://doi.org/10.21437/Interspeech.2012-659 (2012)	The IISC-MILE Corpus is extensively used for speech recognition tasks, particularly for Indian languages and dialects. It is employed to benchmark and compare the performance of automatic speech recognition systems across languages like Hindi, Tamil, and Kashmiri. The dataset supports the development and evaluation of language-specific models, focusing on multilingual and low-resource scenarios. It leverages a diverse, crowdsourced dataset to enhance model robustness and accuracy, addressing challenges such as regional variations and multispeaker data.	
Indian Corpus	citing_context	IIT-B	https://doi.org/10.1109/ICCCNT51525.2021.9580073 (2021)	https://www.semanticscholar.org/paper/eb4b3cceae2198b17ff7fdcf5b1108496690eeaf (2020)	The IIT-B dataset is used to detect fake news in Bangla by analyzing linguistic features and content authenticity. Researchers employ this large-scale news dataset to examine specific linguistic patterns and content indicators that distinguish genuine from fake news, enabling the development of more accurate detection models.	
Indian Corpus	citing_context	IIT Bombay English-Hindi corpus	https://doi.org/10.1109/PCEMS58491.2023.10136051 (2023)		The IIT Bombay English-Hindi corpus is primarily used for training and evaluating machine translation models, specifically for English-to-Hindi translation tasks. It provides additional data to enhance the performance and accuracy of these models. The dataset's bilingual nature is crucial for developing and testing translation algorithms, enabling researchers to address challenges in cross-lingual communication and improve the quality of automated translations.	
Indian Corpus	cited_context	IIT Bombay English–Hindi Corpus	https://www.semanticscholar.org/paper/80f3356b706701a40fbd99395a016da5ace16cc4 (2020)	https://www.semanticscholar.org/paper/25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3 (2017)	The IIT Bombay English–Hindi Corpus is used to obtain parallel data for English-Hindi translation, focusing on the alignment and quality of translations. Specifically, it is utilized to enhance the accuracy and coherence of translation pairs, particularly pair 4. This dataset enables researchers to evaluate and improve machine translation systems by providing high-quality aligned sentences.	
Indian Corpus	citing_context	IIT-Bombay sentiment analysis dataset	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	https://www.semanticscholar.org/paper/3e699c2aee182bdfbdfc82d29a9a39c7ee69dcb6 (2012)	The IIT-Bombay sentiment analysis dataset is used for evaluating the performance of cross-lingual models in sentiment analysis for Indian languages. Despite its small size of 150 samples, it helps researchers understand the limitations and challenges associated with small datasets in training and evaluation, particularly in the context of Indian languages. This dataset enables the exploration of methodologies to improve model performance with limited data.	
Indian Corpus	cited_context	IITKGP-MLILSC	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1109/ICPR48806.2021.9412413 (2021)	The IITKGP-MLILSC dataset is primarily used for spoken language recognition, providing audio data such as mel-spectrograms for training and evaluating language identification models. It supports the development of multilingual Indian language systems, featuring a diverse set of speakers and recording conditions. The dataset enables researchers to build and test language identification systems, often employing architectures like DenseNet, to enhance accuracy and robustness in multilingual environments.	
Indian Corpus	cited_context	IITKGP-MLILSC speech database	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1109/NCC.2012.6176831 (2012)	The IITKGP-MLILSC speech database is used to enhance language identification (LID) systems, particularly for Indian languages. It addresses the scarcity of large, standardized multilingual speech corpora, focusing on improving accuracy and robustness in multilingual settings. Researchers employ this dataset to develop more efficient LID systems, leveraging its comprehensive coverage of Indian languages to overcome limitations in existing databases.	
Indian Corpus	cited_context	IIT-Patna Sentiment Analysis dataset	https://doi.org/10.18653/v1/2020.nlposs-1.10 (2020)	https://www.semanticscholar.org/paper/8ffefa0acd2d23e9a3b8315aea0aa6d7a3292205 (2016)	The IIT-Patna Sentiment Analysis dataset is used to evaluate the performance of pre-trained ULMFiT models in text classification tasks, particularly for Indian languages such as Bengali. It is employed to test the accuracy of these models in classifying news articles and headlines, focusing on sentiment analysis and headline classification. This dataset enables researchers to assess the effectiveness of ULMFiT models in handling Indian language text data.	
Indian Corpus	citing_context	IITP-Movie	https://doi.org/10.1145/3461764 (2021)	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	The IITP-Movie dataset is used to evaluate word embeddings, specifically for sentiment analysis in both movie and product reviews in Indian languages. Researchers employ this dataset to assess the effectiveness of word embeddings in capturing sentiment, enabling the development and refinement of natural language processing models tailored for Indian languages.	
Indian Corpus	cited_context	IITP Movie Reviews (Hindi)	https://doi.org/10.48550/arXiv.2211.11418 (2022)	https://www.semanticscholar.org/paper/8ffefa0acd2d23e9a3b8315aea0aa6d7a3292205 (2016)	The IITP Movie Reviews (Hindi) dataset is used for sentiment analysis of Hindi movie reviews, specifically categorizing samples into positive, negative, and neutral classes. Researchers employ this dataset to develop and evaluate algorithms for sentiment classification, focusing on the nuances of Hindi language reviews. This enables the assessment of public opinion and emotional responses to movies, enhancing natural language processing techniques for Indian languages.	
Indian Corpus	cited_context | citing_context	ILCI corpus	https://doi.org/10.1145/3637877 (2023), https://www.semanticscholar.org/paper/d938d15e64f8a1d9ca2935faeb36b9ecdcefbfc8 (2020)	https://www.semanticscholar.org/paper/9e9741daf545bebf2e3404aa18764f940f76d421 (2021), https://www.semanticscholar.org/paper/bd3d1ef6eaec411c103d0f7d13b054ba3700ccd5 (2014)	The ILCI dataset is used to develop an HMM-based Odia POS tagger, focusing on 0.2 million tokens annotated with 11 tags. This dataset supports low-resourced language processing by providing a structured resource for training and evaluating POS tagging models, enhancing the accuracy and reliability of linguistic analysis in Odia.; The ILCI corpus is used to expand the monolingual Hindi corpus by adding 44 million sentences, significantly enhancing the size and diversity of the training data. This augmentation supports machine translation research, improving the quality and coverage of translation models. The dataset's large volume and rich content enable more robust training and testing of natural language processing systems.	ILCI
Indian Corpus	cited_context	ILCI Phase 1 corpus	https://doi.org/10.1007/s10590-021-09263-3 (2021)	https://www.semanticscholar.org/paper/887d8d4bae488941d9a3a26ec39122abc84fad79 (2010)	The ILCI Phase 1 corpus is used to conduct experiments involving parallel sentences from health and tourism domains across 11 languages, including English and 10 Indian languages. Researchers employ this dataset to explore cross-lingual similarities and differences, focusing on translation and linguistic analysis. The dataset's multilingual and domain-specific nature enables detailed comparative studies and enhances the development of translation models and language processing tools.	
Indian Corpus	cited_context | citing_context	ILCI Tourism domain	https://doi.org/10.48550/arXiv.2204.13743 (2022)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The ILCI Tourism domain dataset is used to annotate both news articles and tourism-related data, focusing on linguistic structures and patterns. It supports research in journalistic writing and travel and hospitality contexts, enabling detailed analysis of language use in these domains. The dataset facilitates the study of linguistic features and content, enhancing understanding of how language functions in specific professional settings.; The ILCI Tourism domain dataset is used to annotate both news-related and tourism-related data, focusing on linguistic patterns and content specific to these domains. Researchers employ annotation methodologies to analyze and understand the unique linguistic features of news articles and tourism materials. This dataset enables detailed examination of domain-specific language use, supporting research into how language varies across different contexts.	
Indian Corpus	citing_context	ILDC	https://doi.org/10.48550/arXiv.2406.04136 (2024)	https://doi.org/10.18653/v1/P19-1424 (2019)	The ILDC dataset is used to predict and explain court judgments in Indian legal documents, focusing on the linguistic and contextual aspects of these texts. Researchers employ machine learning models to analyze legal documents and their outcomes, enabling a deeper understanding of the factors influencing judicial decisions.	
Indian Corpus	citing_context	ILDC-single	https://doi.org/10.1109/ICCCNT61001.2024.10725043 (2024)	https://doi.org/10.18653/v1/N19-1423 (2019)	The ILDC-single dataset is used to evaluate the performance of various deep learning models, including XLNet + BiGRU and six Transformer models (BERT, XLNet, RoBERTa, DeBERTa, ELECTRA, and BigBird), in the context of Indian language processing and court judgment prediction. It focuses on assessing model accuracy and effectiveness, providing a benchmark for comparing these advanced architectures.	
Indian Corpus	citing_context	ILID train set	https://www.semanticscholar.org/paper/8ca047003dc2a9114b4f6571998d1619d2f4991b (2025)	https://www.semanticscholar.org/paper/96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61 (2021)	The ILID train set is used to fine-tune the MuRIL pre-trained model, specifically enhancing its performance in Indian languages. This dataset enables researchers to improve the model's effectiveness in tasks related to Indian languages by providing specialized training data. The focus is on optimizing the model's capabilities in handling linguistic nuances and complexities unique to these languages.	
Indian Corpus	cited_context	ILSL12	https://doi.org/10.21437/Interspeech.2021-1339 (2021)		The ILSL12 dataset is used to map and standardize Indian speech sounds, focusing on the label set for speech processing and recognition tasks in Indian languages. It ensures consistency in labeling speech sounds, which is crucial for developing accurate speech recognition systems. The dataset's standardized grapheme set enhances the reliability and interoperability of speech processing models.	
Indian Corpus	cited_context | citing_context	INCLUDE	https://doi.org/10.1145/3627631.3627660 (2023), https://doi.org/10.18653/v1/2022.emnlp-main.707 (2022), https://doi.org/10.48550/arXiv.2407.14224 (2024), https://doi.org/10.48550/arXiv.2407.05404 (2024), https://doi.org/10.1109/ICCCNT61001.2024.10724360 (2024), https://doi.org/10.18653/v1/2023.findings-eacl.131 (2022)	https://doi.org/10.1145/3394171.3413528 (2020)	The INCLUDE dataset is primarily used for Indian Sign Language (ISL) recognition and translation, supporting large-scale research with 4,287 videos and 0.27 million frames covering 263 signs. It is employed to train and evaluate models like ST-GCN and SL-GCN for sign recognition, neural machine translation for ISL-to-text conversion, and sequence-to-sequence models for continuous sign language recognition. The dataset facilitates research into real-time recognition, gesture segmentation, and cross-language sign recognition, enhancing accessibility and translation accuracy.; The INCLUDE dataset is used to train and evaluate models for Indian Sign Language recognition, featuring 263 classes of signs. It supports research aimed at enhancing sign language translation and accessibility, focusing on improving the accuracy and effectiveness of sign language recognition systems.	
Indian Corpus	citing_context	INCLUDE50	https://doi.org/10.1109/PCEMS58491.2023.10136062 (2023), https://doi.org/10.1109/PhDEDITS60087.2023.10373750 (2023)	https://doi.org/10.1145/3394171.3413528 (2020)	The INCLUDE50 dataset is used to train and evaluate deep learning models for Indian Sign Language Recognition. It focuses on large-scale data to improve the accuracy of gesture recognition, enabling researchers to examine the performance of these models in recognizing Indian Sign Language gestures. The dataset's scale and focus on Indian Sign Language make it particularly useful for enhancing recognition systems.	
Indian Corpus	citing_context	Indian-BhED	https://doi.org/10.1145/3677525.3678666 (2023)	https://doi.org/10.1145/3442188.3445896 (2021)	The Indian-BhED dataset is used to evaluate algorithmic fairness and study caste-based discrimination and religious biases in English-language content, particularly in the Indian context. It contains 229 examples of stereotypical and anti-stereotypical scenarios, enabling researchers to assess and analyze biases in algorithms and content.	
Indian Corpus	cited_context	Indian dependency tree bank	https://www.semanticscholar.org/paper/a0e962d1d912e0e0809e2a85c605ef9255e878cf (2008)	https://www.semanticscholar.org/paper/bcf9eac7b43b9206d73e97c342725f6689836ef1 (2008)	The Indian dependency tree bank is used to enhance linguistic annotations by integrating named entity recognition, specifically to improve dependency structures in Indian languages. This dataset supports research focused on refining syntactic and semantic relationships within Indian language texts, enabling more accurate and nuanced natural language processing tasks.	
Indian Corpus	cited_context	Indian language pairs provided by Kunchukuttan et al. (2014)	https://doi.org/10.3115/v1/N15-3017 (2015)	https://www.semanticscholar.org/paper/6614d9852fec1a69b13889f28845d443f198347d (2014)	The dataset 'Indian language pairs provided by Kunchukuttan et al. (2014)' is used to evaluate transliteration systems within machine translation, focusing on post-editing phrase-based SMT outputs for Indian language pairs. This involves assessing the accuracy and effectiveness of transliteration in improving translation quality, particularly in handling linguistic nuances between Indian languages. The dataset's relevance lies in its provision of paired language data, enabling researchers to test and refine transliteration algorithms.	
Indian Corpus	citing_context	Indian Languages Corpora Initiative (ILCI)	https://doi.org/10.1109/AICCSA56895.2022.10017934 (2022)		The Indian Languages Corpora Initiative (ILCI) dataset is used for manual tagging and preprocessing of linguistic data, specifically for Indian languages. Language experts annotate the dataset, which is then used to train and evaluate part-of-speech tagging models. The dataset's annotated nature supports the creation of comprehensive, labeled data essential for training and testing natural language processing systems. Research focuses on improving the accuracy and robustness of these models through multiple experimental rounds.	
Indian Corpus	cited_context | citing_context	Indian languages test-set	https://doi.org/10.1109/ICIIP47207.2019.8985815 (2019)	https://doi.org/10.1109/ICIIP.2017.8313738 (2017)	The Indian languages test-set is used to evaluate the performance of various optical character recognition (OCR) systems, including CRNN, Tesseract OCR, and MDLSTM, for recognizing printed text in seven Indian languages. The dataset focuses on assessing the accuracy and effectiveness of these systems, enabling researchers to compare their performance and identify strengths and weaknesses in recognizing specific linguistic features.; The Indian languages test-set is used to evaluate the performance of various optical character recognition (OCR) systems, including CRNN, Tesseract OCR, and MDLSTM, for recognizing printed text in seven Indian languages. Research focuses on assessing the accuracy and effectiveness of these systems, providing insights into their strengths and limitations in multilingual contexts.	
Indian Corpus	cited_context	Indian legal corpus	https://doi.org/10.1145/3594536.3595165 (2022)		The Indian legal corpus is used to re-train BERT-based legal pre-trained language models (PLMs), specifically to adapt these models to the Indian legal domain. This involves continuing the pre-training of these models using the corpus, enhancing their performance and relevance for tasks within the Indian legal context. The dataset's focus on Indian legal texts enables researchers to improve model accuracy and applicability in legal analysis and natural language processing tasks specific to India.	
Indian Corpus	citing_context	Indian Lexicon Sign Language Dataset	https://doi.org/10.1109/IICAIET62352.2024.10730290 (2024)	https://doi.org/10.1145/3394171.3413528 (2020)	The Indian Lexicon Sign Language Dataset is used to train and evaluate sign language recognition models, focusing on 351 sign language classes. It employs 128-frame video clips sourced from YouTube and other platforms. This dataset enables researchers to develop and test algorithms that can accurately recognize and classify sign language gestures, enhancing the performance and reliability of sign language recognition systems.	
Indian Corpus	cited_context	Indian sentiment lexicons	https://doi.org/10.1007/978-3-319-26832-3_65 (2015)	https://www.semanticscholar.org/paper/bba6fd95578997b7d090277b21f1e4e290c8c7e2 (2010)	The 'Indian sentiment lexicons' dataset is used to perform sentiment analysis in Indian languages. It provides a list of words categorized as positive, negative, neutral, and ambiguous, along with their corresponding part of speech (PoS) tags. This dataset enables researchers to analyze textual data in Indian languages, facilitating the identification and classification of sentiments in various texts. The inclusion of PoS tags enhances the accuracy of sentiment analysis by considering the grammatical context of words.	
Indian Corpus	cited_context | citing_context	Indian SentiWordNet	https://www.semanticscholar.org/paper/c548717c5e0206c65efe4d3814ff545bd59ea936 (2016)	https://www.semanticscholar.org/paper/bba6fd95578997b7d090277b21f1e4e290c8c7e2 (2010)	The Indian SentiWordNet dataset is used to enhance sentiment analysis in Indian languages by providing sentiment scores for words and constructing vocabularies of unique words. It focuses on lexical acquisition and sentiment scoring, particularly for positive word encoding, and is utilized to evaluate and improve the performance of sentiment analysis models in constrained tasks.; The Indian SentiWordNet dataset is used to provide sentiment scores for words in Indian languages, enhancing the performance of sentiment analysis models. It is employed to construct a vocabulary of unique words and their sentiment scores, focusing on positive word encoding and lexical acquisition. This dataset specifically supports the evaluation and improvement of sentiment analysis systems in Indian languages, particularly in constrained submission tasks.	
Indian Corpus	citing_context	Indian Sign Language dataset	https://doi.org/10.1145/3610661.3616550 (2023)	https://doi.org/10.1109/CVPR.2014.471 (2014)	The Indian Sign Language dataset is used to evaluate models in sign language generation, specifically focusing on the importance of prosody and facial action unit prediction. This dataset enables researchers to assess how well models can generate natural and expressive sign language, incorporating nuanced facial expressions and prosodic features.	
Indian Corpus	citing_context	Indian Sign Language (ISL) fingerspelling dataset	https://doi.org/10.1109/NEleX59773.2023.10421113 (2023)	https://doi.org/10.1109/ACCESS51619.2021.9563346 (2021)	The Indian Sign Language (ISL) fingerspelling dataset is used to train and test CNN-based models for recognizing hand gestures, specifically focusing on fingerspelling accuracy in ISL. This dataset enables researchers to develop and evaluate machine learning models that can accurately interpret ISL fingerspelling, enhancing communication technologies for the deaf community.	
Indian Corpus	citing_context	Indian Sign Language gesture dataset	https://doi.org/10.14569/ijacsa.2021.0120881 (2021)	https://doi.org/10.1109/ICCCT.2010.5640434 (2010)	The Indian Sign Language gesture dataset is used to train and evaluate dynamic hand gesture recognition models, focusing on both single and two-hand manual signs. It is applied in recognizing and interpreting Indian Sign Language gestures for human-robot interaction, particularly continuous single-handed signs. This dataset enables researchers to develop and test algorithms that improve the accuracy and reliability of gesture recognition systems, enhancing communication interfaces.	
Indian Corpus	citing_context	Indian Sign Language Recognition Database	https://doi.org/10.1109/PCEMS58491.2023.10136062 (2023)	https://doi.org/10.1109/CSCITA.2014.6839287 (2014)	The Indian Sign Language Recognition Database is used to develop algorithms for hand tracking and segmentation, focusing on creating a robust dataset for sign language recognition. This dataset enables researchers to enhance the accuracy and reliability of sign language recognition systems, specifically tailored for Indian Sign Language. The dataset's comprehensive annotations and diverse hand gestures support the development and testing of these algorithms.	
Indian Corpus	cited_context	Indian Statistical Institute, Kolkata database	https://doi.org/10.1109/TSMCC.2010.2095841 (2011)	https://doi.org/10.1109/TPAMI.2008.88 (2009)	The Indian Statistical Institute, Kolkata database is primarily used for Devanagari script recognition research, focusing on character-level accuracy and numeral recognition. It contains 22,556 isolated handwritten Devanagari numeral samples, which are utilized to create and evaluate recognition systems. Methodologies include wavelet filter-based multiresolution analysis for improving numeral image classification. The dataset also provides frequency statistics for 20 common Devanagari characters, aiding in the analysis of script occurrence and enhancing recognition performance in mixed numeral contexts.	
Indian Corpus	cited_context	I NDIC A LIGN	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	https://doi.org/10.3758/BRM.42.2.381 (2010)	The I NDIC A LIGN dataset is used to assess the lexical diversity of prompts by applying the Measure of Textual Lexical Diversity (MTLD) score. This methodology evaluates the variety of vocabulary within the dataset, enabling researchers to analyze and quantify the richness and breadth of linguistic content. The dataset's focus on Indian languages provides a unique resource for studying lexical variation in these contexts.	
Indian Corpus	citing_context	IndicBERT	https://doi.org/10.48550/arXiv.2403.20147 (2024)	https://www.semanticscholar.org/paper/96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61 (2021)	IndicBERT is used to train a multilingual model specifically for Indian languages, leveraging news articles and Indian websites. With 12 million parameters, the dataset is designed to enhance performance on Indian language tasks, focusing on improving model accuracy and efficiency in processing and understanding these languages.	
Indian Corpus	cited_context | citing_context	IndicCorp	https://doi.org/10.1109/ACCESS.2024.3396290 (2024), https://doi.org/10.48550/arXiv.2204.08776 (2022), https://doi.org/10.48550/arXiv.2208.11761 (2022), https://www.semanticscholar.org/paper/a05ff6a06948992ecfa93f4c7576583b5272e4c2 (2021), https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	https://doi.org/10.1109/ACCESS.2021.3104106 (2021), https://www.semanticscholar.org/paper/8eb74f94e98006dfce35b8a5de5b6b3cd629efc2 (2020)	The IndicCorp dataset is extensively used in research focused on Indian languages, primarily for language identification, multilingual representation learning, and natural language understanding. It serves as a large monolingual corpus, sourced from diverse web content, enabling the pre-training of models like IndicBERT and the creation of language models such as 6-gram KenLM. Additionally, it facilitates the collection of sentences for speech data and query-based analysis, enhancing performance on downstream tasks and benchmarking models across various linguistic tasks.; The IndicCorp dataset is used to train various models for Indian languages, including IndicBART, IndicFT, and IndicBERT, focusing on developing multilingual language models and word embeddings. It is utilized for training on Devanagari script and large, high-quality news crawls, enabling research in word similarity, word analogy, text classification, and bilingual lexicon induction. The dataset's rich content supports the creation of 300-dimensional word embeddings enriched with subword information, enhancing the quality and applicability of these models in Indian language processing tasks.	
Indian Corpus	cited_context	INDIC CORP V1	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	The INDIC CORP V1 dataset is used to build and enhance monolingual corpora for Indic languages, focusing on creating benchmark datasets and models to support under-resourced languages. It enables researchers to expand linguistic resources, improving the availability and quality of data for these languages. This supports the development of more robust and inclusive natural language processing models.	
Indian Corpus	cited_context	IndicCorp v2	https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc (2019)	IndicCorp v2 is used to train and evaluate cross-lingual models, particularly XLM-R and IndicBERT v2, focusing on enhancing multilingual representation learning at scale. It supports 100 languages, including 15 Indic languages, and is crucial for improving language understanding and modeling capabilities in these languages. The dataset is derived from large-scale multilingual web-crawled corpora and is used to address the lack of pretraining data for underrepresented Indian languages like Bodo, Dogri, and Kashmiri.	
Indian Corpus	citing_context	Indic fastText embeddings	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	https://www.semanticscholar.org/paper/95b3d6b9d50bfd4b0a77a096269762b28e218346 (2017)	The Indic fastText embeddings dataset is used to enhance NLP tasks for Indian languages by experimenting with both trainable and non-trainable embeddings. Researchers use these pre-trained word and subword embeddings to train and evaluate models, aiming to improve representation quality and performance in various Indian language applications. This dataset facilitates the development of more effective NLP models for Indian languages through its robust embedding features.	
Indian Corpus	cited_context	IndicGLUE Datasets	https://doi.org/10.18653/v1/2023.findings-emnlp.252 (2023)	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	The IndicGLUE Datasets are used to evaluate and compare the quality of word embeddings and language models for Indian languages. Researchers employ intrinsic and extrinsic methods to assess embedding performance, including text classification tasks and semantic similarity evaluations. The dataset supports the development and benchmarking of models like IndicBERT and IndiSocialFT, focusing on robust representations in multilingual and native script settings. It provides a standardized resource for enhancing and comparing the accuracy and effectiveness of language models across various Indian languages.	
Indian Corpus	cited_context	IndicGULE	https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	https://doi.org/10.18653/v1/W18-5446 (2018)	The IndicGULE dataset is used to benchmark the performance of multitask models on Indic languages, focusing on their effectiveness in natural language understanding tasks. This dataset enables researchers to evaluate and compare model performance across various Indic languages, providing insights into the challenges and capabilities of these models in multilingual settings.	
Indian Corpus	citing_context	IndicHG	https://www.semanticscholar.org/paper/3e6c42b277b41daa403ed60365d60042d29e5c14 (2023)	https://doi.org/10.18653/v1/2022.emnlp-main.360 (2022)	The IndicHG dataset is used to investigate and address quality issues in multilingual datasets for natural language generation (NLG) tasks in Indic languages. It provides guidelines and benchmarks for diverse NLG tasks, trains and fine-tunes models to improve performance, and validates results through quantitative and qualitative analyses, focusing on reliability, consistency, and robustness in Indic language generation.	
Indian Corpus	citing_context	INDIC-MARCO	https://doi.org/10.48550/arXiv.2506.01615 (2025)	https://doi.org/10.48550/arXiv.2312.09508 (2023)	The INDIC-MARCO dataset is used to translate the MS MARCO dataset into 11 Indian languages, facilitating research in multilingual information retrieval and neural information models. This translation supports the development and evaluation of algorithms that can handle diverse linguistic contexts, enhancing cross-lingual search and information access capabilities.	
Indian Corpus	citing_context	IndicMSMARCO	https://doi.org/10.48550/arXiv.2506.01615 (2025)	https://www.semanticscholar.org/paper/dd95f96e3322dcaee9b1e3f7871ecc3ebcd51bfe (2016)	The IndicMSMARCO dataset is used to address multilingual retrieval challenges in Indian languages by adapting the MS MARCO framework to regional contexts. It highlights the disparity in query volume between English and Indian languages, emphasizing the need for more balanced datasets. This dataset enables researchers to develop and evaluate retrieval systems that better serve Indian language users, focusing on improving multilingual information access.	
Indian Corpus	citing_context	IndicNLG Suite	https://doi.org/10.48550/arXiv.2304.13005 (2023)	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	The IndicNLG Suite is used as a monolingual and multilingual corpus to develop and evaluate natural language understanding (NLU) and natural language generation (NLG) models for Indian languages. It provides a rich resource for training and testing, supporting diverse NLG tasks and enhancing the performance of systems in Indic languages.	
Indian Corpus	citing_context	Indic-nlp	https://www.semanticscholar.org/paper/1b3a5590b0bd5ed81517c3951002f4611c84e8f5 (2020)	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	The Indic-nlp dataset is used to curate monolingual corpora and train word embeddings for Indic languages, particularly Bengali. It supports language processing tasks such as classification and benchmarking, enabling researchers to evaluate the performance of models on monolingual data and word embeddings. This dataset facilitates the development and assessment of natural language processing techniques for Indic languages.	
Indian Corpus	citing_context	IndicNLP News Article dataset	https://doi.org/10.48550/arXiv.2401.02254 (2024)	https://doi.org/10.1007/978-981-16-6407-6_53 (2021)	The IndicNLP News Article dataset is used to evaluate deep learning models for Marathi text classification. Researchers focus on assessing the limitations of target labels within the dataset and achieving high accuracy. This dataset enables the examination of model performance and label quality, contributing to the development of more robust text classification systems for Indian languages.	
Indian Corpus	cited_context	IndicNLP news article classification datasets	https://doi.org/10.48550/arXiv.2304.11434 (2023)	https://doi.org/10.18653/v1/D19-1077 (2019)	The IndicNLP news article classification datasets are used to evaluate the performance of news article classification models across Indian languages. Researchers employ these datasets to assess model accuracy and robustness, focusing on how well they can categorize news articles in various Indian languages. This enables the development and refinement of multilingual natural language processing techniques, specifically tailored for the linguistic diversity of India.	
Indian Corpus	cited_context | citing_context	Indic QA	https://doi.org/10.48550/arXiv.2407.13522 (2024)	https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	The Indic QA dataset is used to evaluate question-answering models in 10 Indian languages, primarily for cloze-style reading comprehension tasks. It is manually curated and has been extended to include Gujarati translations, enabling researchers to assess model performance across diverse linguistic contexts. This dataset facilitates the development and testing of multilingual natural language processing systems, focusing on the nuances and challenges specific to Indic languages.; The Indic QA dataset is used to evaluate question-answering models across 10 Indian languages, focusing on the translation and validation of questions from Hindi to Gujarati. This dataset enables researchers to assess model performance in multilingual settings, ensuring accurate and contextually appropriate translations and answers.	
Indian Corpus	cited_context	IndicTrans	https://doi.org/10.18653/v1/2022.emnlp-main.360 (2022)	https://doi.org/10.48550/arXiv.2204.08776 (2022)	The IndicTrans dataset is used to evaluate multilingual inference and translation quality for Indian languages. It focuses on assessing the performance of models in cross-lingual natural language inference tasks and the accuracy and fluency of translations. This dataset enables researchers to create test sets and benchmark the effectiveness of translation models in Indian language contexts.	
Indian Corpus	cited_context	IndicTrans2	https://doi.org/10.48550/arXiv.2404.16816 (2024)	https://doi.org/10.48550/arXiv.2305.16307 (2023)	The IndicTrans2 dataset is primarily used for natural language processing tasks in Indic languages, specifically for abstractive summarization and headline generation. It supports the creation of concise and coherent summaries and headlines in multiple Indic languages, enhancing automatic news summarization and translation capabilities. The dataset facilitates research by providing a multilingual resource for developing and evaluating NLP models in 14 to 22 Indian languages.	
Indian Corpus	cited_context	IndicTTS	https://doi.org/10.48550/arXiv.2305.12518 (2023)	https://www.semanticscholar.org/paper/1d480a2ae449ee625aa543610ed2574e3c87643e (2016)	The IndicTTS dataset is used to train Text-to-Speech (TTS) models, specifically focusing on noisefree read speech in Indian languages. This enhances the quality and clarity of synthesized speech, addressing the need for high-fidelity TTS systems in these languages. The dataset's clean audio characteristics are crucial for improving the naturalness and intelligibility of the generated speech.	
Indian Corpus	cited_context | citing_context	Indic TTS Database	https://doi.org/10.48550/arXiv.2407.14056 (2024), https://doi.org/10.1109/ACCESS.2020.3028241 (2020), https://doi.org/10.21437/Interspeech.2020-2663 (2020)	https://doi.org/10.18653/v1/2024.acl-long.843 (2024), https://www.semanticscholar.org/paper/1d480a2ae449ee625aa543610ed2574e3c87643e (2016)	The IndicTTS database is used to select neutral-like utterances for Indian language processing, focusing on a diverse range of domains. This dataset enables researchers to enhance the neutrality and diversity of speech data, which is crucial for developing robust text-to-speech systems and improving natural language processing tasks in Indian languages.; The Indic TTS Database is used to develop and evaluate text-to-speech systems for Indian languages, focusing on enhancing speech synthesis quality, naturalness, and phonetic accuracy. Researchers employ this dataset to assess the performance of TTS models, ensuring synthesized speech sounds natural and linguistically correct. The dataset's comprehensive coverage of Indian languages supports these evaluations, enabling advancements in TTS technology for diverse linguistic contexts.	IndicTTS database
Indian Corpus	citing_context	I NDIC V OICES	https://doi.org/10.48550/arXiv.2403.01926 (2024)	https://doi.org/10.1109/ICASSP49357.2023.10094667 (2022)	The I NDIC V OICES dataset is used to evaluate and train speech recognition models, specifically for Indian languages. It supports the comparison of model performance against state-of-the-art systems and facilitates the development of massively multilingual ASR models using a hybrid CTC-RNNT objective. This dataset enables researchers to enhance speech recognition capabilities across multiple Indian languages.	
Indian Corpus	cited_context	Indo4B-Plus	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021)	https://doi.org/10.1162/tacl_a_00343 (2020)	The Indo4B-Plus dataset is used to balance language representation in multilingual neural machine translation models. Researchers adjust the sampling ratios for different languages within the dataset to optimize model performance, ensuring more equitable and effective translation across languages. This approach addresses the challenge of underrepresented languages in multilingual models, enhancing their overall accuracy and reliability.	
Indian Corpus	cited_context	IndoWordNet	https://doi.org/10.29007/MXDH (2018), https://doi.org/10.1145/3383330 (2020)	https://doi.org/10.1007/978-981-10-1909-8_1 (2010)	IndoWordNet is used as a comprehensive resource for multilingual lexical assets, particularly for Indian languages, to develop and enhance lexical resources like Telugu WordNet. It is employed in natural language processing applications, focusing on lexical relationships and semantic structures in Indian languages. This dataset enables researchers to improve the quality and coverage of lexical resources, supporting more accurate and contextually relevant NLP tasks.	
Indian Corpus	citing_context	IndoWordNet 3	https://www.semanticscholar.org/paper/579dd33ab4f615b58d0144a8d161c815a6287823 (2016)	https://doi.org/10.1007/978-981-10-1909-8_1 (2010)	IndoWordNet 3 is used to provide lexical resources for 17 official Indian languages, facilitating semantic and syntactic analyses in computational linguistics. This dataset supports research in understanding language structure and meaning, enabling more accurate and nuanced computational models for these languages. Its comprehensive lexical data enhances the precision of linguistic analyses and applications.	
Indian Corpus	cited_context | citing_context	INFO TABS	https://doi.org/10.18653/v1/2022.naacl-main.295 (2022)	https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de (2019), https://www.semanticscholar.org/paper/ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96 (2019)	The 'INFO TABS' dataset is used to evaluate and compare the performance of MuRIL and RoBERTa LARGE models on various reasoning tasks, including entity type, named entity, negation, numerical, quantification, and simple lookup. It enables researchers to assess model effectiveness in these specific reasoning types, providing insights into their capabilities and limitations.; The INFO TABS dataset is used for natural language inference over tabular data, enabling detailed task analysis. It facilitates the comparison of model performances, specifically MuRIL and RoBERTa LARGE, across various reasoning types such as entity type, named entity, negation, numerical, quantification, and simple lookup. This dataset supports research in understanding and enhancing the capabilities of models in handling complex reasoning tasks over structured data.	
Indian Corpus	cited_context	iNLTK Headline Dataset	https://doi.org/10.1007/978-981-16-6407-6_53 (2021)	https://doi.org/10.18653/v1/2020.nlposs-1.10 (2020)	The iNLTK Headline Dataset is used for classifying Marathi news article headlines into categories such as entertainment, sports, and state. It focuses on natural language processing (NLP) for Indic languages, employing classification methodologies to enhance the understanding and processing of Marathi text. This dataset enables researchers to develop and evaluate NLP models tailored for Indian languages, specifically addressing the categorization of news headlines.	
Indian Corpus	cited_context	INRIA sanskrit reader companion	https://doi.org/10.18653/v1/d18-1530 (2018)	https://www.semanticscholar.org/paper/75aa48c875ceebcb2bb10726326ef99a3fde09d1 (2003)	The INRIA sanskrit reader companion dataset is used to support computational processing of Sanskrit by providing linguistic resources and annotations. It enables researchers to conduct language analysis and understanding, facilitating the development of tools and methodologies for processing Sanskrit text. This dataset enhances the accuracy and depth of linguistic analysis in computational linguistics research.	
Indian Corpus	citing_context	Ishara-Lipi	https://doi.org/10.1109/PCEMS58491.2023.10136062 (2023)	https://www.semanticscholar.org/paper/edf2e1b1317cfd41c2d52e2cdd9fb71b11be35a7 (2015)	The Ishara-Lipi dataset is used to develop and evaluate sign language recognition systems specifically for Bangla. It provides isolated character data, which is essential for training and testing machine learning models. This dataset enables researchers to focus on improving the accuracy and reliability of sign language recognition technologies for the Bangla language.	
Indian Corpus	cited_context	ISL alphabets	https://doi.org/10.1109/RAECS.2014.6799633 (2014)	https://doi.org/10.5120/12174-7306 (2013)	The ISL alphabets dataset is used for recognizing Indian Sign Language alphabets in live video. Researchers employ image-based recognition techniques, specifically focusing on histogram matching and eigenvalue Euclidean distance to enhance accuracy. This dataset enables the testing and development of algorithms aimed at improving real-time sign language recognition.	
Indian Corpus	cited_context | citing_context	ISL-CSLRT	https://doi.org/10.18653/v1/2022.emnlp-main.707 (2022), https://doi.org/10.1109/ICCCNT61001.2024.10724360 (2024)	https://doi.org/10.17632/KCMPDXKY7P.1 (2021)	The ISL-CSLRT dataset is used for continuous Indian Sign Language translation and recognition, focusing on sentence-level data and a subset of 50 signs. It supports the development, training, and evaluation of sign language processing systems, enhancing communication technologies. The dataset includes 700 sign videos captured with a Canon Digital SLR Camera, and has been augmented 4 times, using a fixed head count of 32 and batch size of 128. It facilitates benchmarking and comparison of recognition systems, improving the diversity and coverage of Indian Sign Language data.; The ISL-CSLRT dataset is used for continuous Indian sign language translation and recognition, focusing on the development and improvement of sign language processing systems. It captures 100 sentences in 700 sign videos, providing sentence-level data essential for enhancing Indian Sign Language understanding and translation accuracy.	
Indian Corpus	cited_context	ISL database	https://doi.org/10.5121/IJCSA.2014.4207 (2014)	https://doi.org/10.1109/EURCON.2003.1248169 (2003)	The ISL database is used for classifying DCT-based feature vectors of gestures to determine their presence or absence in the Indian Sign Language dataset. The methodology involves unsupervised learning with Self-Organizing Maps, enabling researchers to analyze and recognize specific gestures within the dataset. This approach addresses the research question of gesture recognition in sign language, leveraging the dataset's feature vectors for accurate classification.	
Indian Corpus	citing_context	ISL Datasets	https://doi.org/10.1088/1742-6596/1950/1/012020 (2021)	https://doi.org/10.1016/j.jvcir.2020.102834 (2020)	The ISL Datasets are used to enhance Indian sign language recognition by focusing on Krawtchouk moment-based local features and HOG parameter selection, particularly in complex background environments. This dataset enables researchers to improve the accuracy and robustness of sign language recognition systems through advanced feature extraction and parameter optimization techniques.	
Indian Corpus	citing_context	ISL Kothadiya	https://doi.org/10.1109/INDICON59947.2023.10440818 (2023)		The ISL Kothadiya dataset is used to develop and evaluate sign language recognition systems for Indian Sign Language, specifically focusing on alphabets and numbers. It contains 36 signs, which provide a structured set of visual gestures for training and testing machine learning models. This dataset enables researchers to improve the accuracy and robustness of sign language recognition systems, facilitating better communication technologies for the deaf community.	
Indian Corpus	cited_context	ISL static alphabets	https://doi.org/10.1109/iccsp.2019.8698006 (2019)	https://www.semanticscholar.org/paper/48d9c25a01bcffa54507fa99fb9159972cd7032c (2014)	The ISL static alphabets dataset is used to train and test hand gesture recognition systems for Indian Sign Language, focusing on static alphabets. This dataset enables researchers to achieve high accuracy in recognizing these gestures, employing methodologies that involve machine learning models. The dataset's specific focus on static alphabets makes it particularly useful for developing robust sign language recognition systems.	
Indian Corpus	cited_context | citing_context	ISL video datasets	https://doi.org/10.1145/3394171.3413528 (2020)	https://www.semanticscholar.org/paper/c7efc0ef3b7a66195cd04efb62898aca47f63206 (2015)	The ISL video datasets are used to develop both continuous and image-based sign language recognition systems, focusing on video data for sentence formation and single images for individual signs. These systems aim to improve sign language interpretation and understanding, employing methodologies that analyze visual data to enhance communication technologies for the deaf and hard-of-hearing communities.; The ISL video datasets are used to develop and test image-based sign recognition methods, focusing on identifying individual signs from static images. They are also employed to train and evaluate continuous Indian Sign Language gesture recognition systems, with a focus on improving sentence formation and gesture recognition accuracy. These datasets enable researchers to enhance the performance and reliability of sign language recognition technologies.	
Indian Corpus	cited_context	JW300	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The JW300 dataset is primarily used to train and evaluate machine translation models across various domains, including religious texts, TED talks, legal and administrative documents, news and blog content, Wikipedia articles, and movie and TV subtitles. It focuses on sentence pairs and their translations, enabling researchers to assess model performance in diverse contexts.	
Indian Corpus	citing_context	Kaggle dataset	https://doi.org/10.22266/ijies2023.0831.32 (2023)		The Kaggle dataset is used for evaluating and comparing different approaches in Indian Sign Language (ISL) recognition, specifically focusing on 36 classes of alphanumeric signs. It supports the assessment of various recognition models, including pre-trained CNNs and the proposed ISLTS method, enabling researchers to measure and enhance the accuracy of ISL gesture translation.	
Indian Corpus	citing_context	Kannada dataset for digits only	https://doi.org/10.1109/GCAT55367.2022.9971971 (2022)	https://www.semanticscholar.org/paper/357da8861465de56c05259aeb7bf2116f7e73d21 (2013)	The Kannada dataset for digits is used to develop and evaluate one-shot learning models, specifically addressing the challenge of recognizing Kannada digits with limited data. This dataset enables researchers to focus on under-resourced scripts, enhancing model performance and robustness in scenarios with scarce training examples.	
Indian Corpus	citing_context	Khasi POS Corpus	https://doi.org/10.1145/3637877 (2023)	https://doi.org/10.1145/3488381 (2021)	The Khasi POS Corpus is used to train and evaluate a Khasi POS tagger using deep learning approaches, specifically focusing on enhancing part-of-speech tagging accuracy in the Khasi language. This dataset enables researchers to develop and refine models that improve the linguistic analysis of Khasi, a less-resourced language.	
Indian Corpus	cited_context	Khasi PoS tagset	https://doi.org/10.1145/3488381 (2021)	https://doi.org/10.1109/NCETACS.2012.6203274 (2012)	The Khasi PoS tagset dataset is used to develop a parts-of-speech tagset for the Khasi language, focusing on the design of the tagset and the establishment of linguistic annotation standards. This dataset enables researchers to create consistent and standardized annotations, facilitating more accurate and reliable linguistic analysis of the Khasi language.	
Indian Corpus	citing_context	Kinect dataset	https://doi.org/10.1109/ICCCNT56998.2023.10308342 (2023)	https://doi.org/10.1109/DICTA47822.2019.8945850 (2019)	The Kinect dataset is used to enhance the recognition of Indian Sign Language gestures by leveraging depth images. Researchers employ depth information to gain insights into gesture dynamics, improving the accuracy of sign language recognition systems. This dataset's depth imaging capabilities are crucial for capturing the nuanced movements of sign language, enabling more robust and reliable gesture recognition in real-world applications.	
Indian Corpus	cited_context | citing_context	Konkani Literature-based TS dataset	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://www.semanticscholar.org/paper/24cb6122acd983ccf5b175a401c300add943ca38 (2019)	The Konkani Literature-based TS dataset is primarily used for automatic text summarization, particularly focusing on abstractive summarization of Konkani folktales and addressing data scarcity. It contains 71 stories, enabling researchers to develop and evaluate models tailored to the specific linguistic features of Konkani. Additionally, the dataset supports lexical and semantic analysis, contributing to computational linguistics tasks in related Indian languages such as Assamese, Telugu, and Marathi.; The Konkani Literature-based TS dataset is primarily used for automatic text summarization, particularly in abstractive summarization techniques for the Konkani language. It contains 71 stories and addresses challenges in summarization for under-resourced languages. The dataset is also utilized to enhance lexical resources and support semantic analysis in other Indian languages like Assamese and Telugu, and for linguistic research in Marathi, focusing on morphological and syntactic analysis.	
Indian Corpus	citing_context	L2-Arctic	https://doi.org/10.48550/arXiv.2307.10587 (2023)	https://doi.org/10.1109/ICASSP49357.2023.10095057 (2023)	The L2-Arctic dataset is used to evaluate the performance of automatic speech recognition (ASR) systems on non-native speakers, particularly focusing on accent variability and pronunciation patterns across different linguistic backgrounds. It is employed to benchmark ASR systems, assessing their accuracy and robustness in handling diverse accents and speech characteristics. This dataset enables researchers to address specific challenges in ASR related to non-native speech, enhancing system performance and reliability.	
Indian Corpus	cited_context	L3Cube-MahaCorpus	https://doi.org/10.48550/arXiv.2304.11434 (2023)	https://www.semanticscholar.org/paper/7ebbb9f14a08fda8d76b3f299254c2b0d2c59d9a (2022)	The L3Cube-MahaCorpus dataset is used to train and evaluate monolingual BERT models specifically for the Marathi language. Researchers focus on improving the performance of these models compared to multilingual models, using the dataset to assess and enhance their effectiveness in Marathi language tasks.	
Indian Corpus	cited_context	L3Cube-MahaSent	https://doi.org/10.48550/arXiv.2211.11418 (2022)	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	The L3Cube-MahaSent dataset is used for sentiment analysis of Marathi tweets, specifically to classify sentiments as positive, negative, or neutral. Researchers employ this dataset to analyze social media content, focusing on the nuanced understanding of public opinion and emotional expression in Marathi language contexts. The dataset's relevance lies in its application to social media analysis, enabling detailed classification and interpretation of sentiment in online discussions.	
Indian Corpus	cited_context	large Telugu corpus	https://doi.org/10.1109/IJCNN52387.2021.9534382 (2021)	https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de (2019)	The large Telugu corpus is used to train pre-trained language models for the Telugu language, leveraging a large-scale dataset of 8,015,588 sentences. This corpus enables researchers to generate robust models, enhancing the performance and reliability of natural language processing tasks specific to Telugu. The extensive size of the dataset supports the development of more accurate and contextually aware language models.	
Indian Corpus	cited_context | citing_context	LDC-IL	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022), https://doi.org/10.48550/arXiv.2205.03018 (2022)	https://doi.org/10.1007/s10579-020-09523-3 (2021)	The LDC-IL dataset is used to study language technology resources for Maithili, Konkani, Bodo, Nepali, Kashmiri, and Urdu. It focuses on linguistic data and annotations, enabling researchers to analyze and develop language-specific technologies. The dataset's comprehensive linguistic annotations support the creation and evaluation of natural language processing tools and systems for these languages.; The LDC-IL dataset is used to source and study linguistic data and tools for Maithili, Konkani, Bodo, Nepali, Kashmiri, and Urdu, focusing on language technology resources. It provides essential words and linguistic data, enabling researchers to develop and enhance language technology applications and tools for these languages.	
Indian Corpus	cited_context	Leipzig corpus	https://doi.org/10.1007/978-3-030-86337-1_30 (2021), https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The Leipzig corpus is used to sample text lexicon for multilingual resources, supporting language studies and NLP tasks. It is employed for data augmentation, particularly to enhance news-related content and monolingual dictionaries across multiple languages, including Hindi. The dataset provides small collections of up to 1 million sentences, typically around 300K, from news and web crawls, which are used to enrich linguistic research and improve the quality and coverage of language data.	
Indian Corpus	cited_context	Leipzig Corpora Collection for Gujarati and Telugu	https://doi.org/10.18653/v1/2021.findings-acl.447 (2021)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The Leipzig Corpora Collection for Gujarati and Telugu is used to train language models for both Gujarati and Sanskrit. It provides additional textual data to supplement speech data for Gujarati and enhances the training corpus for Sanskrit. This dataset enables researchers to improve the performance and accuracy of language models by offering diverse and extensive textual resources.	
Indian Corpus	cited_context	Leipzig Tamil Newscrawl 3	https://doi.org/10.18653/v1/D19-5215 (2019)		The Leipzig Tamil Newscrawl 3 dataset is used to enhance the quality of Tamil monolingual training data by sampling 300K sentences, specifically to avoid deterioration from noise. This method ensures high-quality data for training models, improving their performance in tasks requiring Tamil language processing.	
Indian Corpus	cited_context	lexicon of words for Telugu sentiment analysis	https://doi.org/10.18653/v1/W17-5408 (2017)	https://www.semanticscholar.org/paper/bba6fd95578997b7d090277b21f1e4e290c8c7e2 (2010)	The 'lexicon of words for Telugu sentiment analysis' dataset is used to support the task of Telugu sentiment analysis by providing a publicly available resource of words and their sentiment scores. This dataset enables researchers to enhance the accuracy of sentiment classification models for the Telugu language, facilitating the development of more effective natural language processing tools.	
Indian Corpus	citing_context	linked Indian Wordnets	https://www.semanticscholar.org/paper/f126edb612d87fe5b68b5170d242a2f2f31a2e0f (2020)	https://doi.org/10.1145/3297001.3297045 (2019)	The linked Indian Wordnets dataset is used to generate true cognate data, enhancing the creation of cognate datasets and improving phylogenetic trees for Indian languages. It serves as a valuable resource for cognate detection, aiding in the accurate reconstruction of linguistic relationships and evolutionary histories. This dataset's structured lexical information supports methodologies focused on linguistic phylogenetics and comparative linguistics.	
Indian Corpus	citing_context	LoFTI	https://doi.org/10.48550/arXiv.2506.15355 (2025)	https://doi.org/10.48550/arXiv.2407.11833 (2024)	The LoFTI dataset is used to evaluate large language models' (LLMs) ability to provide localized and factual responses in Indian contexts. Researchers focus on assessing the models' localization and factuality transfer, ensuring that the responses are contextually appropriate and accurate. This dataset enables the examination of how well LLMs can adapt to specific regional and cultural nuances, enhancing their applicability in diverse Indian settings.	
Indian Corpus	citing_context	M3LS	https://doi.org/10.1109/CEC60901.2024.10612059 (2024)	https://doi.org/10.48550/arXiv.2302.06560 (2023)	The M3LS dataset is used to conduct thorough comparisons with existing algorithms, particularly large language models, in an unsupervised setting. It focuses on multi-lingual, multi-modal summarization, enabling researchers to evaluate and enhance the performance of these models across different languages and modalities.	
Indian Corpus	citing_context	Malayalam dataset	https://www.semanticscholar.org/paper/693e0ff7b621e4d6d7a5e1738be13021214248ca (2022)	https://doi.org/10.1109/ICDSE.2014.6974627 (2014)	The Malayalam dataset is used for developing and evaluating text summarization systems, specifically focusing on news articles and their corresponding human-written summaries. This dataset enables researchers to test the accuracy and coherence of automated summarization techniques by comparing machine-generated summaries against human-created ones. The dataset's focus on news articles provides a rich source of content for evaluating summarization algorithms in a real-world context.	
Indian Corpus	cited_context	Manipuri news corpus collection	https://www.semanticscholar.org/paper/8b3cb6e7a8709639c6fd835540243e4154aecad3 (2012)	https://www.semanticscholar.org/paper/080aa68650d13a770bb7a228c10f17ce31baea21 (2010)	The Manipuri news corpus collection is used to gather Manipuri news articles in Unicode format using the Bengali script. This dataset supports research in language processing and translation systems, enabling the development and evaluation of algorithms for handling and translating Manipuri text. The Unicode format ensures compatibility and standardization, facilitating computational analysis and system integration.	
Indian Corpus	cited_context	Mann Ki Baat test set	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/d0986c1792bb6565804985e3ecfd2cae4509a5c3 (2020)	The Mann Ki Baat test set is used to evaluate the quality and accuracy of translations of the Indian Prime Minister’s speeches into 8 Indic languages. Researchers employ this dataset to assess translation performance, focusing on linguistic fidelity and contextual accuracy. This evaluation helps in refining translation models and improving cross-lingual communication in official and public discourse.	
Indian Corpus	citing_context	manually built dataset of 200 cricket news articles with human summaries	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1007/978-981-15-5971-6_43 (2020)	The dataset of 200 cricket news articles with human summaries is used to train and evaluate summarization models, specifically focusing on generating high-quality summaries in the Odia language. It serves as input to proposed summarization systems, enabling researchers to assess the effectiveness of these models in producing accurate and coherent summaries for cricket news articles.	
Indian Corpus	cited_context	Marathi data set	https://www.semanticscholar.org/paper/062092a2b27af84b3becffd7439bf8c43574427b (2021)	https://doi.org/10.18653/v1/2022.acl-long.62 (2020)	The Marathi data set is used to evaluate the performance of a fine-tuned LaBSE transformer model, specifically focusing on achieving a high F1 score in a language-agnostic setting. This dataset enables researchers to assess the model's effectiveness in handling Marathi text, contributing to the development of multilingual natural language processing systems.	
Indian Corpus	cited_context	Marathi Dataset	https://www.semanticscholar.org/paper/82200516b244b785cfd2f08ef990fc54e41f4020 (2021)	https://doi.org/10.1145/3574318.3574326 (2022)	The Marathi Dataset is used to train machine learning models for identifying hate speech and offensive content, specifically focusing on the Marathi language. It contains 1874 training samples with an average of 13 words per sentence. This dataset enables researchers to develop and evaluate models tailored to the linguistic nuances of Marathi, enhancing the accuracy of hate speech detection in this language.	
Indian Corpus	cited_context	Marathi dictionary	https://doi.org/10.1109/ICISET.2018.8745621 (2018)	https://doi.org/10.5120/IJCA2016912374 (2016)	The Marathi dictionary dataset is used to develop feature vectors for text classification in Marathi documents. Researchers employ this dataset to create a feature set for supervised learning algorithms, enabling the classification of Marathi texts. This approach focuses on enhancing the accuracy and effectiveness of text classification models in the Marathi language.	
Indian Corpus	citing_context	Marathi Question Classifier	https://doi.org/10.48550/arXiv.2309.15779 (2023)	https://doi.org/10.1007/978-981-10-8569-7_4 (2018)	The Marathi Question Classifier dataset is used to train machine learning classifiers on 1000 manually translated Marathi questions, specifically for identifying answer types. This dataset enables researchers to develop and evaluate models that can accurately classify questions based on the type of answer they require, enhancing natural language processing capabilities in Marathi.	
Indian Corpus	citing_context	विशेष चरित्र _MBTI	https://doi.org/10.1145/3625228 (2023)	https://doi.org/10.1145/3571584 (2023)	The 'विशेष चरित्र _MBTI' dataset is used for detecting MBTI personality types in Hindi text. Research employs an ensemble of SVM kernels with soft voting, integrating social psychology to analyze online network interactions. This methodology leverages the dataset's Hindi text content to enhance personality detection accuracy in digital communication contexts.	
Indian Corpus	citing_context	mC4	https://doi.org/10.1109/PCEMS58491.2023.10136051 (2023)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The mC4 dataset is used to train the mT5 multilingual transformer, enhancing its cross-lingual capabilities by leveraging a corpus that includes data in 101 languages. This training focuses on improving the model's ability to understand and generate text across diverse linguistic contexts, enabling more effective multilingual natural language processing tasks.	
Indian Corpus	citing_context	MCLS	https://doi.org/10.1109/ACCESS.2024.3454382 (2024)	https://doi.org/10.1007/978-981-99-6207-5_17 (2023)	The MCLS dataset is used for generating summaries from multimodal inputs across languages, specifically for cross-lingual summarization tasks. It enables researchers to develop and evaluate models that can handle large-scale, multilingual data, facilitating the creation of more effective cross-lingual summarization systems.	
Indian Corpus	citing_context	MIRACL	https://doi.org/10.48550/arXiv.2506.01615 (2025)	https://doi.org/10.18653/v1/2020.acl-main.653 (2019)	The MIRACL dataset is used to evaluate and enhance cross-lingual capabilities, particularly in question answering and information retrieval across multiple languages. It supports research in cross-lingual transfer performance, extractive QA, and multilingual information retrieval, addressing challenges in neural machine translation and contributing to the development of more robust multilingual systems.	
Indian Corpus	citing_context	MLQA	https://doi.org/10.48550/arXiv.2204.08776 (2022)	https://doi.org/10.18653/v1/2020.acl-main.653 (2019)	The MLQA dataset is used to evaluate cross-lingual extractive question answering systems, assessing their performance across multiple languages, including Indian languages. Researchers employ this dataset to test and compare the effectiveness of models in understanding and extracting answers from text in different linguistic contexts. This enables the development and refinement of multilingual natural language processing techniques.	
Indian Corpus	citing_context	MLQA Hindi	https://doi.org/10.4028/p-tsrdcl (2023)	https://doi.org/10.18653/v1/2020.acl-main.653 (2019)	The MLQA Hindi dataset is used to assess and evaluate cross-lingual question answering, particularly focusing on the transferability of models trained on English to Hindi. It examines model performance across different languages and question types, enabling researchers to analyze the effectiveness of cross-lingual approaches in extractive question answering tasks.	
Indian Corpus	cited_context | citing_context	mMARCO	https://doi.org/10.48550/arXiv.2312.09508 (2023)	https://doi.org/10.48550/arXiv.2210.09984 (2022)	The mMARCO dataset is used to create a large-scale machine-translated version of MS MARCO in multiple languages, including Hindi, for information retrieval tasks. It enables researchers to evaluate and improve cross-lingual information retrieval systems by providing a multilingual resource that supports the development and testing of algorithms designed to handle diverse linguistic data.	
Indian Corpus	cited_context	Mozilla Common Voice (MCV)	https://doi.org/10.1109/ICASSP49357.2023.10095057 (2023)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Mozilla Common Voice (MCV) dataset is utilized to train and evaluate speech recognition models, focusing on enhancing model accuracy and robustness through multilingual and diverse speech data. This dataset supports research in developing speech recognition systems that perform well across various languages and accents, emphasizing the importance of inclusive data representation.	
Indian Corpus	cited_context	MQA	https://www.semanticscholar.org/paper/58999799249fa5ba9af5d7629c8e51a5bab64999 (2018)	https://www.semanticscholar.org/paper/3aa27be7a37ed1d1f80a146a360b67f16ff78b93 (2010)	The MQA dataset is used for cross-language evaluation, specifically to compare with CLEF datasets. It features question-answer pairs in two languages, enabling researchers to enhance the evaluation of cross-language information retrieval and question answering systems. This dataset facilitates the assessment of system performance across different linguistic contexts, ensuring more robust and comprehensive evaluations.	
Indian Corpus	cited_context	MSCOCO captioning dataset	https://doi.org/10.1162/tacl_a_00452 (2021)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The MSCOCO captioning dataset is used to generate parallel text by translating English sentences to Gujarati, focusing on improving image captioning and translation quality. This involves employing machine translation techniques to enhance the accuracy and fluency of captions in Gujarati, leveraging the dataset's rich image and caption pairs.	
Indian Corpus	citing_context	MSR	https://doi.org/10.48550/arXiv.2208.11761 (2022)		The MSR dataset is extensively used for training and evaluating speech recognition models, particularly for Indian languages and dialects. It supports multilingual and code-switching scenarios, enhancing model performance in mixed-language contexts. The dataset's large, crowdsourced nature and diverse speaker base enable robustness in speech recognition systems, making it valuable for linguistic diversity studies and community-based applications. It is also utilized for speech synthesis and formal language analysis, focusing on improving accuracy and handling complex linguistic structures.	
Indian Corpus	cited_context	MSR­NEWS	https://doi.org/10.18653/v1/2021.eacl-main.303 (2021)	https://doi.org/10.18653/v1/W15-3902 (2015)	The MSR-NEWS dataset is primarily used to compare the scale of different corpora in machine transliteration research, particularly in the context of Indian language datasets. It highlights the larger size of the current dataset relative to existing ones, which is crucial for enhancing the robustness and performance of machine transliteration systems. This comparison underscores the dataset's significance in advancing research by providing a more extensive resource for training and evaluation.	
Indian Corpus	cited_context	multi-language character map (MLCM)	https://doi.org/10.21437/Interspeech.2020-2663 (2020)	https://doi.org/10.21437/ssw.2019-35 (2019)	The multi-language character map (MLCM) dataset is used for character-based and phone-based text representations in Indian languages, primarily to facilitate multilingual end-to-end speech synthesis. This dataset supports the development of speech synthesis systems by providing structured mappings of characters and phonetic units, enabling more accurate and natural-sounding speech generation across multiple Indian languages.	
Indian Corpus	citing_context	Naamapadam	https://doi.org/10.48550/arXiv.2405.04829 (2024)	https://doi.org/10.18653/v1/N19-1423 (2019)	The Naamapadam dataset is used to fine-tune RoBERTa models, serving as a benchmark for evaluating script-based performance in Indian language processing. It is employed in comparative studies to assess performance differences between native and roman scripts, focusing on the effectiveness of language processing techniques in these contexts.	
Indian Corpus	cited_context	Nepali-English dataset	https://doi.org/10.3115/v1/W14-3902 (2014)	https://doi.org/10.3115/v1/W14-3915 (2014)	The Nepali-English dataset is used to evaluate system variants for code-mixing in the 2014 shared task, specifically focusing on word-level language classification. This involves assessing the performance of different models in identifying and classifying words in mixed-language texts, enabling researchers to improve natural language processing techniques for multilingual contexts.	
Indian Corpus	cited_context	Nepali National Corpus (NNC)	https://doi.org/10.1017/nlp.2024.26 (2023)	https://doi.org/10.21437/SLTU.2018-19 (2018)	The Nepali National Corpus (NNC) is used to evaluate and compare Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems for the Nepali language, focusing on translation quality. It contains 6535 sentences, enabling researchers to assess the performance and accuracy of these translation models. The dataset's size and content provide a robust basis for linguistic analysis and system comparison in Nepali language translation research.	
Indian Corpus	cited_context	NER corpus for the Shahmukhi	https://doi.org/10.18653/V1/2021.CALCS-1.15 (2021), https://doi.org/10.4018/IJORIS.20210701.OA1 (2021)	https://doi.org/10.18653/v1/W18-3503 (2018)	The NER corpus for the Shahmukhi script is primarily used to develop and evaluate named entity recognition (NER) and part-of-speech (POS) tagging models, particularly for Hindi-English code-mixed content and Punjabi text. It contains 13,860 NER and 11,391 POS instances, aiding in the linguistic analysis of multilingual social media and supporting cross-lingual performance studies. The dataset is also utilized to extend methods to other languages using Devanagari script, enhancing the accuracy and applicability of NER and POS tagging in diverse linguistic contexts.	
Indian Corpus	cited_context	news dataset	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.4304/JETWI.5.3.257-271 (2013)	The 'news dataset' is used to develop and evaluate text summarization systems specifically for the Punjabi language, focusing on news articles. This dataset enables researchers to test the effectiveness of summarization algorithms in generating concise and accurate summaries of news content in Punjabi. The dataset's focus on news articles provides a rich source of varied and contextually relevant text for this purpose.	
Indian Corpus	cited_context	NEWS 2010 Transliteration Mining Shared Task	https://www.semanticscholar.org/paper/02b692bf83e22f98253671b19fad007cafe985a3 (2012)	https://www.semanticscholar.org/paper/c45d9d930543934efef0b7501596c5effd1857dd (2010)	The NEWS 2010 Transliteration Mining Shared Task dataset is used to mine named entity transliteration pairs from linked Wikipedia titles between English and Indian languages (Hindi, Tamil). Researchers employ this dataset to enhance cross-lingual information retrieval and improve transliteration accuracy, focusing on the extraction and alignment of transliterated names to support more effective multilingual search and information access.	
Indian Corpus	cited_context	NEWS 2015 development corpus	https://doi.org/10.18653/v1/W15-3912 (2015)	https://www.semanticscholar.org/paper/02b692bf83e22f98253671b19fad007cafe985a3 (2012)	The NEWS 2015 development corpus is used to train and evaluate systems focused on Hindi-English transliteration. It serves as a benchmark for performance comparison, enabling researchers to develop and refine models that handle transliteration pairs effectively. This dataset supports the advancement of transliteration techniques by providing a standardized set of data for testing and validation.	
Indian Corpus	cited_context	news articles-summary texts	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1007/978-981-15-5971-6_43 (2020)	The 'news articles-summary texts' dataset is used as input for developing summarization systems, specifically focusing on generating summaries for general news articles. The dataset enables researchers to test and refine algorithms designed to produce concise and accurate summaries, enhancing the efficiency of information consumption. It is not specified for Indian language research.	
Indian Corpus	cited_context	NIST LRE	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1109/APSIPA.2017.8282134 (2017)	The NIST LRE dataset is used to provide speech data for language recognition evaluations, particularly focusing on widely spoken languages in India and South Asia, such as Hindi and Tamil. It is employed to evaluate and improve the accuracy of language recognition systems, using these corpora to enhance language identification tasks.	
Indian Corpus	cited_context	NIT-GOA Code-Switch Corpora	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1109/NCC52529.2021.9530035 (2021)	The NIT-GOA Code-Switch Corpora is used for language diarization in code-switched Kannada-English speech, focusing on identifying speaker turns and language switches. Researchers employ attention-based neural networks to classify and generate synthetically code-switched data, enhancing the accuracy of diarization in mixed-language environments. This dataset enables detailed analysis and improvement of diarization techniques in multilingual contexts.	
Indian Corpus	cited_context	NITS-LD corpus	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1007/s00034-018-0962-x (2018)	The NITS-LD corpus is used to develop and evaluate language identification systems for North-Eastern Indian languages. It focuses on prosodic and spectral features, employing a two-stage approach and deep neural networks for multi-level feature modeling. The dataset enables researchers to address tonal and non-tonal pre-classification, enhancing the accuracy of language identification systems.	
Indian Corpus	cited_context	NQ Open	https://doi.org/10.48550/arXiv.2407.13522 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.46 (2020)	The NQ Open dataset is translated to Hindi to enhance cross-lingual information retrieval, specifically for question answering tasks. It supports cross-lingual open-retrieval question answering, focusing on improving performance in non-English languages and enhancing multilingual capabilities in information retrieval systems. This translation facilitates better handling of Hindi queries and improves the accuracy of answers in multilingual environments.	
Indian Corpus	cited_context	corpus of Hindi movie and product reviews	https://doi.org/10.34028/iajit/17/6/14 (2020)		The corpus of Hindi movie and product reviews is primarily used for sentiment analysis research, employing various machine learning classifiers such as SVM, Naive Bayes, KNN, and Decision Tree. It includes 822 positive and 530 negative sentiment-labelled sentences, enabling the development and comparison of classification models. The dataset also supports N-gram and subjective lexicon approaches to analyze review sentiments and has been utilized in training RNN-based models for linguistic analysis across multiple Indian languages.	
Indian Corpus	citing_context	OGI multiple language telephone speech corpus	https://doi.org/10.1109/ICPR.2018.8545406 (2018)	https://doi.org/10.1109/ICASSP.1994.389377 (1994)	The OGI multiple language telephone speech corpus is used to identify 10 languages from telephone speech with 79% accuracy. Researchers employ phoneme recognition and N-gram modeling techniques to achieve this. The dataset's diverse language coverage and telephonic speech characteristics enable robust language identification, making it valuable for developing and testing multilingual speech processing systems.	
Indian Corpus	cited_context	OGLI-MLTS	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.21437/ICSLP.1992-276 (1992)	The OGLI-MLTS dataset is used to store and analyze 1545 telephone conversations in 11 languages, including Indian languages, for linguistic and phonetic studies. Researchers employ this dataset to conduct detailed analyses of linguistic patterns and phonetic features across diverse languages, enabling in-depth explorations of language structure and usage in multilingual contexts.	
Indian Corpus	cited_context	OPUS3	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/f4af3fe736b616452424d50cbd47d52f0a210582 (2020)	The OPUS3 dataset is used to collect sentence-aligned corpora for translation services, focusing on multilingual data, including Indian languages. It enables researchers to develop and improve translation models by providing a rich, aligned textual resource. This dataset supports the creation of more accurate and contextually appropriate translations across multiple languages.	
Indian Corpus	cited_context	OpusParcus corpus	https://doi.org/10.48550/arXiv.2203.05437 (2022)	https://www.semanticscholar.org/paper/5f25a5a00630f736b05b7d3cc8ed7d5d4224a56e (2018)	The OpusParcus corpus is used in research to compare statistical properties and paraphrase detection between its multilingual portions, particularly with the English language segment. Studies employ this dataset to evaluate and detect paraphrases across multiple languages, leveraging its multilingual nature to enhance cross-lingual understanding and analysis.	
Indian Corpus	cited_context	Original Urdu Dataset	https://www.semanticscholar.org/paper/b5673532a3c39d7d8410f9e3d67b4a4a4ba26df4 (2020)	https://doi.org/10.3233/jifs-179905 (2020)	The Original Urdu Dataset is used for researching fake news detection in Urdu, specifically focusing on the veracity of news articles. It contains 500 real and 400 fake news articles, which are utilized to train and evaluate models. This dataset enables researchers to benchmark and improve fake news detection methodologies in the Urdu language.	
Indian Corpus	cited_context | citing_context	Oriya News corpus	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.4018/IJORIS.20210701.OA1 (2021)	The Oriya News corpus is primarily used for text summarization across various Indian languages, including Punjabi, Nepali, Hindi, and Oriya itself. It supports both extractive and abstractive summarization techniques, often leveraging monolingual data for model training and evaluation. The dataset is also utilized for cross-lingual summarization, enhancing accuracy and coherence through parallel data. Its large and short corpora feature aids in developing and evaluating summarization models, particularly within the TDIL project. Additionally, it provides annotated data for language processing tasks, such as content extraction and human summary generation.; The Oriya News corpus is primarily used for text summarization tasks across various Indian languages, including Punjabi, Hindi, and Oriya. It supports both abstractive and extractive summarization methodologies, often employing techniques like Particle Swarm Optimization to enhance summarization quality. The dataset is also utilized for evaluating language processing systems, particularly in Gujarati and Nepali, and for enhancing cross-lingual summarization capabilities, such as generating Hindi summaries from Punjabi text. Its large and short corpora make it valuable for developing and benchmarking summarization models.	
Indian Corpus	cited_context	parallel Wikipedia titles	https://doi.org/10.1007/s12046-018-0828-8 (2018)	https://www.semanticscholar.org/paper/a8c74d482809ca8da4c406068d2f3d466ffba730 (2010)	The 'parallel Wikipedia titles' dataset is used to test enhanced transliteration mining techniques, specifically for extracting transliterations from parallel titles in English–Arabic, English–Russian, English–Hindi, and English–Tamil language pairs. This involves comparing and analyzing title pairs to identify and extract transliterated terms, enabling research into cross-lingual information retrieval and natural language processing tasks.	
Indian Corpus	cited_context	PEI data	https://www.semanticscholar.org/paper/a459724d08d68687f91f707e4608869aa0f9c801 (2020)	https://doi.org/10.18653/v1/S19-2010 (2019)	The PEI data dataset is used to evaluate offensive language detection in social media, building on methodologies from previous evaluation forums like HASOC FIRE 2019 and SemEval 2019. It enables researchers to assess the effectiveness of algorithms in identifying offensive content, contributing to the development of more robust moderation tools. The dataset's relevance lies in its application to social media platforms, where detecting and mitigating harmful language is crucial.	
Indian Corpus	cited_context | citing_context	Phonetic and Prosodically Rich Transcribed speech corpus in Indian languages: Bengali and Odia	https://doi.org/10.1145/3437256 (2021), https://doi.org/10.21437/Interspeech.2018-2529 (2018)	https://doi.org/10.1109/ICSDA.2013.6709901 (2013)	The Phonetic and Prosodically Rich Transcribed speech corpus in Indian languages: Bengali and Odia is used to study phonetic and prosodic features through transcribed speech data. This dataset supports linguistic analysis and speech technology applications, focusing on read speech in these languages. It enables researchers to examine specific phonetic and prosodic characteristics, enhancing understanding and technological advancements in speech processing for Bengali and Odia.; The dataset is used to study phonetic and prosodic features in Bengali and Odia through transcribed speech data, enabling detailed linguistic analysis. It is also utilized to develop a prosodically guided phonetic search engine, which analyzes phonetic and prosodic aspects of speech, particularly in Kannada. The rich transcription and prosodic annotations are crucial for these analyses.	
Indian Corpus	cited_context | citing_context	PIB and Mann ki Baat data	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://www.semanticscholar.org/paper/d0986c1792bb6565804985e3ecfd2cae4509a5c3 (2020)	The PIB and Mann ki Baat dataset is used to create a multilingual parallel corpus for Indian languages, facilitating translation and linguistic analysis. This dataset supports research in developing translation models and analyzing linguistic features across Indian languages, enhancing the accuracy and effectiveness of multilingual communication tools.	
Indian Corpus	cited_context | citing_context	PMIndia	https://doi.org/10.48550/arXiv.2204.08776 (2022), https://doi.org/10.48550/arXiv.2205.03983 (2022)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019), https://www.semanticscholar.org/paper/bfdc43beebd1f0d2b24b8f1564a9072bb21a6a64 (2021)	The PMIndia dataset is extensively used for pre-training and training models on Indian languages, both monolingually and multilingually. It provides large-scale text resources and parallel corpora, enhancing model performance in tasks such as translation, cross-lingual understanding, and contextualized word embeddings. The dataset supports a variety of Indian languages, facilitating research in low-resource and mid-resource settings. Its multilingual and parallel nature enables robust training and evaluation, improving the handling of multiple Indian languages in natural language processing tasks.; The PMIndia dataset is used to support linguistic research and NLP applications for several lower-resourced Northeast Indian languages, including Mizo, Khasi, and Assamese. It provides comprehensive resources for developing and evaluating machine translation systems, enhancing linguistic studies, and improving NLP tasks specific to these languages. The dataset's focus on under-resourced languages enables researchers to address the unique challenges and nuances of these linguistic contexts, contributing to the broader field of computational linguistics.	
Indian Corpus	cited_context	PMI subset	https://www.semanticscholar.org/paper/a05ff6a06948992ecfa93f4c7576583b5272e4c2 (2021)	https://doi.org/10.18653/v1/2021.wat-1.1 (2021)	The PMI subset dataset is used for fine-tuning models in low-resource settings to improve translation quality for Indian languages. It serves as a training set for multilingual machine translation, specifically designed to enhance performance in these languages. This dataset enables researchers to address the challenge of limited data availability, thereby improving the accuracy and fluency of translations involving Indian languages.	
Indian Corpus	cited_context	Product review dataset	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	https://www.semanticscholar.org/paper/3e699c2aee182bdfbdfc82d29a9a39c7ee69dcb6 (2012)	The Product review dataset is primarily used for sentiment analysis and text classification in Indian languages, focusing on news articles, headlines, and product reviews. It provides annotated data for training and testing machine learning models to classify sentiments and topics. The dataset enables researchers to understand public opinion, consumer sentiments, and emotional nuances in text from various sources, including social media and web content.	
Indian Corpus	cited_context	dataset provided by (Patra et al., 2018)	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020)	https://www.semanticscholar.org/paper/5bb962568be873c59217f01c4ad8e072f9cfa3fc (2018)	The dataset provided by Patra et al. (2018) is used to extract Hindi tokens for sentiment analysis of code-mixed Indian languages, particularly focusing on the linguistic features of Hindi in mixed-language contexts. This dataset enables researchers to analyze and understand the sentiment expressed in Hinglish data, enhancing the accuracy of sentiment analysis models for code-mixed languages.	
Indian Corpus	cited_context	Punjabi morph	https://doi.org/10.4304/JETWI.5.3.257-271 (2013)	https://doi.org/10.1007/978-3-642-19403-0_43 (2011)	The Punjabi morph dataset is used to identify and verify root words for nouns and names in the Punjabi language, primarily supporting the preprocessing and stemming phases in text summarization. This dataset aids in ensuring accurate linguistic analysis, enhancing the effectiveness of text summarization techniques by providing reliable morphological data.	
Indian Corpus	cited_context	Punjabi text corpus	https://doi.org/10.4018/IJORIS.20210701.OA1 (2021)	https://doi.org/10.1007/978-3-642-28569-1 (2012)	The Punjabi text corpus is used to extract various statistical and linguistic features, including headline analysis, title similarity, sentence length, TF-ISF, named entity recognition, cue phrase identification, and common nouns between English and Punjabi. This dataset enables researchers to analyze and compare linguistic patterns and features, supporting studies in computational linguistics and natural language processing.	
Indian Corpus	citing_context	question database for Marathi language	https://doi.org/10.48550/arXiv.2309.15779 (2023)	https://doi.org/10.2139/SSRN.3852112 (2021)	The 'question database for Marathi language' is used to develop a question answering system for the Marathi language. It focuses on creating a comprehensive set of questions and answers for training and evaluation, enabling researchers to build and test models that can effectively handle Marathi language queries.	
Indian Corpus	cited_context	data released by O.Hellwig et al. for Sandhi splitting in 2015	https://doi.org/10.1109/ICCS45141.2019.9065571 (2019)	https://www.semanticscholar.org/paper/9e54eda9a0d1889010d405ac8b2b7156fde65b91 (2015)	The dataset released by O. Hellwig et al. in 2015 is used for training and evaluating models on Sandhi splitting in Sanskrit. It focuses on morphological processes and phonological changes in compound words, enabling researchers to develop and test algorithms that accurately split compounded Sanskrit terms. This dataset supports the study of Sanskrit linguistics and computational approaches to language processing.	
Indian Corpus	cited_context	RGB images of Indian sign language alphabets and numbers	https://doi.org/10.1109/SPACES.2015.7058288 (2015)	https://doi.org/10.1016/0146-664X(82)90034-X (1982)	The dataset of RGB images of Indian sign language alphabets and numbers is used to compute elliptic Fourier features of 36 signs, captured from 4 cameras with 4 orientations and involving two signers. This methodology focuses on analyzing the geometric properties of the signs, enabling detailed feature extraction and comparison across different orientations and signers.	
Indian Corpus	cited_context	RPL-English	https://doi.org/10.18653/v1/2021.acl-long.105 (2021)	https://doi.org/10.18653/v1/P19-1579 (2019)	The RPL-English dataset is primarily used for data augmentation in low-resource language (LRL) to English translation tasks. It enhances neural machine translation (NMT) systems by providing additional training data, leveraging bilingual lexicons, unsupervised NMT, and incorporating transliteration and parallel data from related Indo-Aryan languages. This improves model performance and translation quality in low-resource settings.	
Indian Corpus	citing_context	SAIL dataset	https://doi.org/10.1145/3461764 (2021)	https://doi.org/10.1007/978-3-319-26832-3_65 (2015)	The SAIL dataset is used for sentiment analysis in Indian languages, focusing on tweets in Hindi, Bengali, and Tamil. Researchers employ this dataset to analyze public opinion and emotional content, utilizing methodologies that involve natural language processing and machine learning to extract and categorize sentiments from social media data. This enables detailed insights into public reactions and emotions on various topics.	
Indian Corpus	citing_context	SAIL_Code-Mixed Shared Task @ICON-2017	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	https://www.semanticscholar.org/paper/5bb962568be873c59217f01c4ad8e072f9cfa3fc (2018)	The SAIL_Code-Mixed Shared Task @ICON-2017 dataset is used for sentiment analysis of code-mixed Hindi-English and Bengali-English data pairs. Researchers employ this dataset to evaluate the performance of models on mixed-language texts, focusing on the accuracy and effectiveness of sentiment classification in these specific language combinations. The dataset's code-mixed nature is crucial for assessing how well models handle linguistic blending.	
Indian Corpus	cited_context | citing_context	Samanantar	https://doi.org/10.48550/arXiv.2304.13005 (2023), https://doi.org/10.48550/arXiv.2203.05437 (2022)	https://doi.org/10.3115/991146.991220 (1990)	The Samanantar dataset is used as a reference to compare against other datasets, specifically to enhance the correlation with human judgment using BT BertScore. This application focuses on improving the alignment between automated scoring methods and human evaluations, leveraging the dataset's characteristics to refine machine learning models.; The Samanantar dataset provides parallel corpora between English and 11 Indic languages, enabling research in multilingual natural language processing and translation. It is used to train multilingual models for high-quality translation, focusing on general domain content. This dataset supports the development of robust translation systems by offering extensive linguistic data, enhancing the accuracy and fluency of translations across these languages.	
Indian Corpus	citing_context	Samanantar parallel corpus	https://doi.org/10.48550/arXiv.2212.10168 (2022)	https://doi.org/10.1162/tacl_a_00452 (2021)	The Samanantar parallel corpus is used to support translation and multilingual NLP tasks by providing parallel texts between English and 11 Indic languages. This dataset enables researchers to develop and evaluate translation models and multilingual natural language processing systems, enhancing cross-lingual communication and understanding.	
Indian Corpus	cited_context	S ANGRAHA	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	https://doi.org/10.48550/arXiv.2306.01116 (2023)	The S ANGRAHA dataset, comprising 251 billion tokens across 22 languages, is used as pretraining data in natural language processing (NLP) research. It is extracted from curated URLs, existing multilingual corpora, and large-scale translations. This extensive dataset enables researchers to develop and refine multilingual models, enhancing their performance in tasks such as translation and language understanding.	
Indian Corpus	cited_context	S ANGRAHA V ERIFIED	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	https://www.semanticscholar.org/paper/c20c68c45127439139a08adb0b1f2b8354a94d6c (2019)	The 'S ANGRAHA V ERIFIED' dataset is used to curate high-quality monolingual Indian language datasets through a perplexity-based filtering pipeline. It serves as a benchmark for assessing data quality, specifically enhancing the quality of monolingual data for language modeling in Indian languages. This dataset enables researchers to focus on improving the accuracy and reliability of language models by providing a standardized, high-quality reference.	
Indian Corpus	citing_context	script-corrected Hindi-English code-switched dataset	https://doi.org/10.1145/3639565 (2024)	https://doi.org/10.18653/V1/2021.CALCS-1.15 (2021)	The script-corrected Hindi-English code-switched dataset is used for named entity recognition and part-of-speech tagging tasks, particularly in social media contexts. It focuses on normalization and grapheme-to-phoneme conversion, enabling researchers to improve the accuracy of these tasks in code-switched data. The dataset's relevance to social media contexts enhances its utility for analyzing real-world, informal language use.	
Indian Corpus	cited_context	seed corpus HL ma	https://www.semanticscholar.org/paper/02b692bf83e22f98253671b19fad007cafe985a3 (2012)	https://doi.org/10.1007/978-3-642-00958-7_39 (2009)	The 'seed corpus HL ma' dataset is used to train an EW-HMM model for spotting transliteration equivalents, specifically for enhancing cross-language information retrieval. This dataset focuses on improving the handling of out-of-vocabulary query terms, enabling more effective retrieval of information across languages.	
Indian Corpus	citing_context	SemEval 2020 Task 9	https://doi.org/10.1145/3461764 (2021)	https://doi.org/10.18653/v1/2020.semeval-1.100 (2020)	The SemEval 2020 Task 9 dataset is used to evaluate sentiment analysis models on code-mixed Hindi-English tweets. Researchers focus on assessing the performance of various approaches in handling the linguistic complexity of these mixed-language texts, ensuring models can accurately capture sentiment in diverse linguistic contexts.	
Indian Corpus	cited_context	SentiWordNet for Indian Language	https://www.semanticscholar.org/paper/8ffefa0acd2d23e9a3b8315aea0aa6d7a3292205 (2016)	https://www.semanticscholar.org/paper/bba6fd95578997b7d090277b21f1e4e290c8c7e2 (2010)	The SentiWordNet for Indian Language dataset is used to extract sentiment features, including the number of positive and negative tokens and their average scores, specifically for Indian language texts. It is employed to evaluate sentiment analysis models in Hindi, particularly focusing on algorithm performance in understanding social media text. This dataset enables researchers to assess and improve the accuracy of sentiment analysis tools tailored for Indian languages.	
Indian Corpus	citing_context	SentNoB	https://doi.org/10.18653/v1/2024.naacl-long.425 (2024)	https://doi.org/10.18653/v1/2021.findings-emnlp.278 (2021)	The SentNoB dataset is used to analyze sentiment in noisy Bangla text, focusing on 15,000 instances to understand sentiment patterns in real-world, unclean data. Researchers employ this dataset to develop and evaluate methods for sentiment analysis in challenging, uncurated textual environments, addressing the specific nuances and complexities of Bangla language data.	
Indian Corpus	citing_context	data set provided by Goswami et. al	https://doi.org/10.1109/ICAITPR51569.2022.9844182 (2022)	https://doi.org/10.1504/IJAPR.2015.075955 (2015)	The dataset provided by Goswami et al. is used to evaluate the performance of a proposed model on synthetically generated Gujarati numeral strings, particularly focusing on variable length numerals. This involves assessing the model's accuracy and efficiency in recognizing and processing these synthetic samples, which helps in refining the model's capabilities for handling complex numeral variations.	
Indian Corpus	cited_context	ShabdKosh	https://www.semanticscholar.org/paper/7acfa8e71465e233a32fd8acac176e647c8db0a5 (2014)	https://www.semanticscholar.org/paper/d87ceda3042f781c341ac17109d1e94a717f5f60 (1999)	The ShabdKosh dataset is primarily used to enhance language-specific lexical resources for Indian languages, particularly by integrating it into the right-click menu to provide Hindi-English dictionary support. This integration enhances the functionality of software applications, enabling users to access lexical data seamlessly. The dataset's primary application is in improving user interaction and language support in digital environments.	
Indian Corpus	citing_context	ShareChat	https://doi.org/10.1609/icwsm.v14i1.7274 (2020)	https://doi.org/10.2307/410649 (1956)	The ShareChat dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, based on the provided evidence, it cannot be accurately described how ShareChat is used in research.	
Indian Corpus	cited_context	SHRUTI	https://doi.org/10.1109/ICSDA.2013.6709901 (2013)	https://doi.org/10.1109/ICSDA.2011.6085979 (2011)	The SHRUTI dataset is used to represent acoustic-phonetic information in several Indian languages, including Hindi, Telugu, Tamil, Kannada, and Bengali. It focuses on a minimal set of phones, which is crucial for continuous automatic speech recognition. This dataset enables researchers to develop and test phonetic models, enhancing the accuracy of speech recognition systems in these languages.	
Indian Corpus	citing_context	Skit-S2I	https://doi.org/10.48550/arXiv.2212.13015 (2022)	https://doi.org/10.18653/v1/2020.emnlp-main.588 (2020)	The Skit-S2I dataset is used as a benchmark for evaluating intent classification in spoken language understanding models. It provides a more challenging test compared to the FSC dataset, enabling researchers to assess model performance in complex scenarios. This dataset supports the development and refinement of algorithms for better understanding spoken commands and queries.	
Indian Corpus	cited_context | citing_context	SQuAD	https://doi.org/10.48550/arXiv.2407.13522 (2024)	https://doi.org/10.18653/v1/D16-1264 (2016)	The SQuAD dataset is primarily used to evaluate and benchmark machine comprehension models, particularly in question answering tasks. It is applied to assess both monolingual (English) and cross-lingual (Hindi and other Indian languages) performance, using paragraph-question-answer triplets. The dataset supports research in multilingual question answering, cross-lingual information retrieval, and enhancing model robustness through verified translations and augmented training data.; The SQuAD dataset is primarily used to evaluate and enhance machine comprehension models, particularly in question answering tasks. It is applied across multiple Indian languages, including Hindi, using verified translations to assess cross-lingual performance and robustness. Researchers use it to test multilingual question answering systems, focusing on improving model accuracy, coverage, and diversity through both original and translated paragraph-question-answer triplets. The dataset's large scale and diverse content enable rigorous benchmarking and the development of more effective multilingual models.	
Indian Corpus	cited_context	Srī sūryarāyāṃdhra Telugu nighaṃṭuvu	https://doi.org/10.29007/MXDH (2018)		The 'Srī sūryarāyāṃdhra Telugu nighaṃṭuvu' dataset is used to create a comprehensive Telugu lexicon by manually compiling vocabulary from its 8-volume dictionary. This enhances linguistic resources for Telugu, supporting research in lexicography and language documentation. The dataset's extensive vocabulary and structured format enable detailed lexical analysis and resource development.	
Indian Corpus	cited_context	SynthText	https://doi.org/10.1109/ICDAR.2019.00033 (2019)		The SynthText dataset is used to augment training datasets with synthetic scenes containing large multi-script sequences, particularly modified to include around 50 Unicode fonts. This enhancement improves text recognition in Indian languages. The dataset enables researchers to address challenges in recognizing diverse scripts by providing a rich set of synthetic examples, thereby enhancing model performance and robustness in real-world applications.	
Indian Corpus	cited_context	tagged Bengali news corpus	https://www.semanticscholar.org/paper/718a2dfddfdaa2fbc245f4e3793617a683d1f0f5 (2008)	https://doi.org/10.1007/s10579-008-9064-x (2008)	The tagged Bengali news corpus is used to create gazetteer lists for Bengali named entity recognition (NER). Researchers extract and categorize named entities from the corpus, focusing on improving NER accuracy. This dataset enables the development of more effective NER systems by providing a rich source of annotated text, which is crucial for training and testing these models.	
Indian Corpus	cited_context	tamil-db	https://doi.org/10.1007/978-3-030-86337-1_30 (2021)	https://doi.org/10.1109/ICDAR.2013.162 (2013)	The 'tamil-db' dataset is used to develop and improve the recognition accuracy of Tamil handwritten city names for postal automation. Researchers employ this dataset to enhance the performance of recognition systems, focusing on the specific challenge of accurately processing handwritten text in postal systems. This dataset enables the testing and validation of algorithms designed to automate postal sorting processes, particularly in regions where Tamil is widely used.	
Indian Corpus	cited_context	TDIL corpora	https://doi.org/10.1145/3606696 (2023)	https://doi.org/10.1109/ISACC.2015.7377332 (2015)	The TDIL corpora is used to train and evaluate Hidden Markov Models for Nepali Part-of-Speech (POS) tagging, focusing on achieving high accuracy for known words. It defines a consistent tagset of 42 tags, which ensures standardized annotation and evaluation processes. This dataset enables researchers to develop and refine POS tagging models specifically for the Nepali language.	
Indian Corpus	cited_context	TED dataset	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021)	https://doi.org/10.18653/v1/N18-2084 (2018)	The TED dataset is primarily used for training and evaluating neural machine translation models, focusing on cross-lingual performance and translation quality. It provides parallel texts from news content, blog posts, articles, and TED talk transcripts, enabling researchers to analyze language use in multilingual contexts and assess the utility of pre-trained word embeddings. The dataset supports diverse language pairs, including Asian languages, and addresses localization challenges.	
Indian Corpus	cited_context | citing_context	TEL-NLP	https://doi.org/10.1109/IJCNN55064.2022.9892105 (2022)	https://doi.org/10.18653/v1/W17-5408 (2017)	The TEL-NLP dataset is used for training and evaluating models in various NLP tasks for the Telugu language, including sentiment analysis, hate speech detection, stance recognition, and emotion identification. It contains a large sample size of 16,234 and 9,675 samples, enabling researchers to focus on improving model performance in low-resource settings using active learning techniques.; The TEL-NLP dataset is used as a comprehensive resource for multiple NLP tasks in Telugu, including sentiment analysis. It serves as a larger and more extensive alternative to smaller, single-task datasets, enabling researchers to address a broader range of linguistic challenges. The dataset's size and diversity support the development and evaluation of models across various NLP applications, enhancing the robustness and generalizability of research findings.	
Indian Corpus	cited_context | citing_context	Telugu dataset	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1007/978-981-16-1249-7_7 (2021)	The Telugu dataset is primarily used for developing neural abstractive text summarizers, focusing on enhancing summarization quality for the Telugu language. It is employed in both supervised and unsupervised approaches to generate concise and coherent summaries, contributing to advancements in natural language processing techniques for Indian languages.; The Telugu dataset is primarily used for developing and evaluating abstractive text summarizers, specifically focusing on improving summarization quality for the Telugu language. It is employed in neural network models to enhance the generation of concise and linguistically nuanced summaries. This dataset supports research in natural language processing, particularly in summarization techniques, by providing a resource for training and evaluation.	
Indian Corpus	citing_context	Telugu Corpus	https://doi.org/10.18653/v1/2022.acl-srw.20 (2022)	https://doi.org/10.1109/IJCNN52387.2021.9534382 (2021)	The Telugu Corpus is primarily used for pretraining and fine-tuning NLP models, particularly BERT-based and ELECTRA-Te models, to enhance their performance on Telugu language tasks. It is utilized for named entity recognition (NER) in both newswire and medical domains, addressing NLP challenges in resource-poor languages. The dataset enables researchers to improve model performance and evaluate NER capabilities in specific contexts.	
Indian Corpus	citing_context	Telugu (Gangula and Mamidi, 2018)	https://doi.org/10.48550/arXiv.2303.09364 (2023)	https://doi.org/10.1109/taffc.2016.2598569 (2018)	The Telugu (Gangula and Mamidi, 2018) dataset is primarily used for sentiment analysis in the Telugu language, focusing on the classification and regression of emotional content in text. It is employed to annotate Telugu data for valence and arousal, particularly in song lyrics, enhancing the understanding of emotional intensity and linguistic aspects. This dataset supports machine learning techniques, enabling more accurate sentiment analysis systems.	
Indian Corpus	cited_context	Telugu-Hindi bilingual dictionary	https://doi.org/10.1007/978-3-540-73400-0_54 (2007)	https://www.semanticscholar.org/paper/16d5db716e774c60cf76ef5b3912c001e7c2754b (1999)	The Telugu-Hindi bilingual dictionary is used to enhance bilingual dictionary and cognate identification techniques in the Indian language context. Researchers combine this dataset with pattern recognition methods to improve the effectiveness of these techniques, specifically focusing on identifying and enhancing the accuracy of cognates and dictionary entries. This approach aids in advancing computational linguistics and natural language processing for Indian languages.	
Indian Corpus	cited_context | citing_context	Telugu SA dataset	https://doi.org/10.1109/IJCNN55064.2022.9892105 (2022)	https://doi.org/10.1007/978-3-319-64283-3_26 (2017)	The Telugu SA dataset is used to train and evaluate sentiment analysis models for Telugu, categorizing text into positive, negative, and neutral classes across various domains. It highlights the challenges and impacts of limited sample sizes and low-resource settings, often employing active learning techniques to improve model performance. This dataset enables researchers to address the robustness and effectiveness of sentiment analysis systems in the context of Telugu language.; The Telugu SA dataset is used to train and evaluate sentiment analysis models in Telugu, focusing on categorizing text into positive, negative, and neutral classes across various domains. It highlights the challenges and impacts of limited data on model robustness, enabling researchers to assess and improve the performance of sentiment analysis systems in the Telugu language.	
Indian Corpus	cited_context	text documents for Kannada, Tamil and Telugu	https://doi.org/10.1109/ICISET.2018.8745621 (2018)	https://doi.org/10.1109/ICICA.2014.89 (2013)	The dataset of text documents for Kannada, Tamil, and Telugu is used to classify 100 documents per language using linguistic and statistical methods. Researchers apply Zipf's law, Vector Space Model, and TF-IDF weighting, followed by classification with Decision Tree, Naive Bayes, and KNN algorithms. This enables the evaluation of different classification techniques for Indian languages, focusing on accuracy and effectiveness in text categorization.	
Indian Corpus	cited_context	The Indic multi-parallel corpus	https://www.semanticscholar.org/paper/bd3d1ef6eaec411c103d0f7d13b054ba3700ccd5 (2014)		The Indic multi-parallel corpus is used to study and improve translations from Indian languages to English. Researchers focus on the quality and accuracy of non-expert translations obtained through Mechanical Turk, often extending existing parallel corpora with these translations. This dataset enables the evaluation and enhancement of translation methodologies, particularly for texts sourced from Wikipedia.	
Indian Corpus	cited_context	three million words	https://doi.org/10.1109/TSMCC.2010.2095841 (2011)	https://doi.org/10.1109/34.574803 (1997)	The 'three million words' dataset is used to analyze the occurrence frequency of Devanagari characters, focusing on providing statistics for the 20 most frequent characters in the Devanagari script. This analysis employs a frequency-based methodology to quantify character usage, enabling researchers to understand the distribution and prevalence of specific characters in the script.	
Indian Corpus	cited_context	TRAC-1	https://www.semanticscholar.org/paper/5fd2099e9f4c6d9f748685a3bd11460ce6043914 (2020)	https://www.semanticscholar.org/paper/3f1900276eee46beb7ac438ffcd6163d3980c89f (2018)	The TRAC-1 dataset is used for aggression identification in social media, specifically classifying aggressive and non-aggressive content in Hindi and English tweets. Researchers employ this dataset in shared tasks to develop and evaluate machine learning models for detecting aggression, contributing to the understanding and management of online abusive behavior.	
Indian Corpus	citing_context	training dataset for Hindi	https://doi.org/10.48550/arXiv.2212.05702 (2022)	https://doi.org/10.18653/v1/2022.findings-acl.145 (2021)	The 'training dataset for Hindi' is used to fine-tune IndicBART, a natural language processing model, specifically for improving Hindi language generation capabilities. This involves supervised learning methodologies to enhance the model's performance in generating coherent and contextually appropriate Hindi text. The dataset's focus on Hindi enables researchers to address challenges in natural language generation for this language, contributing to advancements in multilingual NLP systems.	
Indian Corpus	cited_context | citing_context	TS dataset	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)		The TS dataset is used to analyze time-series data from the BBC in 44 languages, including major Indian languages like Bengali. Researchers employ this dataset to study linguistic patterns and media representation over time. The dataset's multilingual scope enables comparative analysis, providing insights into how different languages are represented in media content.; The TS dataset is used to analyze time-series data in multiple languages, including Indian languages such as Bengali. Researchers focus on linguistic patterns and temporal dynamics, employing methodologies that examine how language evolves over time. This dataset enables the study of specific research questions related to linguistic changes and temporal behavior in diverse linguistic contexts.	
Indian Corpus	cited_context	TTS dataset	https://doi.org/10.21437/SLTU.2018-28 (2018)	https://www.semanticscholar.org/paper/1d480a2ae449ee625aa543610ed2574e3c87643e (2016)	The TTS dataset is used to build and adapt language-specific Gaussian Mixture Models (GMMs) for Kannada and English, focusing on text-to-speech synthesis. It is also utilized to create artificially generated code-switch corpora for these languages, enabling the study of bilingual code-switching phenomena through speech samples. This dataset facilitates the development and analysis of speech models tailored to handle multilingual contexts.	
Indian Corpus	cited_context	tweets using the Twitter API	https://doi.org/10.1109/BigData.2016.7840818 (2016)	https://doi.org/10.1145/2701126.2701129 (2015)	The 'tweets using the Twitter API' dataset is used to collect and label tweets for sentiment analysis, specifically focusing on political parties BJP and AAP during the 2014 Indian general elections. Researchers employ this dataset to categorize tweets into negative, neutral, and positive sentiments, enabling detailed analysis of public opinion and emotional responses to political events.	
Indian Corpus	citing_context	Twitter dataset	https://doi.org/10.18653/v1/2023.wassa-1.12 (2023), https://doi.org/10.1109/INOCON60754.2024.10511869 (2024)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The Twitter dataset is used for training XLM-T, a variant of XLM-RoBERTa, on a large multilingual corpus, including over 10,000 Gujarati tweets, to enhance cross-lingual representation learning. It is also utilized to analyze depression in the Bengali community on social media, employing machine learning techniques to detect depressive content. These applications leverage the dataset's multilingual and social media-specific characteristics to advance natural language processing and mental health research.	
Indian Corpus	citing_context	TyDiQAGoldP	https://doi.org/10.18653/v1/2023.findings-eacl.131 (2022)	https://doi.org/10.1162/tacl_a_00317 (2020)	The TyDiQAGoldP dataset is used to evaluate question answering systems across typologically diverse languages, including Indian languages like Bengali, Hindi, and Telugu. It focuses on information-seeking questions, enabling researchers to assess system performance in understanding and retrieving accurate answers in multilingual contexts. This dataset supports the development and improvement of cross-lingual question answering models by providing a benchmark for evaluating their effectiveness in handling diverse linguistic structures and information needs.	
Indian Corpus	citing_context	UD-compliant POS-tagging evaluation dataset for Bhojpuri	https://doi.org/10.18653/v1/2024.findings-acl.857 (2024)	https://www.semanticscholar.org/paper/cc58ebb219feb0f6da5c0f52bed1721dcc4068a1 (2020)	The UD-compliant POS-tagging evaluation dataset for Bhojpuri is used to assess the performance of POS-tagging models, particularly in low-resource settings. Researchers focus on evaluating the accuracy and robustness of these models, leveraging the dataset's compliance with Universal Dependencies standards to ensure consistent and reliable results. This dataset enables the development and refinement of NLP tools for Bhojpuri, addressing the challenges of limited linguistic resources.	
Indian Corpus	citing_context	Universal Dependencies	https://www.semanticscholar.org/paper/1b3a5590b0bd5ed81517c3951002f4611c84e8f5 (2020)	https://www.semanticscholar.org/paper/b805693c17961af2cc7f859c1a54320b26036f46 (2020)	The Universal Dependencies dataset is used to evaluate contextualized embeddings for part-of-speech (POS) tagging across multiple languages, assessing their performance and accuracy in linguistic analysis. It is also employed to annotate treebanks for Hindi and Telugu, focusing on syntactic structures and dependencies. This dataset enables researchers to improve and analyze the effectiveness of linguistic models and annotations in diverse languages.	
Indian Corpus	cited_context	Universal Dependencies treebanks	https://www.semanticscholar.org/paper/cc58ebb219feb0f6da5c0f52bed1721dcc4068a1 (2020)	https://www.semanticscholar.org/paper/f77ec4c85a8505b9c9502ccbd94f79082a4689d5 (2018)	The Universal Dependencies treebanks are used to provide syntactic annotations for various Indian languages, including Sanskrit, Hindi, Urdu, Marathi, Tamil, and Telugu. These annotations support the development and evaluation of parsers within the Universal Dependencies framework, enabling researchers to improve and assess the accuracy of syntactic parsing models for these languages.	
Indian Corpus	cited_context | citing_context	Universal Dependencies v1	https://doi.org/10.18653/v1/2022.naacl-main.295 (2022)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The Universal Dependencies v1 dataset is used to evaluate cross-lingual sentence representations, paraphrase identification, and causal commonsense reasoning across multiple languages. It focuses on transferring understanding, improving robustness against adversarial examples, and assessing cause and effect relationships. The dataset's multilingual annotations enable researchers to test models' performance and generalization capabilities across diverse linguistic structures.; The Universal Dependencies v1 dataset is used for evaluating cross-lingual sentence representations, dependency parsing, and syntactic analysis across multiple languages. It supports research in natural language inference, paraphrase identification, and causal commonsense reasoning, often challenging models with adversarial examples. This multilingual treebank collection facilitates the assessment of linguistic structures and semantic relationships, enabling robust cross-lingual evaluations.	
Indian Corpus	cited_context	Universal Dependency (UD) treebank	https://www.semanticscholar.org/paper/d938d15e64f8a1d9ca2935faeb36b9ecdcefbfc8 (2020)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The Universal Dependency (UD) treebank is used to provide POS tagged data for 4 Indian languages (Hindi, Tamil, Telugu, Marathi). It supports linguistic analysis by focusing on both universal and language-specific part-of-speech tagging, enabling researchers to explore grammatical structures and tagging consistency across these languages.	
Indian Corpus	citing_context	UoH corpus	https://doi.org/10.1109/ICICT57646.2023.10134024 (2023)	https://www.semanticscholar.org/paper/7c942817fda2241f7f0a67e828a39fa816e14aee (2018)	The UoH corpus is used to compile a benchmark dataset of 71,474 words, specifically designed for evaluating Sanskrit Sandhi tools. This dataset facilitates the assessment of computational methods in Sanskrit morphology and syntax, providing standard splits for rigorous testing and comparison of different Sandhi processing algorithms.	
Indian Corpus	cited_context	UPOS	https://www.semanticscholar.org/paper/d938d15e64f8a1d9ca2935faeb36b9ecdcefbfc8 (2020)	https://doi.org/10.1162/tacl_a_00051 (2016)	The UPOS dataset is used to evaluate and compare the performance of FastText, Word2Vec, and GloVE embedding models, specifically focusing on part-of-speech tagging accuracy. It supports research in both language-specific and universal part-of-speech tagging, enabling the assessment of these models' effectiveness in accurately tagging parts of speech across different languages.	
Indian Corpus	citing_context	U. V. Swaminatha Iyer Library 589	https://doi.org/10.1111/exsy.12538 (2020)		The U. V. Swaminatha Iyer Library 589 dataset is used to explore palm leaf manuscripts, focusing on their content and structure. Researchers analyze these documents to understand historical and cultural contexts, employing methodologies that involve detailed examination of the texts. This dataset enables in-depth studies of historical documents, providing insights into past societies and cultures.	
Indian Corpus	cited_context	VoxLingua107	https://doi.org/10.1145/3523179 (2022)	https://doi.org/10.1016/j.csl.2019.101027 (2020)	VoxLingua107 is used for Language Identification (LID) purposes, providing a large speech corpus similar to VoxCeleb for speaker recognition. It focuses on developing and evaluating LID systems, enabling researchers to test and improve algorithms for identifying languages from spoken content. The dataset's extensive size and diverse speech samples facilitate robust model training and validation.	
Indian Corpus	cited_context	WEAT Hindi test set	https://doi.org/10.18653/v1/2022.naacl-main.76 (2021)	https://doi.org/10.4103/0019-5545.161480 (2015)	The WEAT Hindi test set is used to create and quantify gender bias across neutral adjectives in Hindi. Researchers employ this dataset to analyze stereotypical language and societal biases in Indian languages, focusing on how certain adjectives are disproportionately associated with specific genders. This methodology helps in understanding and measuring linguistic and cultural gender biases.	
Indian Corpus	citing_context	Web Based Manipuri News Corpus	https://doi.org/10.1145/3639565 (2024)	https://www.semanticscholar.org/paper/8b3cb6e7a8709639c6fd835540243e4154aecad3 (2012)	The Web Based Manipuri News Corpus is used to develop and evaluate a rule-based approach for Bengali–Meetei Mayek transliteration. This involves creating and testing transliteration rules to ensure their accuracy and effectiveness. The dataset's news content provides a rich source of text for this specific linguistic transformation, enabling researchers to assess the performance of their transliteration methods.	
Indian Corpus	cited_context	WikiAnn NER dataset	https://doi.org/10.18653/v1/2020.findings-emnlp.445 (2020)	https://doi.org/10.18653/v1/P17-1178 (2017)	The WikiAnn NER dataset is used for cross-lingual named entity recognition (NER) and linking, covering 282 languages. It enhances multilingual NLP capabilities by providing a large-scale resource for training and evaluating models to identify and link named entities across diverse linguistic contexts. This dataset supports research in improving the accuracy and robustness of NER systems in multilingual environments.	
Indian Corpus	citing_context	WikiLingua	https://doi.org/10.48550/arXiv.2305.08828 (2023), https://doi.org/10.48550/arXiv.2310.18600 (2023)	https://doi.org/10.18653/v1/2020.findings-emnlp.360 (2020)	The WikiLingua dataset is used for multilingual abstractive summarization, aligning summaries and documents across multiple languages using image pivots, particularly in how-to articles. It serves as a benchmark to evaluate models' performance in generating coherent summaries in various languages, including Hindi, and extends beyond English-centric approaches by exploring over 1,500 language pairs.	
Indian Corpus	cited_context | citing_context	Wikipedia	https://doi.org/10.48550/arXiv.2309.15779 (2023), https://doi.org/10.18653/v1/2023.acl-long.693 (2022)	https://doi.org/10.18653/v1/P18-2124 (2018), https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	The Wikipedia dataset is used for training BERT multilingual models on masked language modeling and next sentence prediction tasks across 104 languages, including Indian languages. This approach leverages the extensive and diverse textual content of Wikipedia to enhance the model's understanding and performance in multilingual contexts. The dataset's broad language coverage and rich textual data enable researchers to develop more robust and versatile natural language processing models.; The Wikipedia dataset is used to train MuRIL, a multilingual BERT model, by providing textual and annotated data in 16 Indic languages. This dataset offers parallel corpora, enhancing language representation and understanding in Indian languages. The research focuses on improving multilingual models' performance in these specific linguistic contexts.	
Indian Corpus	cited_context | citing_context	dataset with around 600 videos across 22 classes	https://doi.org/10.1145/3394171.3413528 (2020)	https://doi.org/10.1007/978-3-642-12214-9_18 (2010)	The dataset with around 600 videos across 22 classes is primarily used for recognizing isolated Indian Sign Language gestures in real time. It focuses on gesture classification and real-time processing, enabling researchers to develop and test algorithms for accurate and timely gesture recognition. The dataset's diverse class representation supports robust model training and evaluation.; The dataset with around 600 videos across 22 classes is primarily used for recognizing isolated Indian Sign Language gestures in real-time. It focuses on gesture classification and real-time processing, employing methodologies that enable accurate and timely gesture recognition. This dataset facilitates research in improving real-time sign language communication systems, enhancing the accuracy and responsiveness of gesture recognition algorithms.	
Indian Corpus	cited_context	WMT 2019 English-Nepali	https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	https://doi.org/10.18653/v1/W19-5404 (2019)	The WMT 2019 English-Nepali dataset is used for the corpus-filtering task under low-resource conditions, aiming to enhance translation quality and data efficiency. Researchers employ this dataset to develop and test methods that improve the performance of machine translation systems by filtering and optimizing the training data. This focus on low-resource settings is crucial for advancing translation technologies in less commonly supported languages like Nepali.	
Indian Corpus	cited_context	WMT conference datasets	https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	https://doi.org/10.18653/v1/W19-5301 (2019)	The WMT conference datasets are used for shared news translation tasks, specifically in English-Hindi and English-Gujarati, and for corpus-filtering tasks in English-Sinhala. These datasets focus on improving machine translation systems by enhancing translation accuracy and filtering relevant corpora, thereby advancing the performance of translation models in these language pairs.	
Indian Corpus	cited_context | citing_context	WordNet	https://www.semanticscholar.org/paper/b4fc9183d76947e5467ed27fae9b80d8aca7b2c0 (2022)	https://doi.org/10.1007/s11431-020-1622-y (2020)	WordNet is used in research for enhancing text summarization, particularly in Assamese. It provides lexical and semantic relationships, which are crucial for generating sentence similarity in abstractive summarization. This improves the quality of generated summaries by leveraging the dataset's rich semantic structure.	
Indian Corpus	cited_context	XGLUE	https://doi.org/10.18653/v1/2022.acl-long.150 (2021)	https://doi.org/10.18653/v1/2020.emnlp-main.484 (2020)	The XGLUE dataset is used in research to compare various models in natural language processing (NLP), focusing on cross-lingual pre-training, understanding, and generation tasks. It enables researchers to evaluate and benchmark model performance across different languages, facilitating advancements in multilingual NLP capabilities.	
Indian Corpus	cited_context | citing_context	XL-Sum	https://doi.org/10.48550/arXiv.2212.05702 (2022), https://doi.org/10.48550/arXiv.2203.05437 (2022), https://www.semanticscholar.org/paper/a05ff6a06948992ecfa93f4c7576583b5272e4c2 (2021)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020), https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The XL-Sum dataset is used to train and evaluate the mT5 model for multilingual and low-resource abstractive summarization tasks, covering 44 languages. It enables researchers to assess the performance of abstractive summarization models in diverse linguistic contexts, focusing on generating concise and coherent summaries. The dataset's multilingual nature supports the development of robust summarization systems across various languages, enhancing cross-lingual transfer capabilities.; The XL-Sum dataset is primarily used for multilingual abstractive summarization research, focusing on improving summarization quality and performance across 44 languages, including Indic languages like Nepali and Sinhala. It is utilized to train and evaluate abstractive summarization models, particularly using IndicBART, to enhance summarization in less-resourced languages through techniques such as script unification. The dataset's curation from the same source as the English XSum ensures consistency and comparability in cross-lingual summarization studies.	
Indian Corpus	cited_context	XPersona	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021)	https://doi.org/10.18653/v1/2021.nlp4convai-1.10 (2020)	The XPersona dataset is used to evaluate generative chatbots in a multilingual chit-chat dialogue setting, focusing on personalized interactions and response quality. It enables researchers to assess how well chatbots can generate contextually appropriate and personalized responses across multiple languages, enhancing the evaluation of conversational AI systems.	
Indian Corpus	cited_context	XQuAD	https://doi.org/10.48550/arXiv.2404.16816 (2024)	https://doi.org/10.18653/v1/2020.acl-main.421 (2019)	The XQuAD dataset is primarily used for cross-lingual question answering (QA) and machine translation, evaluating models' ability to answer questions and translate text across multiple languages, including Indian languages. It assesses cross-lingual transferability of monolingual representations through 240 passages and 1190 QA pairs, extending benchmarks for tasks like cross-lingual summarization to improve model performance in generating summaries and translating between low-resource languages.	
Indian Corpus	cited_context	X-SRL	https://doi.org/10.18653/v1/2022.acl-long.179 (2022)	https://doi.org/10.18653/v1/2020.emnlp-main.321 (2020)	The X-SRL dataset is used for cross-lingual semantic role labeling, where it facilitates the inference of semantic roles in translated sentences. This involves automatic translation and label projection methodologies. The dataset enables researchers to address the challenge of aligning semantic roles across languages, enhancing the accuracy of cross-lingual natural language processing tasks.	
Kyrgyz Corpus	cited_context | citing_context	780 dependency trees	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://www.semanticscholar.org/paper/e94e7940851d2908a993b9f90e3dd87c6de7c2f3 (2016)	The dataset '780 dependency trees' is mentioned in the citation context but lacks detailed descriptions of its usage. Therefore, there is no specific information available regarding its application, methodology, research questions, or enabling capabilities in any particular research area.; The '780 dependency trees' dataset is used to model the grammar, syntax, and morphology of the Kyrgyz language, contributing to the Universal Dependencies initiative. This dataset enables researchers to analyze and standardize linguistic structures, facilitating cross-linguistic research and computational linguistics applications.	
Kyrgyz Corpus	citing_context	CC100	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.18653/v1/P17-1178 (2017)	The CC100 dataset is used to provide monolingual and multilingual text data for the Kyrgyz language, supporting cross-lingual language modeling, representation learning, and named entity recognition (NER). It enhances language model training and evaluation, and provides annotated data for cross-lingual tagging and linking, specifically improving text classification and entity recognition capabilities in Kyrgyz.	
Kyrgyz Corpus	cited_context	Common Voice Corpus 9.0	https://doi.org/10.1109/BigComp57234.2023.00037 (2023)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Common Voice Corpus 9.0 is used to build Automatic Speech Recognition (ASR) models, focusing on evaluating speech recognition performance across multiple languages. Researchers employ this dataset to develop and test ASR systems, leveraging its diverse language coverage to enhance model robustness and accuracy. The dataset's multilingual nature enables comparative analysis and cross-linguistic studies, facilitating advancements in speech technology.	
Kyrgyz Corpus	cited_context	KSC	https://doi.org/10.1109/BigComp57234.2023.00037 (2023)	https://www.semanticscholar.org/paper/a9b7a78d7b8741cc6e8a03789a34fb98820b66c4 (2022)	The KSC dataset is used to enhance the quality and diversity of speech data in the Kyrgyz language by extending open-source TTS corpora and adding more data, speakers, and topics. It is integrated into a larger corpus containing transcribed read and spontaneous speech, which supports the study of Kyrgyz language speech patterns. This dataset enables researchers to improve the naturalness and variability of text-to-speech systems and analyze linguistic features in spoken Kyrgyz.	
Kyrgyz Corpus	cited_context | citing_context	Kyrgyz article titles	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://doi.org/10.18653/v1/D19-1410 (2019)	The 'Kyrgyz article titles' dataset is used to analyze language patterns in Kyrgyz by examining a subset of 500 randomly sampled and annotated article titles. This methodology involves deriving meaningful conclusions about specific topics within the language, enabling researchers to explore linguistic trends and content themes in Kyrgyz media.; The 'Kyrgyz article titles' dataset is used to derive meaningful conclusions about topics by translating and sampling 500 titles from a total of 23,284 Kyrgyz articles. These titles are then processed to obtain their embeddings using SentenceBERT, enabling researchers to analyze and understand the thematic content and structure of Kyrgyz articles through computational methods.	
Kyrgyz Corpus	citing_context	Kyrgyz-Russian Parallel Corpus	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.1109/BigComp57234.2023.00037 (2023)	The Kyrgyz-Russian Parallel Corpus is used to support machine translation and automatic speech recognition tasks, particularly for developing models for less-resourced Turkic languages. This dataset enables researchers to train and evaluate models that enhance the processing and understanding of these languages, focusing on improving translation accuracy and speech recognition performance.	
Kyrgyz Corpus	citing_context	KyrgyzTTS	https://doi.org/10.48550/arXiv.2407.05006 (2024), https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.21437/Interspeech.2021-2124 (2021)	The KyrgyzTTS dataset is used for training and testing text-to-speech (TTS) systems in the Kyrgyz language, focusing on enhancing the quality and accuracy of synthesized speech. This dataset enables researchers to develop more effective TTS models for Kyrgyz, addressing the specific phonetic and linguistic challenges of the language.	
Kyrgyz Corpus	citing_context	Kyrgyz-UzTurk	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.48550/arXiv.2205.06072 (2022)	The Kyrgyz-UzTurk dataset is used as a comprehensive linguistic resource for Central Asian languages, including Kyrgyz. It includes a variety of POS tags and syntactic features, enabling detailed linguistic analysis. Researchers use this dataset to explore grammatical structures and syntactic patterns, enhancing understanding of these languages' linguistic properties.	
Kyrgyz Corpus	citing_context	UD-Kyrgyz	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://www.semanticscholar.org/paper/ba233d75aa403092bda0bffc026be7913673ad69 (2021)	The UD-Kyrgyz dataset is used to train and evaluate natural language processing (NLP) models specifically for the Kyrgyz language. It focuses on syntactic and morphological analysis, enabling researchers to develop and refine NLP tools that can accurately process and understand the complex grammatical structures of Kyrgyz. This dataset supports the advancement of NLP technologies tailored to under-resourced languages like Kyrgyz.	
Kyrgyz Corpus	citing_context	WikiAnn data	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.48550/arXiv.2308.15952 (2023)	The WikiAnn data is used to fine-tune Named Entity Recognition (NER) models in the Kyrgyz language. This dataset focuses on enhancing text classification and entity recognition capabilities, specifically addressing the need for improved NER performance in Kyrgyz. The dataset's annotated entities enable researchers to train and evaluate models, thereby advancing natural language processing tasks in this under-resourced language.	
Lao Corpus	citing_context	Lao Dataset	https://doi.org/10.48550/arXiv.2407.19672 (2024)	https://doi.org/10.48550/arXiv.2502.06298 (2025)	The Lao Dataset is used to construct and evaluate SeaBench, a benchmark for assessing multi-turn instruction-following capabilities in Lao and other Southeast Asian languages. It focuses on local multilingual questions and is employed to benchmark language models, evaluating their performance across diverse linguistic contexts in the region. This dataset enables researchers to test and improve the multilingual proficiency of language models, particularly in handling complex, context-dependent instructions.	
Lao Corpus	citing_context	Lao-Thai dictionary	https://doi.org/10.1109/MAPR63514.2024.10660932 (2024)	https://doi.org/10.1109/JCSSE.2016.7748893 (2016)	The Lao-Thai dictionary dataset is used to facilitate Lao-Thai machine translation by leveraging the linguistic similarities between the two languages. Specifically, it focuses on generating translations from segmented Lao words, enhancing the accuracy of machine translation systems. This dataset enables researchers to improve translation quality by providing a structured resource for word segmentation and translation mapping.	
Lao Corpus	citing_context	OSCAR corpus	https://www.semanticscholar.org/paper/01a207b8f352f1971f04cd2a28b8859c4cde3746 (2021)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019)	The OSCAR corpus is utilized in research to process large-scale corpora, particularly in low-resource settings, focusing on efficient data handling and processing pipelines. It is specifically employed to extract and process Lao language text from the Common Crawl corpus, enabling research in medium to low resource infrastructures. This dataset facilitates the development of text processing techniques tailored for under-resourced languages.	
Minangkabau Corpus	citing_context	Minangkabau–Indonesian parallel corpus	https://www.semanticscholar.org/paper/b50c7c60416e334440e18832a9ee9b6fddf636d2 (2020)	https://www.semanticscholar.org/paper/7c5f9a60b5942f6a56210e0efa38988620d254d0 (2008)	The Minangkabau–Indonesian parallel corpus is used for various linguistic and computational tasks, including sentiment analysis, linguistic diversity studies, and language preservation. It consists of manually and automatically translated sentences and words, enabling researchers to analyze Minangkabau's dialectal variations, build bilingual dictionaries, and develop machine translation systems. The dataset's construction from Twitter and Wikipedia enhances its utility for language documentation and processing.	
Minangkabau Corpus	citing_context	SmSA	https://doi.org/10.56127/ijst.v3i3.1739 (2024)	https://doi.org/10.1109/ICAICTA.2019.8904199 (2019)	The SmSA dataset is used to create a culturally relevant Minangkabau language sentiment analysis dataset. Skilled bilingual speakers translate content, ensuring cultural accuracy. This process supports research in developing sentiment analysis tools for the Minangkabau language, addressing the need for culturally sensitive natural language processing resources.	
Northern Thai Corpus	citing_context	Northern Thai Dialect Dataset	https://doi.org/10.48550/arXiv.2504.05898 (2025)	https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The Northern Thai Dialect Dataset is used to train and evaluate models for summarization tasks, specifically focusing on the Northern Thai dialect. It supports the development of multilingual abstractive summarization systems, enabling researchers to enhance the performance and accuracy of these models in handling Northern Thai content.	
Northern Thai Corpus	citing_context	Northern Thai language dataset	https://doi.org/10.48550/arXiv.2502.09042 (2025)	https://doi.org/10.48550/arXiv.2501.19393 (2025)	The Northern Thai language dataset is used to train and evaluate models for Northern Thai language translation, employing fine-tuning techniques. This dataset supports research focused on improving translation accuracy and linguistic processing for Northern Thai, enabling the development of more effective language models and systems.	
Norwegian Corpus	cited_context | citing_context	ASK Corpus	https://doi.org/10.48550/arXiv.2305.03880 (2023)	https://www.semanticscholar.org/paper/af7417fdebb1d51e08f4b77111d6464fbaa9f6a5 (2006)	The ASK Corpus is used as the foundation for deriving NoCoLA, a dataset focused on Norwegian Bokmål texts written by language learners. It enables researchers to study writing patterns and second language acquisition, specifically through the analysis of learner-produced texts. This corpus provides essential data for understanding how non-native speakers acquire and use Norwegian Bokmål.; The ASK Corpus is used to derive NoCoLA, focusing on Norwegian Bokmål texts written by language learners. It supports research into second language acquisition patterns and linguistic features, enabling studies that analyze how learners acquire and use the language. The corpus provides a foundation for understanding the linguistic challenges and progressions in learning Norwegian Bokmål.	
Norwegian Corpus	citing_context	Aviskorpuset	https://www.semanticscholar.org/paper/5297eddbd7fbbc44f61e942df6605bee7367555b (2019)	https://www.semanticscholar.org/paper/104719c7165e93aba1a84d5e82975467d2bdc585 (2017)	Aviskorpuset is used to enhance word embedding models in Norwegian language research. It is employed to retrofit existing models by integrating additional linguistic information from the dataset, thereby improving model performance. Additionally, it serves as a rich source of text for training word embeddings, particularly for lexicon expansion, focusing on adjectives and adverbs. This dataset enables researchers to enrich the vocabulary and linguistic capabilities of their models.	
Norwegian Corpus	citing_context	CLARINO repository	https://doi.org/10.1007/s10579-018-9411-5 (2018)		The CLARINO repository is used to facilitate access to CSV and TSV files containing variable and annotation data for the Norwegian language dataset. Researchers employ this structured repository to obtain and analyze linguistic data, enhancing the accessibility and usability of these resources for various studies. This dataset enables detailed linguistic analysis by providing well-organized and annotated data, supporting research in Norwegian language studies.	
Norwegian Corpus	citing_context	Common Crawl	https://doi.org/10.48550/arXiv.2303.17183 (2023)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019)	The Common Crawl dataset is used to create and enhance multilingual corpora, particularly The Nordic Pile, by providing filtered and cleaned web text. It contributes to the development of Norwegian and other Nordic language datasets through language-specific filtering and quality enhancement. The dataset supports research in multilingual text processing and the creation of high-quality monolingual datasets, facilitating the development of automatic data processing pipelines.	
Norwegian Corpus	cited_context	ConanDoyle-neg	https://www.semanticscholar.org/paper/7bf9f3a9ff4f7caa555baea20edad0b3a8c7de1f (2021)	https://doi.org/10.3115/1596374.1596381 (2009)	The ConanDoyle-neg dataset is primarily used to adapt and refine negation annotation guidelines for Norwegian. It serves as a reference for developing annotation standards, focusing on negation cues and scopes. The dataset is utilized in parallel corpora and Spanish negation-annotated texts to inform the structure and labeling of negation in Norwegian, enabling consistent and linguistically informed annotation practices.	
Norwegian Corpus	citing_context	CrowS-Pairs	https://doi.org/10.18653/v1/2024.gebnlp-1.22 (2024)	https://doi.org/10.18653/v1/2020.emnlp-main.154 (2020)	The CrowS-Pairs dataset is used to measure social biases in masked language models, particularly in the context of Norwegian language models such as NorBERT and NB-BERT. Researchers employ the dataset's metric to score these models, assessing their biases. This enables the evaluation and comparison of social bias levels in different language models, contributing to the development of more equitable AI systems.	
Norwegian Corpus	citing_context	DanNet2	https://www.semanticscholar.org/paper/5297eddbd7fbbc44f61e942df6605bee7367555b (2019)	https://doi.org/10.1561/1500000011 (2008)	The DanNet2 dataset is used as a foundational resource for translating and developing the Norwegian WordNet. It focuses on the structure and content of synsets and named entities, enabling researchers to enhance lexical resources and semantic networks for the Norwegian language. This dataset supports the creation and refinement of linguistic tools, facilitating more accurate and nuanced natural language processing applications in Norwegian.	
Norwegian Corpus	cited_context | citing_context	EDEN	https://www.semanticscholar.org/paper/c10d727438b3c67bd93b1b98e87d71854095d544 (2025), https://www.semanticscholar.org/paper/da4c9c0906f111191d32f18807ec1d4870b4014e (2024)	https://doi.org/10.48550/arXiv.2501.07718 (2025)	The EDEN dataset is primarily used for event detection and abstractive summarization in Norwegian news articles. It provides richly annotated data for identifying and classifying events, which is crucial for evaluating event-based metrics and training models like NorEventGen. The dataset also serves as a benchmark for summarization tasks, enabling researchers to assess the quality and coherence of generated summaries using metrics such as ROUGE-L and BERTScore. Its comprehensive annotations and human-authored summaries make it a valuable resource for advancing natural language processing in Norwegian.; The EDEN dataset is used to compare the size of Norwegian language datasets, particularly focusing on its token count relative to English datasets. Researchers employ this dataset to investigate the adequacy of dataset sizes for language processing tasks, using it to highlight disparities and assess the implications for model training and performance.	
Norwegian Corpus	citing_context	ENPC+	https://doi.org/10.15845/bells.v11i1.3436 (2021)		The ENPC+ dataset is used to measure Mutual Correspondence of verbs and nouns between English and Norwegian, facilitating cross-linguistic analysis. This dataset enables researchers to demonstrate high comparability between the two languages, supporting studies that require precise linguistic alignment and comparison.	
Norwegian Corpus	citing_context	GermanQuAD	https://doi.org/10.48550/arXiv.2305.01957 (2023)	https://doi.org/10.18653/v1/2021.mrqa-1.4 (2021)	The GermanQuAD dataset is used to compare question categorization in non-English languages, specifically focusing on differences in how question types are handled in German versus Norwegian. This involves analyzing and contrasting the methodologies and outcomes of question categorization in these two languages, highlighting the unique characteristics and challenges each presents. The dataset enables researchers to explore linguistic and cultural nuances in question formulation and processing, providing insights into the effectiveness of natural language processing techniques across different languages.	
Norwegian Corpus	citing_context	HPLT corpus	https://doi.org/10.48550/arXiv.2504.07749 (2025)	https://doi.org/10.48550/arXiv.2503.10267 (2025)	The HPLT corpus is used to select contexts for annotation from diverse sources such as Wikipedia, news, books, and public documents. This supports the development of high-performance language technologies. The dataset enables researchers to enhance the quality and breadth of annotated data, which is crucial for training and evaluating natural language processing models.	
Norwegian Corpus	cited_context | citing_context	HurtLex	https://doi.org/10.48550/arXiv.2305.03880 (2023)	https://doi.org/10.4000/BOOKS.AACCADEMIA.3085 (2018)	HurtLex is used in research to map generated text completions to language-specific offensive words, focusing on word-level completion scores. This dataset enables the identification and quantification of offensive language in generated text, facilitating the study of how AI models produce potentially harmful content. The dataset's focus on offensive language allows researchers to evaluate and improve the ethical and social implications of natural language processing systems.; HurtLex is used in research to map generated text completions to language-specific offensive words, focusing on word-level completion scores. This dataset enables the identification and quantification of offensive language, facilitating studies on the detection and analysis of harmful content in natural language processing tasks.	
Norwegian Corpus	citing_context	IMDB	https://www.semanticscholar.org/paper/5297eddbd7fbbc44f61e942df6605bee7367555b (2019)	https://doi.org/10.3765/SALT.V20I0.2565 (2010)	The IMDB dataset is primarily used for sentiment analysis, particularly in creating and evaluating sentiment lexicons. Researchers leverage the dataset to extract words from positive and negative reviews, expanding sentiment lexicons using methods like Potts (2011). It is often combined with other datasets like NoReC to enhance lexicon effectiveness and assess performance in sentiment classification tasks, especially for Norwegian text.	
Norwegian Corpus	citing_context	JaQuAD	https://doi.org/10.48550/arXiv.2305.01957 (2023)	https://www.semanticscholar.org/paper/2ec3fb471f059fe401c7e1d7d7a199ba4feae814 (2022)	The JaQuAD dataset is used to inspire the creation of re-phrasal examples for a Norwegian language dataset, specifically focusing on enhancing question answering and machine reading comprehension systems. This involves generating alternative phrasings of questions to improve model robustness and understanding. The dataset's relevance lies in its ability to support the development of more effective natural language processing models for Norwegian.	
Norwegian Corpus	citing_context	Millennium Cohort Study	https://doi.org/10.1080/10888691.2022.2051510 (2022)	https://www.semanticscholar.org/paper/ccdb08ad95b7a73399bc6507219f0e690f7ac5e5 (2015)	The Millennium Cohort Study is used to examine the impact of socioeconomic factors on language development, particularly in a Norwegian context. While the dataset is not exclusively focused on Norwegian language, it supports research into socioeconomic gaps and their effects on language skills. The dataset enables longitudinal analysis, providing insights into how socioeconomic status influences language development over time.	
Norwegian Corpus	cited_context	MIM-GOLD-NER	https://www.semanticscholar.org/paper/7e555915106090b5e33acef49f94a41b678392c1 (2023)	https://www.semanticscholar.org/paper/f9c67d6fb03d69d712e813cfee1fa9a8fa344d9d (2022)	The MIM-GOLD-NER dataset is used to train the IceBERT model, enhancing its performance in Norwegian language tasks. This dataset, characterized by its large size and language specificity, demonstrates the significant improvement in model performance when trained on extensive, language-specific data. It enables researchers to address the challenge of improving natural language processing models for the Norwegian language.	
Norwegian Corpus	citing_context	Module 3 of the NB Tale dataset	https://www.semanticscholar.org/paper/dcf43f4762d4a9e5252425f97c05b2259d3c3eb8 (2022)		Module 3 of the NB Tale dataset is used to analyze spontaneous speech data in Norwegian, focusing on linguistic patterns and usage in natural settings. It evaluates performance across 12 dialect groups, enabling research on dialectal variation in the Norwegian language. This dataset supports studies that require authentic speech samples to explore regional linguistic differences and speech patterns.	
Norwegian Corpus	citing_context	NLEBench	https://doi.org/10.48550/arXiv.2504.07749 (2025)	https://doi.org/10.48550/arXiv.2312.01314 (2023)	NLEBench is used for comprehensive evaluation and benchmarking of Norwegian language models, focusing on tasks such as text embeddings, question-answering, document-level comprehension, sentiment analysis, and review classification. It supports empirical analysis and assessment of model capabilities and limitations, providing a standardized framework for comparing language models across Scandinavian languages. The dataset includes a combination of 19 existing and 5 newly created datasets, enabling medium-scale benchmarking efforts and comprehensive testing of generative language models in Norwegian Bokmål and Nynorsk.	
Norwegian Corpus	citing_context	NO-BoolQ	https://doi.org/10.48550/arXiv.2501.11128 (2025)	https://doi.org/10.48550/arXiv.2312.01314 (2023)	The NO-BoolQ dataset is used to evaluate generative language models in Norwegian, focusing on their ability to answer yes/no questions derived from Wikipedia passages. This dataset enables researchers to assess model performance in understanding and generating responses in the Norwegian language, providing insights into the models' capabilities and limitations in a specific linguistic context.	
Norwegian Corpus	cited_context | citing_context	NO-CNN/DailyMail	https://doi.org/10.48550/arXiv.2312.01314 (2023)	https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008 (2004)	The NO-CNN/DailyMail dataset is used to fine-tune baseline models for completing related tasks, primarily involving summarization and news article generation. It enables researchers to enhance model performance in these areas through targeted fine-tuning, leveraging the dataset's characteristics to improve the quality and relevance of generated content.; The NO-CNN/DailyMail dataset is used to fine-tune baseline models for completing related tasks, primarily involving summarization and news article generation. It enables researchers to enhance model performance in these areas by providing a rich source of text data. The dataset's characteristics support the development and evaluation of conversational AI and dialogue systems, though its primary application is in text summarization and news generation tasks.	
Norwegian Corpus	citing_context	NoMusic	https://doi.org/10.48550/arXiv.2501.03870 (2025)	https://www.semanticscholar.org/paper/972706306f85b1bfb40c7d35c796ad5174eb0c9c (2021)	The NoMusic dataset is used to fine-tune the mDeBERTa model for Norwegian language tasks. This specialized training data improves model performance by enhancing its ability to handle Norwegian-specific linguistic features. The dataset enables researchers to address challenges in natural language processing for Norwegian, focusing on tasks such as text classification and language understanding.	
Norwegian Corpus	citing_context	NorDial	https://doi.org/10.18653/v1/2024.vardial-1.9 (2024)	https://www.semanticscholar.org/paper/6339db5f01123bee3ad52cee14883899401a7507 (2021)	The NorDial dataset is used to study written Norwegian dialect use, focusing on the presence and frequency of dialect indicators in sentences, particularly comparing written and spoken language. It is also employed to identify and annotate written dialect posts in social media, providing annotated data for linguistic analysis. This dataset enables researchers to analyze and understand the nuances of Norwegian dialects in both formal and informal contexts.	
Norwegian Corpus	citing_context	Nordic Dialect Corpus (NDC)	https://doi.org/10.18653/v1/2024.vardial-1.9 (2024)	https://www.semanticscholar.org/paper/026d698a34d9242d58c3b1070c014ac7f0e079d1 (2009)	The Nordic Dialect Corpus (NDC) is used to analyze Norwegian dialects and language use through transcriptions of interviews. Researchers employ the dataset to study dialectal variations, focusing on linguistic patterns and regional differences. The transcriptions provide insights into sociolinguistic factors and help identify specific linguistic features across various regions. This enables detailed examinations of how language varies and evolves within Norway.	
Norwegian Corpus	citing_context	Nordic Tweet Stream	https://doi.org/10.18653/v1/2024.vardial-1.9 (2024)	https://doi.org/10.5617/dhnbpub.11050 (2018)	The Nordic Tweet Stream dataset is used for real-time monitoring of large, rich language data, focusing on addressing copyright and licensing issues specific to the Nordic region. This dataset enables researchers to analyze and manage the complexities of streaming social media data, providing insights into legal and ethical considerations in data usage.	
Norwegian Corpus	cited_context | citing_context	NoReC	https://www.semanticscholar.org/paper/5297eddbd7fbbc44f61e942df6605bee7367555b (2019), https://www.semanticscholar.org/paper/7bf9f3a9ff4f7caa555baea20edad0b3a8c7de1f (2021)	https://doi.org/10.3115/1220355.1220555 (2004), https://www.semanticscholar.org/paper/16b17270fb6b6cf67e2de7aedf72ad396c3fc9d0 (2017)	The NoReC dataset is used in research to test and evaluate sentiment analysis models, particularly focusing on parameter consistency across different data splits and the impact of filter size on performance with longer documents. It is employed to assess model performance on Norwegian reviews, enabling researchers to refine multi-class sentiment prediction techniques.; The NoReC dataset is used for analyzing professional reviews from Norwegian online news sites across various domains such as music, literature, products, movies, and restaurants. It supports research into Norwegian language and review sentiment, particularly for fine-grained sentiment analysis. The dataset is annotated for detailed sentiment categories, enabling nuanced studies of sentiment in professional reviews.	
Norwegian Corpus	cited_context | citing_context	NoReC_fine	https://doi.org/10.48550/arXiv.2504.07749 (2025), https://doi.org/10.48550/arXiv.2404.18832 (2024), https://doi.org/10.48550/arXiv.2305.03880 (2023), https://www.semanticscholar.org/paper/d0e6e7388287c14c18af3021d528d5e8d6bc1952 (2021)	https://www.semanticscholar.org/paper/3a8b3f8755b8d131d13c87561d66331811cc9ad2 (2019), https://doi.org/10.48550/arXiv.2304.14241 (2023)	The NoReC_fine dataset is extensively used in Norwegian language research, primarily for sentiment analysis at various levels, including sentence, document, and entity-level. It provides fine-grained sentiment annotations and serves as a benchmark for evaluating language models and sentiment classification tasks. Additionally, it supports Named Entity Recognition (NER) and acceptability classification, offering annotated data for entities and grammatical acceptability. This dataset enables comprehensive testing and empirical analysis of Norwegian language models and systems.; The NoReC fine dataset is used for entity-level and fine-grained sentiment analysis in Norwegian text, focusing on annotating sentiment holders, targets, expressions, and their polarities. It also supports negation detection by annotating negation cues and their in-sentence scopes, enhancing the accuracy of sentiment and negation analysis in the Norwegian Review Corpus.	NoReC fine
Norwegian Corpus	cited_context | citing_context	NorNE	https://www.semanticscholar.org/paper/75abf2a44f5dc08b54d2839feabd6e210bd711a5 (2024), https://www.semanticscholar.org/paper/84b3e22fbdefa58452c5136659c12de2adf0e93f (2023), https://doi.org/10.48550/arXiv.2501.03870 (2025), https://www.semanticscholar.org/paper/7e555915106090b5e33acef49f94a41b678392c1 (2023), https://www.semanticscholar.org/paper/d0e6e7388287c14c18af3021d528d5e8d6bc1952 (2021), https://www.semanticscholar.org/paper/5b91479c211b09c38cb0c782713639f3d5b3f2ee (2021)	https://www.semanticscholar.org/paper/0322f23d07085e474254f1a40962a72ccfbd4b47 (2019)	The NorNE dataset is primarily used for named entity recognition (NER) in Norwegian text, focusing on categories such as names, organizations, locations, products, events, and derived words. It is employed to train Conditional Random Fields (CRFs) and annotate text, enhancing NER accuracy for both Bokmål and Nynorsk. The dataset supports natural language processing research, contributing to the development of NER models and corpora, including a Scandinavian NER-corpus.; The NorNE dataset is primarily used for Named Entity Recognition (NER) in Norwegian, specifically in both Bokmål and Nynorsk variants. It provides annotated data to train and evaluate NER models, focusing on entity types such as person, organization, and location. The dataset enhances the Norwegian Dependency Treebank with rich entity annotations, improving the accuracy and consistency of entity recognition in Norwegian text. It also supports comparative studies across Nordic languages, including Swedish.	
Norwegian Corpus	cited_context | citing_context	NorQuAD	https://doi.org/10.48550/arXiv.2305.03880 (2023), https://doi.org/10.48550/arXiv.2504.07749 (2025)	https://doi.org/10.48550/arXiv.2305.01957 (2023)	The NorQuAD dataset is used to evaluate question answering systems and machine reading comprehension models in the Norwegian language. It measures token-level F1 scores on manually created question-answer pairs derived from Wikipedia and news articles. Research focuses on comparing model performance against human performance, specifically evaluating the capability to generate text spans as answers.; The NorQuAD dataset is used to evaluate question answering systems by measuring token-level F1 scores on manually created question-answer pairs derived from Wikipedia and news articles. It specifically assesses extractive question answering in Norwegian, comparing model performance against a human baseline of 91.1% F1. This dataset enables researchers to benchmark and improve the accuracy of natural language processing models in understanding and extracting answers from text.	
Norwegian Corpus	citing_context	Norsk Ander-språkscorpus	https://doi.org/10.48550/arXiv.2504.07749 (2025)	https://www.semanticscholar.org/paper/af7417fdebb1d51e08f4b77111d6464fbaa9f6a5 (2006)	The Norsk Ander-språkscorpus dataset is used for grammatical error correction, focusing on Norwegian as a second language. It serves as a learner corpus, enabling researchers to analyze and correct grammatical errors made by non-native speakers. This dataset facilitates the development and evaluation of error correction models and tools, enhancing the understanding of common linguistic challenges faced by learners of Norwegian.	
Norwegian Corpus	citing_context	Norske Synonymer	https://www.semanticscholar.org/paper/5297eddbd7fbbc44f61e942df6605bee7367555b (2019)	https://www.semanticscholar.org/paper/d38763b4b0bbf1ed91b55107c658fcc96f8ef82d (2011)	The Norske Synonymer dataset is primarily used to create and enrich sentiment lexicons for Norwegian, serving as a comprehensive dictionary for word meanings, synonyms, and usage. It is translated and adapted from English to Norwegian, often involving manual adjustments. This dataset enhances lexical resources, supports sentiment analysis, and evaluates the performance of machine-translated sentiment lexicons, focusing on improving accuracy and representation of polarity in Norwegian text.	
Norwegian Corpus	citing_context	NorSumm	https://www.semanticscholar.org/paper/c10d727438b3c67bd93b1b98e87d71854095d544 (2025)	https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008 (2004)	The NorSumm dataset is used to generate and evaluate abstractive summaries of Norwegian news articles. Researchers focus on optimizing the highest average scores of ROUGE-L and BERTScore metrics to assess summary quality. This dataset enables the development and comparison of summarization models, specifically tailored for the Norwegian language, by providing a benchmark for evaluating their performance.	
Norwegian Corpus	cited_context	Norwegian Analogy Test Set	https://www.semanticscholar.org/paper/6a0eb98f274239499ebab7132adb870f5a647481 (2018)	https://www.semanticscholar.org/paper/c4fd9c86b2b41df51a6fe212406dda81b1997fd4 (2013)	The Norwegian Analogy Test Set is used to evaluate analogical reasoning in Norwegian by translating and adapting the Google analogies test set. It focuses on linguistic regularities in word representations, enabling researchers to assess how well models capture semantic and syntactic relationships in the Norwegian language. This dataset facilitates the evaluation of word embedding models and their ability to handle linguistic nuances specific to Norwegian.	
Norwegian Corpus	cited_context	Norwegian Bokmål Universal Dependencies treebank	https://doi.org/10.18653/v1/W18-6017 (2018)	https://www.semanticscholar.org/paper/a63e4d7fc0c83b27dc9a656ce4f0bce448ae92e8 (2009)	The Norwegian Bokmål Universal Dependencies treebank is primarily used to train delexicalised models for parsing, emphasizing syntactic and morphological features in Norwegian NLP. It serves as a foundational resource for developing accurate parsers and is also utilized in the Apertium machine translation system to translate content into Swedish, Danish, Norwegian Bokmål, and Norwegian Nynorsk. This dataset's rich syntactic annotations enable robust training and evaluation of NLP models, enhancing their performance in parsing and translation tasks.	
Norwegian Corpus	cited_context | citing_context	Norwegian Colossal Corpus (NCC)	https://www.semanticscholar.org/paper/7e555915106090b5e33acef49f94a41b678392c1 (2023), https://www.semanticscholar.org/paper/bbe6b4ec62ad36dba3eb376a805fffdfe5ec6387 (2022)	https://www.semanticscholar.org/paper/bbe6b4ec62ad36dba3eb376a805fffdfe5ec6387 (2022), https://www.semanticscholar.org/paper/5b91479c211b09c38cb0c782713639f3d5b3f2ee (2021)	The Norwegian Colossal Corpus (NCC) is primarily used to train large Norwegian language models, including BERT-based models, by providing extensive and clean data from various sources. It supports the development of language models for Norwegian Bokmål and Nynorsk, enabling the creation of comprehensive resources like the NB-BERT model. The dataset's size and comprehensiveness make it a critical asset for advancing NLP applications in the Norwegian language, though some studies note the need for further evaluation.; The Norwegian Colossal Corpus (NCC) is primarily used to develop and train large-scale Norwegian language models, particularly BERT-based models, focusing on Bokmål and Nynorsk. It supports the creation of a national digital library and transformer models by providing a vast, clean corpus of 49GB to 109GB, making it the largest public collection in the Nordics. Despite its size, some iterations lack rigorous evaluation, indicating a need for further research.	
Norwegian Corpus	cited_context | citing_context	Norwegian Dependency Treebank	https://doi.org/10.48550/arXiv.2501.07718 (2025), https://www.semanticscholar.org/paper/5b91479c211b09c38cb0c782713639f3d5b3f2ee (2021)	https://www.semanticscholar.org/paper/da4c9c0906f111191d32f18807ec1d4870b4014e (2024), https://www.semanticscholar.org/paper/87b924e2134f6782acc59ab567f6bf87cb5e5d9f (2016)	The Norwegian Dependency Treebank is primarily used for creating and evaluating event detection models in Norwegian news articles, focusing on summarization tasks. It provides rich linguistic annotations, including event triggers, arguments, named entities, morphosyntactic details, and co-reference information. The dataset's dev and test splits ensure annotation consistency and support high-quality summarization and translation of news articles. It serves as a foundational resource for natural language processing tasks, particularly in the context of Norwegian news.; The Norwegian Dependency Treebank is used for linguistic and computational linguistic studies, focusing on event and argument identification, named entity recognition, and morphosyntactic analysis in Norwegian news articles. It provides richly annotated data, including event triggers, arguments, and co-reference information, which are utilized to train and evaluate event detection models. The dataset's manual annotations of morphological features, syntactic functions, and hierarchical structures enable detailed linguistic analysis and support tasks such as summarization and model performance evaluation.	
Norwegian Corpus	cited_context | citing_context	Norwegian Dependency Treebank (NDT)	https://www.semanticscholar.org/paper/da4c9c0906f111191d32f18807ec1d4870b4014e (2024), https://doi.org/10.48550/arXiv.2210.06150 (2022), https://www.semanticscholar.org/paper/d0e6e7388287c14c18af3021d528d5e8d6bc1952 (2021)	https://www.semanticscholar.org/paper/c4adb896b0c905d610b96891b4e5019627783f82 (2014)	The Norwegian Dependency Treebank (NDT) is used for training and evaluating dependency parsers, focusing on Norwegian syntactic structures. It supports morphosyntactic, named entity, and co-reference annotations in news texts, facilitating linguistic analysis and NLP tasks such as POS tagging, lemmatization, and parsing. The dataset serves as a gold-standard resource, enabling robust research into Norwegian language processing methodologies.; The Norwegian Dependency Treebank (NDT) is used for part-of-speech tagging, dependency parsing, and named entity recognition in Norwegian texts, including both Bokmål and Nynorsk. It provides annotated linguistic data in the Universal Dependencies format, supporting syntactic and morphosyntactic analysis, as well as co-reference annotations in news texts. This dataset enables detailed linguistic analysis and enhances natural language processing tasks by offering richly annotated data.	
Norwegian Corpus	citing_context	Norwegian Language Bank	https://www.semanticscholar.org/paper/49de31929b47ba2cb9635bcdfa138e7d647ffa5b (2006)		The Norwegian Language Bank is utilized as a foundational resource for language technology research and development, offering a vast collection of both written and spoken Norwegian data. This dataset supports the creation and improvement of language technologies by providing extensive, authentic linguistic material. It enables researchers to develop and test algorithms, models, and applications specific to the Norwegian language, enhancing natural language processing capabilities.	
Norwegian Corpus	citing_context	Norwegian LBK corpus	https://doi.org/10.48550/arXiv.2502.06692 (2025)	https://doi.org/10.5617/osla.8176 (2020)	The Norwegian LBK corpus is used to analyze the frequency and context of commas preceding 'that' in Norwegian texts. Researchers employ this dataset to examine linguistic patterns and usage variations, focusing on syntactic structures and punctuation norms in written Norwegian. This analysis helps in understanding grammatical trends and stylistic choices in the language.	
Norwegian Corpus	cited_context	Norwegian newspaper corpus	https://doi.org/10.18653/v1/2022.aacl-short.53 (2022), https://doi.org/10.18653/v1/2022.gebnlp-1.21 (2022)	https://www.semanticscholar.org/paper/d0e6e7388287c14c18af3021d528d5e8d6bc1952 (2021)	The Norwegian newspaper corpus is primarily used to train NorBERT, a contextualized language model for Norwegian. It focuses on large-scale language modeling, utilizing about two billion word tokens from diverse news articles and web-crawled data. This enhances the model's ability to capture linguistic patterns and general knowledge, improving its performance in Norwegian language tasks.	
Norwegian Corpus	citing_context	Norwegian Newspaper Corpus	https://doi.org/10.18653/v1/2024.gebnlp-1.22 (2024)	https://www.semanticscholar.org/paper/66f1184d4436aa27ac8b2bafdb0098f65113248f (2010)	The Norwegian Newspaper Corpus is primarily used to train word embeddings and large language models (LLMs) for the Norwegian language. It supports linguistic and computational research by capturing semantic vectors and representing diverse writing styles. The dataset, comprising 49GB of textual data from newspapers, books, and public documents, is also used to investigate and address potential biases, particularly concerning LGBTQIA+ communities, in trained models.	
Norwegian Corpus	citing_context	Norwegian Newspaper Corpus (NAK)	https://doi.org/10.18653/v1/2024.gebnlp-1.22 (2024)	https://www.semanticscholar.org/paper/6a0eb98f274239499ebab7132adb870f5a647481 (2018)	The Norwegian Newspaper Corpus (NAK) is used to pre-train static word embeddings for the Norwegian language, specifically focusing on capturing semantic vectors from newspaper texts. This methodology involves leveraging the large volume of textual data to generate high-quality word representations that capture the semantic nuances present in newspaper articles. These embeddings enable researchers to address tasks requiring nuanced understanding of Norwegian language semantics, such as natural language processing and text analysis.	
Norwegian Corpus	cited_context | citing_context	Norwegian Review Corpus (NoReC)	https://www.semanticscholar.org/paper/a1db5c13b276e0bb1f1daa16b8ab466b1dfa8b1a (2023), https://www.semanticscholar.org/paper/3a8b3f8755b8d131d13c87561d66331811cc9ad2 (2019), https://www.semanticscholar.org/paper/5b91479c211b09c38cb0c782713639f3d5b3f2ee (2021)	https://www.semanticscholar.org/paper/7bf9f3a9ff4f7caa555baea20edad0b3a8c7de1f (2021), https://www.semanticscholar.org/paper/3a8b3f8755b8d131d13c87561d66331811cc9ad2 (2019)	The Norwegian Review Corpus (NoReC) is used to annotate and analyze negation and fine-grained sentiment in professional reviews across multiple domains such as literature, video games, music, products, movies, TV-series, and restaurants. It aids in studying the impact of negation on sentiment, the relationship between sentiment and negation, and the prominence of positive over negative valence in Norwegian reviews. The dataset supports parsing tasks using static embeddings and provides structured annotations for detailed emotional content analysis.; The Norwegian Review Corpus (NoReC) is used to train and evaluate fine-grained sentiment analysis models for Norwegian, focusing on nuanced sentiment in review texts. It is also utilized to analyze the sentiment valence in Norwegian reviews, examining the distribution of positive and negative ratings at the document level. The dataset includes professionally authored reviews across various domains such as literature, video games, music, products, movies, TV series, and restaurants, enabling comprehensive sentiment analysis and model evaluation.	
Norwegian Corpus	cited_context | citing_context	Norwegian Statistics bureau	https://doi.org/10.48550/arXiv.2305.03880 (2023)	https://doi.org/10.18653/v1/2022.gebnlp-1.21 (2022)	The Norwegian Statistics bureau dataset is used to study occupational biases in Norwegian language models. Researchers extract a set of occupations to analyze gendered headwords and verbs associated with performing these jobs. This involves examining how language models encode gender biases, using the dataset's comprehensive list of occupations to identify and quantify these biases.; The Norwegian Statistics bureau dataset is used to analyze gender biases in Norwegian language models by extracting a set of occupations. Researchers focus on the relationship between gendered headwords and occupational verbs, employing the dataset to identify and quantify gender biases in language. This enables a deeper understanding of linguistic gender dynamics and their implications.	
Norwegian Corpus	citing_context	Norwegian Treebank	https://www.semanticscholar.org/paper/16b17270fb6b6cf67e2de7aedf72ad396c3fc9d0 (2017)	https://www.semanticscholar.org/paper/c5f5bb3131f5f1082bd82cce8a0ac08dea1e9366 (2016)	The Norwegian Treebank is used to train models for syntactic parsing and morphological analysis, specifically enhancing the accuracy of dependency parsing and part-of-speech (POS) tagging in Norwegian. This dataset enables researchers to develop and refine algorithms that better understand and process Norwegian language structures, contributing to advancements in natural language processing (NLP) for Norwegian.	
Norwegian Corpus	cited_context	Norwegian UD treebanks	https://www.semanticscholar.org/paper/6a0eb98f274239499ebab7132adb870f5a647481 (2018), https://www.semanticscholar.org/paper/16b17270fb6b6cf67e2de7aedf72ad396c3fc9d0 (2017)	https://www.semanticscholar.org/paper/c5f5bb3131f5f1082bd82cce8a0ac08dea1e9366 (2016)	The Norwegian UD treebanks are used to train models for syntactic parsing and morphological analysis of Norwegian, including tokenization, POS tagging, and dependency parsing in CoNLL-U format. These treebanks support joint parsing of Norwegian Bokmål and Nynorsk, focusing on syntactic structures and dependency relations within the Universal Dependencies framework. They enable the development of pre-trained models for Norwegian Bokmål, enhancing syntactic parsing and dependency analysis.	
Norwegian Corpus	cited_context | citing_context	Norwegian Universal Dependencies Treebank	https://doi.org/10.48550/arXiv.2305.03880 (2023)	https://www.semanticscholar.org/paper/c4adb896b0c905d610b96891b4e5019627783f82 (2014)	The Norwegian Universal Dependencies Treebank is used for named entity recognition and dependency parsing in Norwegian text, covering both Bokmål and Nynorsk. It is part of UD 2.11 and has been annotated with named entities and converted from NDT. This dataset enables researchers to perform Universal Dependencies tasks, enhancing parsing and dependency analysis in Norwegian language studies.; The Norwegian Universal Dependencies Treebank is used for named entity recognition and dependency parsing in Norwegian text, covering both Bokmål and Nynorsk variants. It is part of UD 2.11 and has been converted from NDT. Researchers annotate and parse the dataset to enhance understanding and processing of Norwegian language structures, supporting tasks such as parsing and dependency analysis.	
Norwegian Corpus	cited_context	Norwegian Web as Corpus	https://www.semanticscholar.org/paper/6a0eb98f274239499ebab7132adb870f5a647481 (2018)	https://www.semanticscholar.org/paper/66f1184d4436aa27ac8b2bafdb0098f65113248f (2010)	The Norwegian Web as Corpus is extensively used for linguistic studies and natural language processing tasks, particularly for training word and semantic vectors. It provides a large-scale resource of over 1 billion tokens of Norwegian Bokmål, enabling researchers to analyze linguistic patterns, language variation, and usage in web-based texts. The dataset's extensive coverage and diverse content facilitate the enhancement of semantic vectors and support studies in language processing and linguistic analysis.	
Norwegian Corpus	cited_context | citing_context	NoWaC	https://www.semanticscholar.org/paper/3a8b3f8755b8d131d13c87561d66331811cc9ad2 (2019), https://www.semanticscholar.org/paper/6a0eb98f274239499ebab7132adb870f5a647481 (2018), https://www.semanticscholar.org/paper/66f1184d4436aa27ac8b2bafdb0098f65113248f (2010)	https://www.semanticscholar.org/paper/104719c7165e93aba1a84d5e82975467d2bdc585 (2017), https://www.semanticscholar.org/paper/9c6b30794a3e2aa577b216be48f020ac81e84b62 (2008)	The NoWaC dataset is used to train 100-dimensional fastText SkipGram word embeddings, specifically for the Norwegian language. It provides a large textual resource that supports linguistic analysis, enabling researchers to explore and model Norwegian language data effectively. This dataset facilitates the development of language-specific embeddings, enhancing the accuracy and relevance of linguistic studies in Norwegian.; The NoWaC dataset is used to create a large web-derived corpus of Norwegian Bokmål, primarily for linguistic analysis. It involves collecting URLs through search engine queries and is utilized to study language patterns and linguistic structures. Additionally, it serves as a resource for training 100-dimensional fastText SkipGram word embeddings, enhancing the analysis of the Norwegian language.	
Norwegian Corpus	citing_context	NQ dataset	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://doi.org/10.18653/v1/D19-1410 (2019)	The NQ dataset is used to evaluate model performance on Norwegian language tasks, particularly focusing on out-of-domain performance. Researchers use this dataset to check for registered answers, ensuring models can handle diverse and potentially unfamiliar content. This enables assessments of model robustness and generalization capabilities in Norwegian language processing tasks.	
Norwegian Corpus	citing_context	NST dataset	https://doi.org/10.48550/arXiv.2307.01672 (2023)		The NST dataset is used to analyze Norwegian language data, focusing on phonetic and phonetic transcriptions. It supports technical improvements in speech recognition systems by enhancing pronunciation accuracy and system robustness. The dataset's comprehensive nature, including a diverse set of phonetic and lexical items, aids in the development of speech technologies and contributes to research in linguistics and language technology.	
Norwegian Corpus	citing_context	sentiment classification dataset presented in Svensson (2017)	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://www.semanticscholar.org/paper/5bda1881b6abecdd28bc746cc9e09769768498c4 (2017)	The sentiment classification dataset presented in Svensson (2017) is used for analyzing sentiment in Norwegian text, particularly in reviews from Swedish websites. It employs sentiment classification methodologies to assess positive, negative, or neutral sentiments. This dataset enables researchers to explore the nuances of sentiment expression in Norwegian, providing insights into consumer opinions and feedback.	
Norwegian Corpus	citing_context	SID data	https://doi.org/10.48550/arXiv.2501.03870 (2025)	https://doi.org/10.48550/arXiv.2303.17683 (2023)	The SID data dataset is used to train models for speaker identification and analyzing dialectal variations across multiple languages. It contains speech samples from various languages and dialects, enabling researchers to develop and test algorithms that can accurately identify speakers and distinguish between different dialects. This dataset supports research in multilingual speech processing and dialectology.	
Norwegian Corpus	cited_context	Språkbanken	https://www.semanticscholar.org/paper/5b91479c211b09c38cb0c782713639f3d5b3f2ee (2021)		Språkbanken is used to provide text collections and curated corpora for linguistic research, specifically supporting the scholarly community with Norwegian language data. This dataset enables researchers to access high-quality, structured linguistic resources, facilitating studies on language structure, usage, and evolution. The curated nature of the corpora ensures that the data is reliable and suitable for rigorous academic analysis.	
Norwegian Corpus	cited_context | citing_context	Stanford Alpaca dataset	https://doi.org/10.48550/arXiv.2312.01314 (2023)	https://doi.org/10.48550/arXiv.2212.10560 (2022)	The Stanford Alpaca dataset is used to create a Norwegian Bokmål version through translation using OpenAI’s gpt-3. This process focuses on aligning language models with self-generated instructions, enabling research into multilingual language model adaptation and the effectiveness of automated translation in maintaining instructional coherence.	
Norwegian Corpus	citing_context	SUC3	https://doi.org/10.48550/arXiv.2304.00906 (2023)		The SUC3 dataset is used to provide NER-enriched data for Norwegian language research, focusing on linguistic features and structures. It supports the analysis of named entities within the Norwegian language, enabling researchers to explore linguistic patterns and structures. This dataset facilitates detailed linguistic research by offering enriched data that enhances the accuracy and depth of analyses.	
Norwegian Corpus	cited_context	Talbanken	https://www.semanticscholar.org/paper/87b924e2134f6782acc59ab567f6bf87cb5e5d9f (2016)	https://www.semanticscholar.org/paper/daddb6421bbff147afb19d2d705727aadf9cd2c6 (2006)	Talbanken is used as a reference for developing dependency representations in a Norwegian treebank. It focuses on syntactic annotation schemes and dependency structures, aiding researchers in standardizing and refining annotation practices for Norwegian linguistic data. This dataset enables the creation of consistent and reliable syntactic annotations, supporting the development of robust linguistic resources.	
Norwegian Corpus	cited_context	Talk of Norway Corpus	https://www.semanticscholar.org/paper/6339db5f01123bee3ad52cee14883899401a7507 (2021)	https://doi.org/10.1007/s10579-018-9411-5 (2018)	The Talk of Norway Corpus is utilized for analyzing linguistic diversity and language use in a multilingual context, particularly focusing on Norwegian and Sámi languages. It includes transcriptions of parliamentary speeches and spoken data, enabling detailed studies of dialectal variations, linguistic features, and historical patterns in various Norwegian language varieties. The rich annotations and multilingual resource nature of the dataset support comprehensive linguistic analysis.	
Norwegian Corpus	citing_context	Tatoeba	https://doi.org/10.48550/arXiv.2504.07749 (2025)	https://doi.org/10.48550/arXiv.2501.07718 (2025)	The Tatoeba dataset is primarily used for evaluating machine translation capabilities between English and Malay (EN ↔ BM) and English and Norwegian Nynorsk (EN ↔ NN). It is also utilized for abstractive news summarization, focusing on generating coherent and human-like summaries of Norwegian news articles. Additionally, the dataset is employed to assess the effectiveness of instruction-tuned language models in grammatical error correction and the interpretation and generation of Norwegian idioms. These applications highlight the dataset's versatility in natural language processing tasks, particularly in multilingual settings and specific linguistic challenges.	
Norwegian Corpus	citing_context	The Talk of Norway dataset	https://doi.org/10.1038/s41597-024-04142-x (2025)	https://doi.org/10.2307/j.ctv177tghd.12 (2020)	The Talk of Norway dataset is used to analyze the participation and speech length of government MPs in parliamentary debates from 1998 to 2016. Researchers employ quantitative methods to examine patterns and trends in MP contributions, focusing on how frequently and extensively they participate in debates. This dataset enables detailed longitudinal analysis of parliamentary discourse, providing insights into legislative engagement and political representation.	
Norwegian Corpus	cited_context	UD-Norwegian	https://www.semanticscholar.org/paper/0322f23d07085e474254f1a40962a72ccfbd4b47 (2019)	https://www.semanticscholar.org/paper/e89a5d90c5249cf8c6e46e2c86f32034c54fea7a (2019)	The UD-Norwegian dataset is used to train and evaluate models for named entity recognition in Norwegian, focusing on syntactic and morphological annotations for both Bokmål and Nynorsk varieties. It maintains an 80-10-10 split for training, development, and testing, ensuring genre balance and preserving contiguous texts. The dataset emphasizes linguistic consistency and standardization, mirroring the original Norwegian Dependency Treebank (NDT) and its UD-version.	
Norwegian Corpus	cited_context	UD_Norwegian-Bokmaal	https://doi.org/10.18653/v1/W18-6017 (2018)	https://www.semanticscholar.org/paper/87b924e2134f6782acc59ab567f6bf87cb5e5d9f (2016)	The UD_Norwegian-Bokmaal dataset is used to provide syntactic annotations for Bokmål, a written standard of the Norwegian language, supporting linguistic research and NLP applications. It is also utilized to train delexicalized models, focusing on syntactic patterns and structures in written Bokmål and Nynorsk. This dataset enables researchers to analyze and model syntactic features, enhancing understanding and processing of Norwegian languages.	
Norwegian Corpus	cited_context	UD treebanks	https://www.semanticscholar.org/paper/87b924e2134f6782acc59ab567f6bf87cb5e5d9f (2016)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The UD treebanks dataset provides a multilingual treebank collection, including Norwegian, which is used for syntactic and morphological annotation. This dataset supports cross-linguistic and computational linguistic research by enabling the analysis of grammatical structures across languages. Researchers use it to address questions related to linguistic diversity and computational models of language.	
Norwegian Corpus	citing_context	Universal Dependencies datasets	https://doi.org/10.48550/arXiv.2304.00906 (2023)	https://www.semanticscholar.org/paper/daddb6421bbff147afb19d2d705727aadf9cd2c6 (2006)	The Universal Dependencies datasets are used to create the ScaLA datasets, which focus on linguistic and typological variations and annotations for Danish, Norwegian, Swedish, Icelandic, and Faroese. These datasets enable researchers to study and compare grammatical structures and linguistic features across these Nordic languages, facilitating cross-linguistic research and the development of language-specific resources.	
Norwegian Corpus	cited_context	Universal Dependencies (UD)	https://www.semanticscholar.org/paper/87b924e2134f6782acc59ab567f6bf87cb5e5d9f (2016)	https://www.semanticscholar.org/paper/16b26c3bfb3feef8f2d1889b4e76c15e3f5b6fb7 (2014)	The Universal Dependencies (UD) dataset is used to create cross-linguistically consistent syntactic annotations, facilitating cross-linguistic analysis and annotation efforts. Researchers employ this dataset to ensure uniformity in syntactic structures across languages, enhancing comparability and consistency in linguistic studies. This approach supports the development of robust, language-agnostic models and tools, enabling more generalized insights into syntactic phenomena.	
Norwegian Corpus	cited_context | citing_context	Universal Dependencies v1	https://doi.org/10.48550/arXiv.2305.03880 (2023)	https://www.semanticscholar.org/paper/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46 (2016)	The Universal Dependencies v1 dataset is used to predict universal part-of-speech tags, universal features, lemmas, and dependency trees across multiple languages. This dataset enables researchers to develop and evaluate models that can handle linguistic structures consistently across different languages, facilitating cross-lingual research and improving the accuracy of natural language processing tasks.; The Universal Dependencies v1 dataset is used to predict universal part-of-speech tags, universal features, lemmas, and dependency trees across multiple languages. It supports cross-linguistic research by providing a consistent framework for syntactic annotation, enabling comparative studies and the development of multilingual natural language processing models. While it includes Norwegian, specific Norwegian applications are not detailed.	
Norwegian Corpus	citing_context	xSID	https://doi.org/10.48550/arXiv.2412.10095 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.197 (2021)	The xSID dataset is used to develop a Norwegian translation dataset, specifically for translating phrases into four Norwegian dialects. It is employed in zero-shot spoken language understanding tasks, where the dataset facilitates the creation of development and test sets to evaluate model performance across different dialects without prior exposure. This enables researchers to assess and improve the robustness of speech recognition systems in handling diverse linguistic variations.	
Norwegian Corpus	citing_context	xSID 0.6	https://doi.org/10.48550/arXiv.2501.03870 (2025)	https://www.semanticscholar.org/paper/15c10b24ef645d83ff4059affd86945c33e00328 (2018)	The xSID 0.6 dataset is used to enhance cross-lingual research in Norwegian language processing by extending the dataset with Norwegian data. It is also utilized to train and evaluate spoken language understanding systems, incorporating re-annotated versions of two SID datasets to improve accuracy and coverage. This dataset supports research in improving the performance and robustness of language processing models, particularly in Norwegian and cross-lingual contexts.	
Sadri Corpus	citing_context	Sadri	https://doi.org/10.1109/AISP61396.2024.10475269 (2024)	https://doi.org/10.1109/IRANIANMVIP.2011.6121550 (2011)	The Sadri dataset is used to evaluate methods for Farsi word-level recognition, particularly in clustering word images in offline handwritten systems. This involves analyzing the effectiveness of the proposed methodologies in recognizing and categorizing handwritten Farsi words, leveraging the dataset's specific characteristics to enhance the accuracy and reliability of recognition algorithms.	
Serbian Corpus	citing_context	1984	https://www.semanticscholar.org/paper/0f14ab624404397cf3c6cee2ce6b8fd044b08c6c (2004)	https://www.semanticscholar.org/paper/f9ecbf367e6ff38b514b0f888b57a7edc0b1c160 (2000)	The '1984' dataset is used for evaluating and testing morphosyntactic taggers and lemmatisers. Researchers compare the performance of different models and tagsets using this dataset, which helps in assessing the accuracy and effectiveness of these tools in processing Serbian language data. This dataset enables detailed performance comparisons and model optimization.	
Serbian Corpus	citing_context	55 literary metaphors drawn from Serbian poetry	https://doi.org/10.1080/10926488.2024.2380348 (2023)		The dataset of 55 literary metaphors drawn from Serbian poetry is used to norm and evaluate the linguistic and cognitive properties of metaphors in Serbian poetry. Researchers employ this dataset to explore how these metaphors are processed, focusing on their linguistic structure and cognitive impact. This enables a deeper understanding of metaphorical language in Serbian literature.	
Serbian Corpus	citing_context	AbCoSER	https://www.semanticscholar.org/paper/0eec2a32167fffc2caa564665c4b066d39c0a573 (2024), https://www.semanticscholar.org/paper/a2d4ca551ec9b1e1f9d6fbec0711f6a01b786662	https://www.semanticscholar.org/paper/8de7553a7d7f996ada5b8c97c3654f611d040a79 (2021)	The AbCoSER dataset is primarily used for researching and developing methods to detect and analyze abusive speech in the Serbian language, particularly in online and social media contexts. It includes annotated tweets, which are used to train and evaluate models for identifying abusive content. The dataset supports feature selection experiments, focusing on tweet characteristics like length and hate_lex scores. It also aids in enhancing lexicons for abusive language and fine-tuning transformer models for better classification performance.	
Serbian Corpus	citing_context	abusive speech lexicon that includes MWEs	https://www.semanticscholar.org/paper/0eec2a32167fffc2caa564665c4b066d39c0a573 (2024)	https://www.semanticscholar.org/paper/e2959cb4bad9b6f4c3b8657bdaee731f60959391 (2020)	The abusive speech lexicon that includes MWEs is used to enhance a hybrid classification system for detecting abusive speech in Serbian. This dataset focuses on multi-word expressions to improve the robustness of the classification system, addressing the specific challenge of identifying complex abusive phrases. The inclusion of MWEs enables more accurate and nuanced detection of abusive content, thereby improving the overall performance of the system in real-world applications.	
Serbian Corpus	citing_context	BFþ2–70	https://doi.org/10.1080/00223891.2017.1370426 (2019)		The BFþ2–70 dataset is used in psycholexical studies of the Serbian language to measure seven dimensions, employing Tellegen and Waller’s nonrestrictive methodology. It explores lexical structures and semantic spaces, providing insights into the organization and relationships within the Serbian lexicon. This dataset enables researchers to analyze and understand the nuanced semantic properties and distributions of words in the Serbian language.	
Serbian Corpus	citing_context	CLASSLA web corpus	https://doi.org/10.18653/v1/2023.bsnlp-1.7 (2023)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The CLASSLA web corpus is used to train models on Bosnian, Croatian, Montenegrin, and Serbian web content, primarily for cross-lingual representation learning. It serves as a large-scale multilingual corpus for unsupervised cross-lingual representation learning, facilitating the development of language models for South Slavic languages. This dataset enables researchers to analyze Croatian literature and newspapers, enhancing the understanding and processing of these languages.	
Serbian Corpus	citing_context	CLSS.news.sr	https://www.semanticscholar.org/paper/e699ba7f51e7d26ece9b5f2e99571a034da5bca7 (2022)	https://doi.org/10.18653/v1/S16-1081 (2016)	The CLSS.news.sr dataset is used to analyze and evaluate semantic textual similarity (STS) in Serbian, particularly in newswire texts. It is employed to compare the Serbian dataset with English counterparts, focusing on phrase-sentence and sentence-paragraph pairs. The dataset supports research on fine-grained similarity annotations, annotation consistency, and the construction of a cross-level semantic similarity corpus. It enables the exploration of relationships between programming and natural language, and provides a statistical overview of the distribution of text pairs across similarity scores.	
Serbian Corpus	citing_context	COPA-SR	https://doi.org/10.18653/v1/2024.vardial-1.7 (2024)	https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6 (2019)	The COPA-SR dataset is used to evaluate and develop models for various Serbian dialects, including Cerkno, Torlak, and Chakavian, focusing on language processing tasks and benchmark measurements. It is also used to translate and adapt the XCOPA dataset for Serbian, adhering to established methodologies. Additionally, it benchmarks large language models on causal reasoning tasks in Serbian, comparing linguistic features and corpus sizes with other Slavic languages.	
Serbian Corpus	cited_context	Croatian gold standard sentiment lexicon	https://doi.org/10.48550/arXiv.2206.00929 (2022)	https://www.semanticscholar.org/paper/02a031748050c1754418f7beae15de29a5719835 (2016)	The Croatian gold standard sentiment lexicon is translated into Serbian using a rule-based translator and combined with the original lexicon to extract unique entries with a single sentiment affinity. This process generates seed words for sentiment analysis, enabling researchers to enhance the accuracy and coverage of sentiment lexicons in Serbian. The dataset's unique entries and sentiment affinities are crucial for developing robust sentiment analysis tools.	
Serbian Corpus	cited_context	DSL corpus	https://www.semanticscholar.org/paper/74782d315d5cd472bef874ffbca589cc2285a99f (2016)	https://www.semanticscholar.org/paper/c24865d7f34a616d4b5f6f98145900962e59411b (2016)	The DSL corpus is used for subtask 1, which likely involves linguistic data analysis. Specific usage details are limited, but the dataset's role in this subtask suggests it supports research methodologies focused on analyzing linguistic patterns or structures. The dataset's characteristics relevant to this task are not explicitly detailed, but its use indicates it facilitates linguistic studies, particularly in the context of subtask 1.	
Serbian Corpus	cited_context	DSL Corpus Collection	https://www.semanticscholar.org/paper/74782d315d5cd472bef874ffbca589cc2285a99f (2016)	https://www.semanticscholar.org/paper/0af1f6d083f1b7bc7632e857ea39f43b88ccee12 (2014)	The DSL Corpus Collection is used to compile comparable data sources for discriminating between similar languages, specifically focusing on the first edition of the DSL task. This involves methodologies that compare linguistic features to distinguish closely related languages, enabling research in language discrimination and classification.	
Serbian Corpus	citing_context	EXAMS	https://doi.org/10.1109/ICEST58410.2023.10187370 (2023)	https://doi.org/10.1016/j.procs.2021.08.108 (2021)	The EXAMS dataset is used to provide a question-answering (QA) resource for the Serbian language, containing nearly 1600 questions across 14 subjects. It highlights the need for additional Serbian QA corpora, enabling researchers to develop and evaluate QA systems specifically tailored for the Serbian language. This dataset supports the creation and improvement of language-specific NLP models and tools.	
Serbian Corpus	citing_context	FRANK dataset	https://www.semanticscholar.org/paper/0eec2a32167fffc2caa564665c4b066d39c0a573 (2024)	https://doi.org/10.24867/16be39cvejic (2022)	The FRANK dataset is used to fine-tune and evaluate BERTić and RoBERTa-based models for detecting hate speech, particularly targeting LGBT and migrant groups in Croatian. It also supports multilingual model training and evaluation, focusing on Serbian and related regional languages. The dataset provides labeled text data, curated by the Language Resources and Technologies Society Jerteh, enabling robust model performance assessment and enhancing language modeling capabilities.	
Serbian Corpus	citing_context	It-Sr-NER corpus	https://doi.org/10.15439/2024F8827 (2024)	https://www.semanticscholar.org/paper/4fdb15555732769af1d73d10f692fcbe2e0e4bc2 (2013)	The It-Sr-NER corpus is used for named entity recognition (NER) in Italian and Serbian texts, particularly to annotate and analyze sentences from various novels. It is employed to train and evaluate NER and geoparsing models, focusing on entity recognition and location tagging in literary works. The dataset includes 10,000 aligned Italian-Serbian sentence segments, enabling researchers to enhance cross-lingual NER capabilities and assess system performance.	
Serbian Corpus	citing_context	Jerteh-81	https://www.semanticscholar.org/paper/0eec2a32167fffc2caa564665c4b066d39c0a573 (2024)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The Jerteh-81 dataset is used to fine-tune BERTić and RoBERTa-base models for detecting hate speech, particularly targeting LGBT and migrant groups, in Croatian. It provides labeled data for model evaluation and training, leveraging corpuses curated by the Language Resources and Technologies Society Jerteh. This dataset enables researchers to improve the accuracy and robustness of hate speech detection models in the Croatian language.	
Serbian Corpus	citing_context	kakvfilm	https://doi.org/10.1109/ICEST58410.2023.10187370 (2023)	https://doi.org/10.1371/journal.pone.0242050 (2020)	The 'kakvfilm' dataset is used to collect and analyze approximately 3500 movie comments, primarily focusing on sentiment articulation and annotation. This resource-limited dataset supports research in sentiment analysis, enabling the study of how sentiments are expressed and annotated in online movie reviews. The dataset's size and focus on movie comments make it suitable for exploring sentiment in a specific context, facilitating the development and evaluation of sentiment analysis models in a constrained data environment.	
Serbian Corpus	cited_context | citing_context	LeXimirka	https://www.semanticscholar.org/paper/3372b740359499a2b946f8132308b55813599749 (2024), https://www.semanticscholar.org/paper/e2959cb4bad9b6f4c3b8657bdaee731f60959391 (2020)	https://www.semanticscholar.org/paper/e42022a4ec0aec91dc917c0dd86b1cfc9a4431d7	The LeXimirka dataset is used for Serbian language research, primarily to extract lexical entries and morphological data. It supports tagger training tasks and lexicographic data development through hand-crafted rules and web application functionalities. The dataset provides morphological and lexical information, enabling researchers to develop and maintain linguistic resources and explore lexicographic data in Serbian.; The LeXimirka dataset is used to support tagger training tasks for the Serbian language, featuring export functions tailored for this purpose. It enables researchers to develop and refine tagging models, enhancing the accuracy of part-of-speech tagging and other linguistic annotations in Serbian text processing.	
Serbian Corpus	cited_context	morphological electronic dictionaries of Serbian (SMD)	https://www.semanticscholar.org/paper/e42022a4ec0aec91dc917c0dd86b1cfc9a4431d7	https://www.semanticscholar.org/paper/44c4d264a4eff69204a6dd7c939c56c4f2278f48 (2006)	The morphological electronic dictionaries of Serbian (SMD) dataset is used for developing and managing Serbian language dictionaries, with a focus on morphological aspects and electronic representation. It enhances lexical resources and dictionary functionalities, supporting the creation and improvement of morphological data. This dataset enables researchers to refine and expand the capabilities of Serbian language dictionaries, ensuring they are more comprehensive and user-friendly.	
Serbian Corpus	cited_context	MULTEX-East lexicons	https://www.semanticscholar.org/paper/e42022a4ec0aec91dc917c0dd86b1cfc9a4431d7	https://www.semanticscholar.org/paper/67b28714fd303391b7d1276a666fe54ee1d991fe (2007)	The MULTEX-East lexicons are used to enhance lexical resources for Serbian NLP, focusing on morpho-semantic relations. They provide critical support for converting Serbian lexicons into lexical databases, improving lexical coverage, and developing morphological data. These enhancements are crucial for advancing dictionary functionalities and lexical modeling in Slavic languages, particularly Serbian.	
Serbian Corpus	cited_context | citing_context	MULTEXT-East	https://www.semanticscholar.org/paper/0f14ab624404397cf3c6cee2ce6b8fd044b08c6c (2004), https://www.semanticscholar.org/paper/cf1f6226953487c60b58a1451dd054d02bbe78ac (2004), https://doi.org/10.18653/v1/W19-3704 (2019)	https://www.semanticscholar.org/paper/eb1fa531e67c2038cecac89e94b51448fa05c800 (2010), https://doi.org/10.1007/s10579-011-9174-8 (2011)	The MULTEXT-East dataset is used to develop and standardize language resources for Central and Eastern European languages, including Serbian. It integrates dictionary and corpus data to create multilingual morphosyntactic specifications, lexicons, and corpora. This dataset supports the creation of parallel and comparable corpora and multilingual text tools, enhancing lexical resource development for Slavic languages.; The MULTEXT-East dataset is used to define morphosyntactic descriptions for Central and Eastern European languages, including Serbian. It focuses on part-of-speech tagging and feature-value pairs, enabling researchers to standardize linguistic annotations. This standardization facilitates comparative linguistic studies and supports the development of language processing tools.	
Serbian Corpus	citing_context	dataset of articles of 3 authors in Serbian	https://doi.org/10.1109/INFOTEH57020.2023.10094205 (2023)	https://www.semanticscholar.org/paper/f09c903e338680e70265569838fec9d437c6711c (2011)	The dataset of articles by 3 authors in Serbian is used to classify authorship, specifically focusing on the accuracy of the CNG distance method. This method is employed to distinguish between the authors, enabling research into authorship attribution techniques and their effectiveness in Serbian texts.	
Serbian Corpus	citing_context	dataset of articles of 5 authors	https://doi.org/10.1109/INFOTEH57020.2023.10094205 (2023)	https://www.semanticscholar.org/paper/78d3cab50da1c9f3b7448454f5b5b50c1eed3fc2 (2012)	The dataset of articles by 5 authors is used to evaluate n-gram and syllable-based profiles for authorship attribution in Serbian. Specifically, it focuses on using CNG distance with character n-grams, achieving 96% accuracy. This dataset enables researchers to test and refine computational methods for identifying authors based on textual features, contributing to the field of digital forensics and natural language processing.	
Serbian Corpus	cited_context | citing_context	corpus of tweets	https://doi.org/10.1371/journal.pone.0242050 (2020)	https://doi.org/10.2298/CSIS180122013L (2019)	The corpus of tweets is used to evaluate methods for handling negation in Serbian sentiment analysis, specifically classifying tweets into positive, negative, and neutral categories. This private dataset supports research in improving sentiment analysis accuracy by addressing linguistic challenges such as negation.; The corpus of tweets is used to evaluate methods for handling negation in Serbian sentiment analysis, specifically classifying tweets into positive, negative, and neutral categories. This private dataset supports research in improving sentiment analysis techniques by addressing the complexities of negation in the Serbian language.	
Serbian Corpus	cited_context	Ontology of Rhetorical Figures for Serbian	https://www.semanticscholar.org/paper/3559d220b582ab2b9238b17b9a9808f74de29266 (2020)	https://doi.org/10.1007/978-3-642-40585-3_49 (2013)	The Ontology of Rhetorical Figures for Serbian is used to model and detect rhetorical figures in the Serbian language, particularly focusing on their role in abusive language. This dataset provides a structured representation of rhetorical devices, enabling researchers to analyze and understand the nuanced use of language in abusive contexts. The ontology facilitates the identification and classification of rhetorical figures, supporting research into the linguistic mechanisms of abuse.	
Serbian Corpus	cited_context	OSEVAL	https://www.semanticscholar.org/paper/74782d315d5cd472bef874ffbca589cc2285a99f (2016)	https://www.semanticscholar.org/paper/36e72b54d06adb8bec6b7d25a0f8b4c4ec74b7e5 (2015)	The OSEVAL dataset is used to evaluate systems designed to discriminate between similar languages. Research focuses on performance metrics and comparative analysis of various approaches. This dataset enables researchers to assess the effectiveness of different methodologies in distinguishing closely related languages, providing insights into system accuracy and reliability.	
Serbian Corpus	citing_context	paraphrase.sr	https://www.semanticscholar.org/paper/e699ba7f51e7d26ece9b5f2e99571a034da5bca7 (2022)	https://doi.org/10.1109/TELFOR.2011.6143778 (2011)	The 'paraphrase.sr' dataset is used to train and evaluate paraphrase identification models, specifically focusing on binary similarity judgments in Serbian newswire texts. This dataset enables researchers to develop and test algorithms that can accurately identify whether two sentences are paraphrases of each other, enhancing natural language processing capabilities in the Serbian language.	
Serbian Corpus	citing_context	ParlaSent-BCS	https://doi.org/10.1109/ICEST58410.2023.10187370 (2023)	https://doi.org/10.48550/arXiv.2206.00929 (2022)	The ParlaSent-BCS dataset is used to analyze sentiment in parliamentary proceedings from Bosnia-Herzegovina, Croatia, and Serbia. It employs a 6-level sentiment schema for annotation, enabling researchers to explore nuanced emotional content in political discourse. This dataset facilitates detailed sentiment analysis, providing insights into the emotional dynamics of legislative discussions across these regions.	
Serbian Corpus	citing_context	SemEval2019	https://www.semanticscholar.org/paper/0eec2a32167fffc2caa564665c4b066d39c0a573 (2024)	https://doi.org/10.18653/v1/S19-2010 (2019)	The SemEval2019 dataset is used to evaluate offensive language detection in social media, serving as a multi-lingual benchmark for comparing performance across different languages. It focuses on categorizing and identifying offensive content, employing a standardized evaluation methodology to maintain consistency. This dataset enables researchers to extend their benchmarks to multiple languages, facilitating cross-lingual comparisons and enhancing the robustness of offensive language identification models.	
Serbian Corpus	cited_context | citing_context	sentiment analysis dataset in Serbian	https://doi.org/10.1371/journal.pone.0242050 (2020)	https://doi.org/10.1109/TELFOR.2016.7818923 (2016)	The sentiment analysis dataset in Serbian is used to evaluate morphological normalization methods, focusing on document-level movie reviews categorized as positive, negative, or neutral. This dataset enables researchers to assess the effectiveness of normalization techniques in processing and classifying Serbian text, contributing to improvements in natural language processing for the Serbian language.; The sentiment analysis dataset in Serbian is used to evaluate morphological normalization methods, specifically focusing on document-level movie reviews categorized as positive, negative, or neutral. This dataset enables researchers to assess the effectiveness of normalization techniques in improving sentiment classification accuracy for Serbian text.	
Serbian Corpus	citing_context	Serbian ELTeC Collection	https://doi.org/10.15439/2024F8827 (2024)	https://doi.org/10.18485/infotheca.2021.21.2.3 (2021)	The Serbian ELTeC Collection is used to train models on Serbian novels published between 1840 and 1920, specifically for named entity recognition (NER) tasks. The dataset employs sentences generated from Wikidata and Leximirka, enabling researchers to develop and evaluate NER models tailored to historical Serbian literature. This approach addresses the challenge of recognizing named entities in a culturally and temporally specific context, enhancing the accuracy of NER in historical texts.	
Serbian Corpus	cited_context	Serbian Movie Review Dataset	https://doi.org/10.1109/TELFOR.2016.7818923 (2016)	https://doi.org/10.1007/978-3-540-24775-3_3 (2004)	The Serbian Movie Review Dataset is used to establish reliable baselines for sentiment analysis in Serbian, focusing on the replicability of significance tests for comparing learning algorithms. This dataset enables researchers to evaluate and compare the performance of different machine learning models in sentiment analysis tasks, ensuring that results are consistent and reproducible.	
Serbian Corpus	cited_context	Serbian Movie Review Dataset – SerbMR1	https://doi.org/10.1109/TELFOR.2016.7818923 (2016)	https://www.semanticscholar.org/paper/88c3ce3f22185d1eff28cadbf000517ba7b910ce (2016)	The Serbian Movie Review Dataset – SerbMR1 is used for sentiment analysis in Serbian, focusing on establishing reliable baselines and ensuring the replicability of significance tests for comparing learning algorithms. It serves as a resource-limited language dataset, enabling researchers to train and evaluate models on movie reviews, thereby addressing the challenges of working with less extensively studied languages.	
Serbian Corpus	citing_context	Serbian native readers	https://doi.org/10.1109/ACCESS.2023.3234438 (2023)	https://doi.org/10.1371/journal.pone.0165508 (2016)	The 'Serbian native readers' dataset is used to study reading behavior by analyzing eye movements and reading patterns using a remote eye-tracker. Research focuses on how different color configurations affect these behaviors. This dataset enables researchers to explore specific aspects of visual processing and cognitive engagement during reading, providing insights into the mechanics of reading among native Serbian speakers.	
Serbian Corpus	cited_context	Serbian WordNet	https://doi.org/10.2298/CSIS180122013L (2019)	https://www.semanticscholar.org/paper/526c3be44bb8c4957950f8efa25eb03e0355d9c8 (2004)	The Serbian WordNet dataset is used to expand the dictionary of Serbian synonyms, thereby enhancing the morphological dictionary. This expansion supports linguistic research and natural language processing tasks by providing a richer resource for synonym identification and morphological analysis. The dataset's comprehensive list of synonyms aids in improving the accuracy and breadth of linguistic tools and applications.	
Serbian Corpus	cited_context	Serbian Wordnet (SWN)	https://www.semanticscholar.org/paper/526c3be44bb8c4957950f8efa25eb03e0355d9c8 (2004)	https://www.semanticscholar.org/paper/8fdc3e53c00edc921af93a10d87c2ec44425fe0e (2001)	The Serbian Wordnet (SWN) dataset is used to develop and standardize lexical resources, particularly in the context of multilingual databases. It contributes to the BalkaNet project by creating a semantic network for Serbian, focusing on lexical relations and semantic structures. This dataset facilitates the comparison and integration of wordnets across Balkan languages, enhancing cross-linguistic research and resource standardization.	
Serbian Corpus	cited_context | citing_context	SerbMR	https://www.semanticscholar.org/paper/88c3ce3f22185d1eff28cadbf000517ba7b910ce (2016)	https://doi.org/10.3115/1218955.1218990 (2004), https://doi.org/10.3115/1118693.1118704 (2002)	The SerbMR dataset is used as a benchmark for comparing sentiment analysis performance, particularly in relation to English datasets. It helps researchers balance the dataset to ensure fair comparisons, providing a baseline for evaluating sentiment analysis models across languages. This enables the assessment of model performance and the identification of language-specific challenges in sentiment analysis.; The SerbMR dataset is used to compare sentiment analysis performance between Serbian and English datasets, with a focus on balancing the dataset to improve Serbian language processing. This involves methodologies aimed at ensuring the dataset's representativeness, enabling researchers to evaluate and enhance sentiment analysis models specifically for Serbian.	
Serbian Corpus	cited_context	SerbMR-2C	https://doi.org/10.1109/TELFOR.2016.7818923 (2016)	https://doi.org/10.1145/1656274.1656278 (2009)	The SerbMR-2C dataset is used to evaluate sentiment classification models, specifically focusing on binary sentiment analysis in Serbian. Researchers employ a bag-of-words/n-grams approach to analyze and classify sentiments. This dataset enables the assessment of model performance in understanding and categorizing positive and negative sentiments within Serbian text, providing insights into the effectiveness of different classification techniques.	
Serbian Corpus	citing_context	sr_HurtLex	https://www.semanticscholar.org/paper/a2d4ca551ec9b1e1f9d6fbec0711f6a01b786662	https://doi.org/10.4000/BOOKS.AACCADEMIA.3085 (2018)	The sr_HurtLex dataset is used to study derogatory and offensive language in Serbian, focusing on the prevalence and impact of hurtful words. Researchers employ the HurtLex lexicon to identify and categorize such language, enabling analysis of its usage and effects in the Serbian context. This dataset facilitates the examination of linguistic patterns and social impacts of offensive language.	
Serbian Corpus	cited_context	srLex	https://doi.org/10.18653/v1/W19-3704 (2019)	https://www.semanticscholar.org/paper/1daaa093152b8d87b80f02071bab01abc7fbaeb0 (2018)	The srLex dataset is used in Serbian language research for training models to distinguish proper nouns from foreign residuals, analyzing Serbian morphology, and developing word2vec and fastText embeddings. It contains over 100 thousand lemmas and 3 million inflected forms, enabling detailed linguistic analysis and model training. The dataset's size, notably 87 thousand tokens, is compared with Slovenian and Croatian datasets to highlight its smaller scale, which impacts model performance and training efficiency.	
Serbian Corpus	citing_context	srpELTeC-gold-extended	https://doi.org/10.15439/2024F8827 (2024)	https://doi.org/10.26615/978-954-452-072-4_141 (2021)	The srpELTeC-gold-extended dataset is used to train the SrpCNNER2 model for named entity recognition in Serbian. It combines gold-standard annotated texts with newspaper articles and generated sentences, enhancing the model's ability to accurately identify entities in diverse textual contexts. This dataset enables researchers to improve the performance and robustness of named entity recognition systems specifically tailored for the Serbian language.	
Serbian Corpus	citing_context	SrpKor	https://www.semanticscholar.org/paper/978fde16faf34aca7b3172827cda7f9b8c24c76e (2015)	https://www.semanticscholar.org/paper/f6d367c5a6ca3b7cb516f98b5d9210c1c2f47534 (2005)	The SrpKor dataset is used to develop and refine Serbian language resources, particularly focusing on the interactions between text and dictionaries. This dataset supports research projects by enabling the creation and enhancement of linguistic tools and resources within a defined project timeline. It facilitates the analysis and integration of textual data with dictionary entries, enhancing the accuracy and utility of Serbian language resources.	
Serbian Corpus	cited_context | citing_context	SrpMD	https://www.semanticscholar.org/paper/418de221eae91c6e215c17e72a98c55fb5970ae6 (2016), https://www.semanticscholar.org/paper/3559d220b582ab2b9238b17b9a9808f74de29266 (2020)	https://www.semanticscholar.org/paper/b4bff89ec40e64e63df57a848ac5e83e30a474e6 (2016)	The SrpMD dataset serves as a comprehensive lexical resource for Serbian, containing 85,721 lemmas. It is indexed on Meta-Share but not available for download, primarily supporting Serbian language research by providing a rich, accessible dictionary. This dataset enables researchers to explore lexical structures and linguistic features, enhancing studies in Serbian linguistics and lexicography.; The SrpMD dataset is used to construct lemmas for multi-word expressions (MWEs) in Serbian, facilitating the semi-automatic creation of new MWE nominal entries. It serves as a comprehensive lexical resource containing 85,721 lemmas, published under a non-commercial license and indexed on Meta-Share. This dataset supports linguistic research by providing a robust foundation for the development and expansion of Serbian lexical resources.	
Serbian Corpus	citing_context	SrpMD-DELA	https://www.semanticscholar.org/paper/978fde16faf34aca7b3172827cda7f9b8c24c76e (2015)		The SrpMD-DELA dataset is primarily used as a lexical resource for morphological analysis and training Part-of-Speech (PoS) taggers for the Serbian language. It enhances linguistic processing capabilities and improves the Serbian language corpus, particularly within the EU-funded CESAR project. The dataset focuses on the accuracy and coverage of lexical entries, contributing to the development and enhancement of linguistic resources and tools for Serbian.	
Serbian Corpus	cited_context | citing_context	srWac	https://doi.org/10.1371/journal.pone.0242050 (2020)	https://doi.org/10.3115/v1/W14-0405 (2014)	The srWac dataset is used to train word embeddings on the largest publicly available corpus of Serbian texts, comprising 555 million tokens. This methodology leverages the extensive size and linguistic richness of the dataset to generate high-quality embeddings, which are essential for various natural language processing tasks and research questions in the Serbian language.; The srWac dataset is used to train word embeddings, leveraging its status as the largest publicly available corpus of Serbian texts, which contains 555 million tokens. This dataset enables researchers to develop high-quality linguistic models specifically tailored for the Serbian language, enhancing natural language processing tasks and applications in Serbian.	
Serbian Corpus	citing_context	Stanković et al. (2020)	https://www.semanticscholar.org/paper/a2d4ca551ec9b1e1f9d6fbec0711f6a01b786662	https://doi.org/10.1007/978-3-642-13059-5_5 (2010)	The Stanković et al. (2020) dataset is primarily used to detect and classify offensive language in Serbian, including curses, insults, and threatening content. It employs dictionary-based methods and lexicon support for multi-level classification, enabling researchers to identify and categorize abusive speech in social media and online environments. This dataset facilitates the study of derogatory and hurtful language, enhancing the detection and analysis of abusive terms in digital communication.	
Sicilian Corpus	citing_context	Mparamu lu sicilianu	https://www.semanticscholar.org/paper/318e3ced09aa6ce5fc6e457768e270a9714eb518 (2021)		The 'Mparamu lu sicilianu' dataset is used to translate homework exercises into Sicilian, English, and Italian, facilitating linguistic analysis and educational applications. It provides authentic and structured language samples, enhancing research on translation accuracy and language bridging. The dataset supports studies in linguistic content and grammatical structures, aiding both academic and pedagogical research.	
Slovak Corpus	cited_context | citing_context	Aranea Web Corpus	https://doi.org/10.1109/ACCESS.2023.3262308 (2023)	https://www.semanticscholar.org/paper/96f95b9f0a3a0b616e2551c9272a903cedf5db39 (2016)	The Aranea Web Corpus is utilized as a large web-based resource to support linguistic studies and natural language processing tasks, specifically for the Slovak language. It provides a dependency corpus with marked morphological markers and lemmas, enabling detailed linguistic analysis and enhancing NLP tasks by offering richly annotated data.; The Aranea Web Corpus is utilized as a large web-based resource to support linguistic analyses and natural language processing tasks in the Slovak language. It provides a dependency corpus with marked morphological markers and lemmas, enabling detailed syntactic and morphological analysis. This dataset facilitates research by offering richly annotated data, enhancing the accuracy and depth of linguistic studies.	
Slovak Corpus	citing_context	BSNLP-2017 Shared Task	https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.18653/v1/W17-1412 (2017)	The BSNLP-2017 Shared Task dataset is used to annotate Slovak Wikipedia data for named entity recognition (NER), employing the CoNLL-2003 NER tagset and introducing a MISC category for diverse entities. This dataset facilitates research in NER for the Slovak language, enhancing the identification and classification of named entities in text.	
Slovak Corpus	citing_context	C4 corpus	https://www.semanticscholar.org/paper/11f885301c26ad965d9108be2506abdc09c19e47 (2023)	https://doi.org/10.18653/v1/D15-1075 (2015)	The C4 corpus is used as a multilingual text corpus containing text in 101 languages, including Slovak, to train the mT5 model. It is employed without specific task fine-tuning, enabling the development of multilingual natural language processing models capable of handling diverse linguistic data. The large scale and multilingual nature of the dataset facilitate the creation of robust, general-purpose language models.	
Slovak Corpus	citing_context	CES-corpus	https://doi.org/10.55630/dipp.2011.1.16 (2011)	https://www.semanticscholar.org/paper/80d8a20897a577f8acb9020aa9189fef437e8f5d (2000)	The CES-corpus is used to create a well-structured and lemmatized multilingual machine translation evaluation (MTE) corpus, focusing on language-specific resources. It enables researchers to evaluate machine translation systems by providing high-quality, linguistically annotated data. This dataset supports the development and assessment of translation models, particularly in contexts requiring precise linguistic analysis and evaluation.	
Slovak Corpus	cited_context	CLEF 2007 Ad-Hoc Track	https://www.semanticscholar.org/paper/bf93b516b881124b395aecd31a22822a4d8b8ea1 (2016)	https://www.semanticscholar.org/paper/388029c41282c353619800678ea375ef5099119b (2016)	The CLEF 2007 Ad-Hoc Track dataset is used to evaluate information retrieval performance, particularly in the context of evaluating a proposal for Czech, a language with properties similar to Slovak. The dataset enables researchers to test and compare the effectiveness of different retrieval methods, focusing on how well they perform in a Slavic language setting.	
Slovak Corpus	citing_context	Demagog	https://doi.org/10.1007/s00521-024-10113-5 (2023)	https://doi.org/10.26615/978-954-452-056-4_113 (2019)	The Demagog dataset is used to analyze claims and their veracity in West Slavic languages, with a focus on statistical analysis of human-annotated rationales and metadata. It includes political affiliations, enabling researchers to explore the relationship between political context and claim veracity. This dataset supports research into the linguistic and contextual factors influencing the spread and perception of misinformation.	
Slovak Corpus	citing_context	EWA-DB	https://doi.org/10.1109/ICASSP49660.2025.10889445 (2025)	https://doi.org/10.1016/J.CSL.2021.101216 (2021)	The EWA-DB dataset is used to evaluate models for detecting neurodegenerative diseases, particularly Parkinson's, from Slovak speech. It employs a dual-head architecture combining self-supervised learning (SSL) and Wavelet-based speech representations, demonstrating superior performance over baseline models. This dataset enables researchers to assess the effectiveness of these advanced techniques in handling speech affected by neurodegenerative conditions.	
Slovak Corpus	citing_context	HPLT project’s 1.2 data release	https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.1007/978-3-030-89579-2_3 (2021)	The HPLT project’s 1.2 data release is used to train and evaluate a monolingual Slovak BERT-base model, specifically leveraging 33.4GB of filtered Slovak Common Crawl data. This dataset supports text classification tasks by providing a large corpus for pre-training, enabling researchers to develop and assess the performance of models tailored to the Slovak language.	
Slovak Corpus	citing_context	MADLAD-400	https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.48550/arXiv.2309.04662 (2023)	The MADLAD-400 dataset is used to evaluate translation systems, particularly for document-level translations into Slovak. It provides multilingual and audited text, enabling researchers to assess the performance of these systems. The dataset's audited content ensures reliable evaluation, making it valuable for improving and benchmarking translation technologies.	
Slovak Corpus	citing_context	mC4	https://www.semanticscholar.org/paper/11f885301c26ad965d9108be2506abdc09c19e47 (2023)	https://www.semanticscholar.org/paper/645a317c9305207e95d03b5756a65e7e850f32d5 (2022)	The mC4 dataset is used to extract and preprocess Slovak parts for a multilingual document-oriented corpus. This involves cleaning and preparing the data for language-specific applications, ensuring high-quality content for subsequent linguistic analysis and processing tasks.	
Slovak Corpus	cited_context	morphological database of the Slovak language	https://doi.org/10.15398/jlm.v0i1.35 (2012)	https://www.semanticscholar.org/paper/4254604b232bc275af17dc1278742a0d35c79f12 (2006)	The morphological database of the Slovak language is used to store and analyze morphological information, encompassing over 97,000 lemmas and 3.2 million inflected and tagged entries. Researchers utilize this dataset to explore linguistic patterns and structures, enabling detailed analyses of the Slovak language's morphology. The extensive coverage of inflected forms and tags facilitates robust linguistic research, supporting studies on morphological variation and grammatical structure.	
Slovak Corpus	cited_context	Morphological Database of the Slovak National Corpus	https://doi.org/10.7717/peerj-cs.2465 (2024)		The Morphological Database of the Slovak National Corpus is used to enhance linguistic analysis of Slovak by supplementing root morpheme data, analyzing lemmas, and supporting morphological segmentation. It provides full word paradigms for over 98,000 unique lemmas, facilitating the extraction of word roots and the analysis of Slovak language structures. This dataset enables detailed morphological studies and supports the examination of derived and alternative roots.	
Slovak Corpus	citing_context	MTE multilingual aligned corpus	https://doi.org/10.55630/dipp.2011.1.16 (2011)	https://www.semanticscholar.org/paper/ba97deb7a45dc9746711b3d29592dc7ed03e0b68 (1998)	The MTE multilingual aligned corpus is used to study multilingual alignment, particularly focusing on translations of George Orwell’s '1984' into six languages, aligned with the English original. This dataset enables researchers to analyze translation consistency and linguistic patterns across languages, employing methodologies that compare and align textual data to understand cross-linguistic variations and translation strategies.	
Slovak Corpus	citing_context	MultiLing 2015	https://www.semanticscholar.org/paper/de30ffee9ea4ece248314142518686657bdcf473 (2020)	https://www.semanticscholar.org/paper/6247dd5de4e69effc4816f8c788e0dbb416fdde5 (2018)	The MultiLing 2015 dataset is used for extensive multilingual document processing and analysis, covering 41 languages, including Slovak. It supports research in multilingual document analysis by enabling the examination of linguistic features and processing techniques across a wide range of languages. This dataset facilitates the development and evaluation of multilingual NLP systems and methodologies, enhancing cross-lingual understanding and processing capabilities.	
Slovak Corpus	citing_context	Prague Dependency Treebank	https://doi.org/10.7717/peerj-cs.2026 (2024)	https://www.semanticscholar.org/paper/b3776cb02c6d6b77950eb029e00bba9b9b07bdc1 (2005)	The Prague Dependency Treebank is primarily used for linguistic research and annotation tasks in Slavic languages, with a focus on Czech. It serves as a guideline for annotating texts, ensuring consistency and accuracy by aligning with specific grammatical rules. This dataset supports the development of linguistic resources and facilitates comparative studies across Slavic languages, including adaptations for Slovak text annotation.	
Slovak Corpus	citing_context	Reviews3	https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.18653/v1/W19-3716 (2019)	The Reviews3 dataset is used for sentiment analysis of Slovak customer reviews, where the data are manually labeled as positive, negative, or neutral by two annotators reaching consensus. This dataset enables researchers to evaluate and improve sentiment classification models specifically tailored for the Slovak language, focusing on the accuracy and reliability of sentiment detection in customer feedback.	
Slovak Corpus	cited_context	rmak corpus	https://doi.org/10.15398/jlm.v0i1.35 (2012)		The 'rmak corpus' dataset is mentioned in research citations but lacks detailed descriptions of its usage, methodology, or specific research applications. There is no explicit information on how it is employed in studies or its relevance to particular research questions or characteristics.	
Slovak Corpus	citing_context	SentiSK dataset	https://doi.org/10.1109/RADIOELEKTRONIKA65656.2025.11008427 (2025)	https://doi.org/10.3390/electronics13040703 (2024)	The SentiSK dataset is used for sentiment analysis in the Slovak language, focusing on training and evaluating machine learning models with Slovak text data. This dataset enables researchers to develop and test algorithms that can accurately classify sentiments in Slovak texts, enhancing natural language processing capabilities for this specific language.	
Slovak Corpus	cited_context	SICK	https://doi.org/10.18653/v1/2022.findings-emnlp.530 (2021)	https://doi.org/10.3115/v1/S14-2001 (2014)	The SICK dataset is used to create a Slovak version focusing on semantic relatedness and textual entailment, specifically for evaluating machine translation. It employs methodologies that assess semantic textual similarity, enabling researchers to improve and evaluate the performance of machine translation systems in the Slovak language.	
Slovak Corpus	cited_context | citing_context	SK-QuAD	https://doi.org/10.1109/ACCESS.2023.3262308 (2023), https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.1007/978-3-030-58219-7_1 (2019)	The SK-QuAD dataset is used to train and evaluate multilingual question answering systems specifically for the Slovak language. It focuses on enhancing model performance in understanding and generating answers from Slovak texts. This dataset enables researchers to develop more accurate and contextually relevant question-answering models for Slovak, contributing to advancements in natural language processing for under-resourced languages.; The SK-QuAD dataset is used as a training set for developing and evaluating Slovak question-answering models, specifically focusing on reading comprehension tasks. It enables researchers to assess model performance on Slovak text, ensuring that the models can effectively understand and respond to questions posed in the Slovak language. This dataset is crucial for improving the accuracy and reliability of Slovak language processing systems.	
Slovak Corpus	cited_context | citing_context	Slovak Categorized News Corpus	https://doi.org/10.1109/COGINFOCOM.2014.7020469 (2014), https://www.semanticscholar.org/paper/bf93b516b881124b395aecd31a22822a4d8b8ea1 (2016)	https://www.semanticscholar.org/paper/c0f0e6d426e0ec94df64e3930e207186f1a29ab7 (2014)	The Slovak Categorized News Corpus is used to train and evaluate NLP models, particularly for categorizing and annotating news articles in the Slovak language. It supports preprocessing and analysis of category distributions, enhances named entity recognition, and improves the accuracy and fluency of language models for Slovak dictation systems. This dataset enables researchers to develop and refine NLP tools tailored to the Slovak language, focusing on categorization, annotation, and language modeling.; The Slovak Categorized News Corpus is primarily used for document classification research, specifically for categorizing news articles in the Slovak language. It serves as the document set for analysis, enabling researchers to develop and test classification algorithms tailored to the Slovak language. The dataset's categorized nature facilitates the evaluation of these methods in accurately classifying news articles into predefined categories.	
Slovak Corpus	cited_context | citing_context	Slovak Categorized News Corpus (SCNC)	https://doi.org/10.1109/ACCESS.2023.3262308 (2023)	https://www.semanticscholar.org/paper/bf93b516b881124b395aecd31a22822a4d8b8ea1 (2016)	The Slovak Categorized News Corpus (SCNC) is used to evaluate the performance of information retrieval systems, specifically focusing on categorization accuracy and relevance ranking in Slovak news articles. Researchers employ this dataset to assess how well these systems can classify and rank news content, leveraging its categorized structure to validate the effectiveness of their methodologies.	
Slovak Corpus	citing_context	Slovak Dataset for Multilingual Question Answering	https://doi.org/10.18653/v1/2024.vardial-1.11 (2024)	https://doi.org/10.1109/ACCESS.2023.3262308 (2023)	The Slovak Dataset for Multilingual Question Answering is used to develop and evaluate question answering systems specifically for the Slovak language. Researchers focus on enhancing multilingual capabilities and assessing performance across various linguistic contexts. This dataset enables the testing and improvement of algorithms designed to handle Slovak, contributing to the broader field of multilingual natural language processing.	
Slovak Corpus	citing_context	Slovak Dependency Treebank	https://doi.org/10.1109/ACCESS.2023.3262308 (2023)		The Slovak Dependency Treebank is used to provide a dependency corpus with marked morphological markers and lemmas, supporting linguistic analysis and natural language processing tasks in Slovak. It serves as a large web-based corpus, facilitating various linguistic studies and NLP tasks, particularly for the Slovak language. This dataset enables researchers to conduct detailed analyses of Slovak syntax and morphology, enhancing the development of language-specific NLP tools and models.	
Slovak Corpus	citing_context	Slovak National Corpus	https://doi.org/10.7717/peerj-cs.2026 (2024), https://doi.org/10.1109/COGINFOCOM.2014.7020469 (2014)	https://doi.org/10.1007/978-3-540-30120-2_12 (2004)	The Slovak National Corpus is used primarily for training morphological tagging models such as TreeTagger and MorphoDiTa, enhancing accuracy in Slovak language processing. It is also utilized to train classifiers on trigram counts and features, focusing on linguistic patterns and trigram frequencies to improve language processing tasks. These applications leverage the corpus's extensive linguistic data to refine and optimize natural language processing models for the Slovak language.	
Slovak Corpus	citing_context	Slovak National Corpus (SNC)	https://doi.org/10.7717/peerj-cs.2026 (2024)	https://doi.org/10.1007/978-3-540-30120-2_12 (2004)	The Slovak National Corpus (SNC) is used as a morphologically annotated and lemmatized corpus for linguistic research, focusing on the analysis of the Slovak language. It is utilized to study linguistic phenomena through its two sub-corpora, enabling detailed examination of morphological structures and lemmas. This dataset supports research questions related to Slovak language morphology and lexicography, providing a rich resource for linguistic analysis.	
Slovak Corpus	citing_context	Sound Archive of Slovak Dialects	https://doi.org/10.18653/v1/2024.vardial-1.11 (2024)	https://doi.org/10.1109/DISA.2018.8490639 (2018)	The Sound Archive of Slovak Dialects is used to analyze the distinguishability of Slovak dialects in spoken language, focusing on regional variations and phonetic features. Researchers employ the dataset, which includes approximately 150 hours of recordings, to study how different dialects can be identified based on their unique phonetic characteristics. This analysis helps in understanding the linguistic diversity within Slovakia and supports research into regional linguistic patterns.	
Slovak Corpus	citing_context	SST2-sk	https://www.semanticscholar.org/paper/11f885301c26ad965d9108be2506abdc09c19e47 (2023)	https://doi.org/10.18653/v1/d13-1170 (2013)	The SST2-sk dataset is used for evaluating model performance on sentiment analysis tasks in the Slovak language. Researchers focus on assessing text classification accuracy and the model's ability to handle semantic compositionality. This dataset enables the examination of how well models can classify sentiments in Slovak texts, providing insights into the effectiveness of different approaches in this specific linguistic context.	
Slovak Corpus	citing_context	STSbenchmark	https://doi.org/10.18653/v1/2022.findings-emnlp.530 (2021)	https://doi.org/10.3115/v1/S14-2001 (2014)	The STSbenchmark dataset is used to create a Slovak version for evaluating language models, particularly in semantic textual similarity, relatedness, and textual entailment tasks. It focuses on enhancing machine translation and cross-lingual transfer, enabling researchers to assess model performance across languages. This adaptation supports the development and evaluation of multilingual natural language processing systems.	
Slovak Corpus	citing_context	Twitter account of one of the biggest Slovak journal	https://doi.org/10.48550/arXiv.2304.04026 (2023)	https://doi.org/10.1162/tacl_a_00051 (2016)	The Twitter account dataset of one of the biggest Slovak journals is used to create a dataset with 10,000 NER-annotated tweets and 16,000 entities. This dataset is specifically employed for training a Named Entity Recognition (NER) model using FastText vectors and a BiLSTM neural network architecture. The dataset enables researchers to develop and evaluate NER models tailored for the Slovak language, enhancing the accuracy of entity recognition in social media content.	
Slovak Corpus	cited_context	UD data	https://doi.org/10.18653/v1/2022.findings-emnlp.530 (2021)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The UD data dataset is used to train systems that support UPOS and XPOS tagsets, focusing on cross-lingual representation learning at scale. This involves methodologies that enhance the ability to generalize linguistic features across different languages, enabling more robust and versatile natural language processing models.	
Slovak Corpus	citing_context	Wayback archive	https://www.semanticscholar.org/paper/de30ffee9ea4ece248314142518686657bdcf473 (2020)	https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72 (2015)	The Wayback Archive is used to collect a large corpus of Slovak language content, specifically over 100 thousand articles from SME.sk. This dataset is employed for linguistic analysis and processing, enabling researchers to study Slovak language usage, trends, and patterns in a comprehensive manner. The extensive size and focused content of the dataset facilitate detailed and nuanced research into the Slovak language.	
Slovak Corpus	citing_context	WikiGoldSK	https://doi.org/10.48550/arXiv.2304.04026 (2023)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The WikiGoldSK dataset is used to evaluate NLP toolkits on named entity recognition tasks for the Slovak language. Researchers focus on structured prediction and fine-tuning capabilities, assessing how well these tools can identify and classify entities in Slovak text. This dataset enables the comparison of different NLP models and techniques, contributing to advancements in Slovak language processing.	
Slovak Corpus	citing_context	Wikipedia snapshots	https://doi.org/10.1007/s00521-024-10113-5 (2023)		The Wikipedia snapshots dataset is used as target corpora in research, particularly for CsFEVER and EnFEVER projects, focusing on the Slovak language. It includes both full snapshots and older snapshots limited to leading sections. This dataset enables researchers to analyze and verify claims in the context of fact-checking, leveraging the comprehensive and evolving nature of Wikipedia content.	
Slovak Corpus	citing_context	XNLI	https://doi.org/10.48550/arXiv.2506.21508 (2025)	https://doi.org/10.18653/v1/N18-1101 (2017)	The XNLI dataset is used to evaluate cross-lingual sentence representations, focusing on the ability to transfer understanding across languages, which is crucial for multilingual NLP tasks. It also supports research on semantic textual similarity, with a methodological emphasis on removing duplicate entries during preprocessing to ensure data quality. This dataset enables researchers to assess and improve the performance of models in understanding and processing text across multiple languages.	
Southern Kurdish Corpus	citing_context	Annotated Gumar Corpus	https://doi.org/10.18653/v1/2021.sigmorphon-1.25 (2021)	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)	The Annotated Gumar Corpus is used for morphological analysis in the Southern Kurdish language, specifically to extract annotations for nouns and adjectives. This dataset enables researchers to focus on the detailed linguistic structures, facilitating studies in morphological patterns and grammatical features of Southern Kurdish.	
Southern Kurdish Corpus	citing_context	audio samples for Southern Kurdish	https://doi.org/10.48550/arXiv.2304.01319 (2023)	https://doi.org/10.1007/978-3-030-87802-3_5 (2021)	The dataset of audio samples for Southern Kurdish is used to collect 11 hours of audio data, primarily from radio and television content, for spoken dialect recognition. This dataset enables researchers to focus on the specific characteristics of Southern Kurdish dialects, enhancing the accuracy of speech recognition systems tailored to this language.	
Southern Kurdish Corpus	citing_context	Pewan corpus	https://doi.org/10.48550/arXiv.2304.01319 (2023)	https://www.semanticscholar.org/paper/0657ee32d949a1f8bb98362f4e539c8db03d52c5 (2020)	The Pewan corpus is used to analyze linguistic patterns and statistical properties, specifically focusing on the rank-size distribution for Northern and Central Kurdish. Researchers employ statistical methods to examine the frequency and distribution of linguistic elements, enabling insights into the structural characteristics of these dialects. This dataset facilitates a quantitative approach to understanding Kurdish language dynamics.	
Southern Kurdish Corpus	citing_context	Shaqlawa	https://doi.org/10.1515/flin-2024-2049 (2024)		The Shaqlawa dataset is used to study Southern Kurdish folklore and linguistic patterns, specifically focusing on traditional stories and dialect variations. Researchers employ qualitative analysis to explore the cultural and linguistic nuances within these texts, enabling a deeper understanding of Southern Kurdish oral traditions and dialectal differences.	
Southern Kurdish Corpus	citing_context	Southern Kurdish language dataset	https://www.semanticscholar.org/paper/2293b20b8721bcffeb97e2a94a3415ca4238d252	https://www.semanticscholar.org/paper/a18b2736041977700500b63ba80c1e24e8f6f061 (2019)	The Southern Kurdish language dataset, produced by Ahmadi et al. (2019), is used to align with the Southern Kurdish lexicon at the sense level for translation inference tasks. It focuses on exploring lexical and semantic relationships, enabling researchers to enhance translation accuracy and infer meanings across languages.	
Southern Thai Corpus	citing_context	Common Voice Corpus	https://doi.org/10.1109/iSAI-NLP60301.2023.10354956 (2023)	https://doi.org/10.1163/9789004724334 (2025)	The Common Voice Corpus is used to source genuine speech data for language research and development. It provides authentic audio samples that enable researchers to study speech patterns and improve speech recognition systems. The dataset's genuine data characteristic is crucial for enhancing the accuracy and reliability of language models.	
Southern Thai Corpus	citing_context	iAPP	https://doi.org/10.48550/arXiv.2504.05898 (2025)	https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The iAPP dataset is used for training and evaluating abstractive summarization models and QA systems for Thai local dialects, including Southern Thai. It focuses on multilingual summarization performance and the quality of generated questions and answers. The dataset supports human evaluation methods, enabling researchers to assess model performance in generating coherent and contextually relevant summaries and QA pairs.	
Southern Uzbek Corpus	citing_context	Southern Uzbek language dataset	https://doi.org/10.18653/v1/2024.eacl-long.100 (2024)		The Southern Uzbek language dataset is not explicitly described in the provided usage information, and no specific research applications, methodologies, or characteristics related to this dataset are mentioned. Therefore, there is no evidence to support any particular use of this dataset in research.	
Southern Uzbek Corpus	citing_context	STS-Benchmark	https://doi.org/10.18653/v1/2024.eacl-long.100 (2024)	https://doi.org/10.18653/v1/D18-1269 (2018)	The STS-Benchmark dataset is used to translate English instances into target languages, focusing on semantic textual similarity. Specifically, 800 instances from the benchmark are utilized to assess and improve translation accuracy and semantic preservation. This dataset enables researchers to evaluate and enhance cross-lingual semantic understanding and translation systems.	
Swiss German Corpus	cited_context	70-hours labeled training set of automatically aligned Swiss German speech	https://doi.org/10.48550/arXiv.2301.06790 (2023)	https://www.semanticscholar.org/paper/15e06647f706a1153d495b88d18e3f6d79f033e2 (2020)	The dataset '70-hours labeled training set of automatically aligned Swiss German speech' is mentioned in research contexts but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided in the available literature. Therefore, its exact role and impact in research cannot be accurately determined from the given information.	
Swiss German Corpus	cited_context | citing_context	ArchiMob	https://doi.org/10.21256/ZHAW-21549 (2020), https://doi.org/10.1007/s10579-019-09457-5 (2019), https://doi.org/10.18653/v1/W19-1421 (2019), https://doi.org/10.21437/interspeech.2021-1735 (2021), https://www.semanticscholar.org/paper/8f63bcffbaddb13c43ba131a8185dfc02e406b74 (2020), https://www.semanticscholar.org/paper/d464f291fbbc9ffffc35e02edc5dd8336d27b47d (2020), https://doi.org/10.48550/arXiv.2205.09501 (2022), https://www.semanticscholar.org/paper/3e6e1e480fb8711296a0056d0ea06e5de98b71f5 (2017)	https://doi.org/10.5167/UZH-126460 (2016)	The ArchiMob dataset is primarily used for studying spoken Swiss German, focusing on spontaneous speech patterns, dialectal variations, and regional linguistic features. It is utilized in transcribing interviews and social media content, supporting tasks like dialect identification, morphosyntactic tagging, and character-level machine translation. The dataset enables detailed phonetic analysis and provides a rich source of informal, multidialectal data, facilitating research on linguistic variations and continuous language changes in Switzerland.; The ArchiMob dataset is used extensively for linguistic analysis of spoken Swiss German, focusing on linguistic features, variability, and usage patterns. It provides 69 hours of multidialectal spontaneous speech data, with an average of 15,540 tokens per recording, enabling detailed phonetic, lexical, and morphological studies. The dataset supports automatic speech recognition, machine translation, and dialect identification tasks, often employing normalization techniques and cross-validation methods to enhance model performance and accuracy.	
Swiss German Corpus	cited_context	ArchiMob-1.0	https://www.semanticscholar.org/paper/15e06647f706a1153d495b88d18e3f6d79f033e2 (2020)	https://doi.org/10.5167/UZH-126460 (2016)	The ArchiMob-1.0 dataset is used to enhance the performance of speech recognition systems specifically for spoken Swiss German. It serves as additional training data, improving the accuracy and robustness of these systems in recognizing Swiss German dialects. This dataset enables researchers to address the challenge of dialectal variations in speech recognition, contributing to more effective and inclusive speech technology applications.	
Swiss German Corpus	citing_context	ArchiMob - A Corpus of Spoken Swiss German	https://doi.org/10.1007/s10579-019-09457-5 (2019)	https://doi.org/10.5167/UZH-126460 (2016)	The ArchiMob - A Corpus of Spoken Swiss German dataset is used to study spoken Swiss German, focusing on phonetic, phonological, and linguistic aspects in natural conversational contexts. It contains 34 recordings with an average of 15,540 tokens each, enabling detailed analysis of linguistic patterns and variability. Researchers use the dataset to explore the impact of additional data on model performance, particularly in automatic speech recognition, and to harmonize transcriptions for more accurate linguistic insights.	
Swiss German Corpus	cited_context | citing_context	challenge corpus of dialectal and standard German speech	https://www.semanticscholar.org/paper/8f63bcffbaddb13c43ba131a8185dfc02e406b74 (2020)	https://www.semanticscholar.org/paper/47f872379564173042a185941e3740f21ae098fe (2010)	The 'challenge corpus of dialectal and standard German speech' dataset is used to evaluate adapted Automatic Speech Recognition (ASR) systems by measuring Word Error Rates (WERs) on both dialectal and standard German speech. It specifically addresses the challenge of recognizing Swiss German varieties, enabling researchers to assess the performance and accuracy of ASR models in handling regional dialects.; The 'challenge corpus of dialectal and standard German speech' is used to evaluate adapted Automatic Speech Recognition (ASR) systems, particularly focusing on word error rates (WERs) for Swiss German varieties. The corpus includes both dialectal and standard German speech, offering a comprehensive test environment that enables researchers to assess the performance and accuracy of ASR systems across different linguistic variations.	
Swiss German Corpus	citing_context	chatmania data	https://www.semanticscholar.org/paper/3573cba825804c32cd5172eee6ccecf507e77737 (2020)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The 'chatmania data' dataset is used to collect forum entries and web data, primarily from chat forums, for Swiss German language analysis. It focuses on informal online communication patterns and is utilized to build large dictionaries and analyze linguistic patterns across diverse online sources. This dataset enables researchers to study the nuances of Swiss German in informal settings, providing insights into language use and evolution in digital contexts.	
Swiss German Corpus	citing_context	Common Voice	https://doi.org/10.48550/arXiv.2310.15135 (2023)	https://doi.org/10.21437/interspeech.2021-329 (2020)	The Common Voice dataset is used to fine-tune XLS-R and Whisper models, specifically examining the impact of dialects in training data on model performance. It is also utilized for additional evaluations, focusing on cross-lingual representation learning for speech recognition. This dataset enables researchers to enhance model accuracy and robustness across different linguistic contexts.	
Swiss German Corpus	citing_context	CSS10	https://www.semanticscholar.org/paper/823c3d7684e3345535b1bcab044174fe2e0adb08 (2021)	https://doi.org/10.21437/interspeech.2019-1500 (2019)	The CSS10 dataset is used to train models on the German part of the dataset, particularly to enhance data efficiency in low-resource settings. This approach provides a good initialization for models, improving their performance with limited data. The dataset's German segment is crucial for these applications, enabling researchers to address challenges in resource-constrained environments.	
Swiss German Corpus	citing_context	Dialect Atlas of German-Speaking Switzerland (DAGSS)	https://doi.org/10.48550/arXiv.2310.09088 (2023)	https://doi.org/10.1177/00754240022005072 (2000)	The Dialect Atlas of German-Speaking Switzerland (DAGSS) is used to quantify differences between Swiss German dialects through dialectometric data. Researchers focus on linguistic variations and their geographic distribution, employing methods that analyze and map these variations to understand the spatial patterns and relationships among dialects. This dataset enables detailed studies of linguistic diversity and regional linguistic characteristics in German-speaking Switzerland.	
Swiss German Corpus	citing_context	DSGS fingerspelling sequences	https://doi.org/10.1007/978-3-319-58703-5_1 (2017)	https://doi.org/10.18653/v1/W15-5103 (2015)	The DSGS fingerspelling sequences dataset is used to evaluate the comprehensibility of synthesized finger alphabet animations in Swiss German Sign Language. Research focuses on user perception and recognition accuracy, employing methodologies that assess how well users can understand and recognize the synthesized animations. This dataset enables researchers to improve the effectiveness of sign language communication tools by providing a standardized set of sequences for testing and evaluation.	
Swiss German Corpus	cited_context	DSGS lexicon	https://www.semanticscholar.org/paper/eee05fad72aaa6c14e14525e74f35a923c0077ff (2018)	https://doi.org/10.1075/SLL.4.12.10BOY (2002)	The DSGS lexicon dataset is used to verify the base form of signs in Swiss German Sign Language, ensuring accurate representation and standardization. Researchers employ this dataset to focus on the precise forms of signs, which aids in linguistic studies and the development of standardized sign language resources. This dataset enables detailed analysis and verification of sign language forms, supporting research in sign language linguistics and standardization efforts.	
Swiss German Corpus	citing_context	DSGS teaching materials	https://doi.org/10.29140/LEA.V2N1.85 (2019)		The DSGS teaching materials dataset is used to sample test items at the A1 level, focusing on the content and structure of the materials for educational assessment. This involves analyzing the educational content to ensure it aligns with A1 proficiency standards, enabling researchers to evaluate the effectiveness of teaching materials in educational settings.	
Swiss German Corpus	cited_context	German SMS corpus	https://doi.org/10.1017/S1351324919000391 (2019)	https://www.semanticscholar.org/paper/bd0bab6fc8cd43c0ce170ad2f4cb34181b31277d (1994)	The German SMS corpus is used to adapt a general standard German model for part-of-speech tagging, specifically focusing on normalization for Swiss German SMS messages. This involves developing methodologies to handle the unique linguistic features of SMS communication, enabling more accurate tagging and processing of informal, abbreviated text.	
Swiss German Corpus	citing_context	GSWNorm 2022	https://doi.org/10.48550/arXiv.2310.09088 (2023)		The GSWNorm 2022 dataset is used for the GSWNorm 2022 Shared Task, focusing on word-level normalization of Swiss German sentences into Standard German. It contains 7284 sentences (106K tokens / 20K types). Researchers employ this dataset to develop and evaluate normalization models, addressing challenges in converting dialectal forms to standard language, enhancing the interoperability and accessibility of Swiss German content.	
Swiss German Corpus	cited_context | citing_context	HeLI	https://www.semanticscholar.org/paper/823c3d7684e3345535b1bcab044174fe2e0adb08 (2021)	https://doi.org/10.21437/Interspeech.2014-480 (2014)	The HeLI dataset is used for Swiss German dialect identification experiments, employing character-level convolutional neural networks to distinguish between different dialects. This methodology focuses on leveraging the dataset's character-level features to enhance the accuracy of dialect recognition, addressing specific research questions related to linguistic differentiation and dialect classification.	
Swiss German Corpus	cited_context | citing_context	Idiotikon	https://doi.org/10.1007/s10579-019-09457-5 (2019), https://doi.org/10.5167/UZH-126461 (2015)	https://doi.org/10.5167/UZH-117114 (2015)	The Idiotikon dataset is used to study isolated word types in Swiss German dialects, focusing on lexical variations and historical usage. It is applied to analyze regional linguistic patterns, providing a comprehensive geographical distribution of dialectal features. This dataset enables researchers to explore the nuanced differences and historical evolution of Swiss German dialects through detailed lexical analysis.; The Idiotikon dataset is used to analyze Swiss German dialectology, focusing on dialectal variations and lexical items. It serves as a comprehensive lexicon to study the distribution and usage of Swiss German dialects, regional linguistic variations, and specific vocabulary. This dataset enables detailed linguistic and dialectal studies by providing extensive lexical coverage and regional data.	
Swiss German Corpus	cited_context | citing_context	Leipzig corpora collection	https://www.semanticscholar.org/paper/38e38bf3e6adf7060f889f5c1e54ef60ee0924c1 (2019)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The Leipzig corpora collection is used to support research into the Swiss German language and linguistic patterns by providing access to the only comprehensive written Swiss German corpus. It is also utilized to build a Swiss German Language Identification system, where texts gathered from the Internet are used to train and evaluate the model. The dataset's comprehensive nature and focus on written Swiss German enable these specific research applications.; The Leipzig corpora collection is used to support research into the Swiss German language and linguistics by providing access to the only comprehensive written Swiss German corpus. This dataset enables researchers to analyze linguistic patterns and build a Swiss German Language Identification (LID) system, enhancing language detection capabilities using internet-gathered texts.	
Swiss German Corpus	cited_context | citing_context	MediaParl corpus	https://www.semanticscholar.org/paper/8f63bcffbaddb13c43ba131a8185dfc02e406b74 (2020)	https://doi.org/10.1109/SLT.2012.6424233 (2012)	The MediaParl corpus is used to train Automatic Speech Recognition (ASR) systems on Swiss German speech data, particularly from the canton of Valais. It includes text transcriptions in standard German, enabling research on bilingual mixed-language accented speech. This dataset facilitates the development and evaluation of ASR models tailored to handle the complexities of regional dialects and language mixing.; The MediaParl corpus is used to train Automatic Speech Recognition (ASR) systems, specifically focusing on Swiss German speech data from the canton of Valais. The dataset includes audio recordings with corresponding text transcriptions in standard German, enabling researchers to develop and evaluate ASR models tailored for this unique linguistic context.	
Swiss German Corpus	cited_context	Mozilla Common Voice 13 German	https://doi.org/10.48550/arXiv.2412.15726 (2024)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Mozilla Common Voice 13 German dataset is used to train and evaluate models aimed at mitigating language forgetting, particularly in the context of the Swiss German language dataset. This involves maintaining model performance across multiple languages. The dataset's multilingual capabilities and extensive speech samples enable researchers to address challenges in cross-lingual model stability and performance.	
Swiss German Corpus	citing_context	NOAH corpus	https://www.semanticscholar.org/paper/3573cba825804c32cd5172eee6ccecf507e77737 (2020)	https://www.semanticscholar.org/paper/38e38bf3e6adf7060f889f5c1e54ef60ee0924c1 (2019)	The NOAH corpus is used to enhance Swiss German language research by providing a diverse collection of texts from various genres, including chat data and user-generated content from forums and social media. This enriches the dataset with real-time and informal communication patterns, capturing a broad spectrum of linguistic usage. The corpus supports research into conversational dynamics and the breadth of Swiss German language in digital contexts.	
Swiss German Corpus	cited_context | citing_context	corpus of radio news broadcasts in Valais German	https://www.semanticscholar.org/paper/8f63bcffbaddb13c43ba131a8185dfc02e406b74 (2020)	https://doi.org/10.1109/ICASSP.2019.8683513 (2019)	The corpus of radio news broadcasts in Valais German is used to train Automatic Speech Recognition (ASR) systems, specifically focusing on dialectal transcriptions. This dataset enables researchers to achieve a Word Error Rate (WER) of 19.4%, enhancing the accuracy of ASR models for Valais German dialects. The dataset's relevance lies in its authentic dialectal content, which is crucial for improving speech recognition in under-resourced languages.; The corpus of radio news broadcasts in Valais German is used to train Automatic Speech Recognition (ASR) systems, specifically focusing on dialectal transcriptions. This dataset enables researchers to achieve a Word Error Rate (WER) of 19.4%, enhancing the accuracy of ASR models for Valais German dialects. The dataset's relevance lies in its authentic dialectal content, which is crucial for improving speech recognition in under-resourced dialects.	
Swiss German Corpus	citing_context	Radio Rottu Oberwallis dataset	https://doi.org/10.48550/arXiv.2207.00412 (2022)	https://doi.org/10.21437/Interspeech.2014-480 (2014)	The Radio Rottu Oberwallis dataset is used to study Swiss German dialects, particularly the Wallis dialect. It consists of 7 hours of audio, with 2 hours transcribed in Standard German. Researchers employ this dataset to analyze linguistic features and variations within the Wallis dialect, using the transcribed data for detailed phonetic and phonological analysis. This enables insights into the structure and evolution of regional Swiss German dialects.	
Swiss German Corpus	citing_context	SAND1	https://doi.org/10.1515/DIALECT-2016-0006 (2016)	https://doi.org/10.1016/J.LINGUA.2009.02.001 (2009)	The SAND1 dataset is used to analyze syntactic structures in Swiss German, providing syntactic data for linguistic studies. Researchers employ this dataset to explore specific syntactic patterns and structures, contributing to the understanding of Swiss German grammar. The dataset's focus on syntactic elements enables detailed linguistic analysis, supporting research into the grammatical features of Swiss German.	
Swiss German Corpus	cited_context	SDS	https://doi.org/10.3389/frai.2021.642505 (2021)	https://doi.org/10.13097/ARCHIVE-OUVERTE/UNIGE:26361 (2012)	The SDS dataset is used to analyze Swiss German dialects, with a focus on digitization efforts for linguistic projects. It supports research into the preservation and documentation of regional dialects, enabling scholars to explore linguistic variations and develop digital resources for Swiss German.	
Swiss German Corpus	citing_context	SDS-200	https://doi.org/10.48550/arXiv.2207.00412 (2022), https://doi.org/10.48550/arXiv.2304.11075 (2023)	https://www.semanticscholar.org/paper/823c3d7684e3345535b1bcab044174fe2e0adb08 (2021)	The SDS-200 dataset is primarily used for fine-tuning and training models on spoken Swiss German, particularly for speech-to-text tasks. It contains 189 hours of audio with clean labels and is utilized to align spoken Swiss German with written Standard German, enhance speech recognition accuracy, and capture multidialectal variations. The dataset is often used to fine-tune the XLS-R 1B model, improving its performance in dialectal speech processing and generalization to standard German. It is also applied to evaluate speech recognition performance, focusing on word error rates in multi-speaker, multi-dialect settings.	
Swiss German Corpus	cited_context	Shared Task data	https://doi.org/10.21256/ZHAW-21550 (2020)	https://doi.org/10.21437/interspeech.2019-1819 (2019)	The 'Shared Task data' dataset is used to fine-tune a pre-trained character-based Automatic Speech Recognition (ASR) system, specifically for recognizing Swiss German dialects. The dataset is employed for evaluation purposes, enabling researchers to assess the performance and accuracy of the ASR system in handling Swiss German speech.	
Swiss German Corpus	citing_context	SMILE DSGS database	https://doi.org/10.1145/3536221.3556623 (2022)	https://www.semanticscholar.org/paper/eee05fad72aaa6c14e14525e74f35a923c0077ff (2018)	The SMILE DSGS database is used to develop and study Swiss German sign language processing systems. It focuses on assessing sign language and analyzing linguistic patterns and features through linguistically annotated data. This dataset enables researchers to enhance sign language technology and deepen understanding of Swiss German sign language's unique characteristics.	
Swiss German Corpus	cited_context	sms4science	https://doi.org/10.5167/UZH-49872 (2011)		The 'sms4science' dataset is used to study texting patterns in multilingual Switzerland, focusing on linguistic variations and challenges in Swiss German and other languages. Researchers employ this dataset to analyze how different linguistic groups use text messaging, examining specific variations and challenges in communication. This enables insights into the sociolinguistic dynamics of multilingual environments.	
Swiss German Corpus	cited_context | citing_context	sms4science project	https://doi.org/10.3115/v1/W14-5310 (2014)	https://doi.org/10.5167/UZH-49872 (2011)	The 'sms4science project' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described beyond its mere mention in the literature.; The 'sms4science project' dataset is used to study text messages in Swiss German dialects, offering a unique written resource for linguistic analysis. It provides insights into the multilingual landscape of Switzerland, enabling researchers to explore the use and variation of Swiss German in digital communication. This dataset supports the examination of linguistic patterns and dialectal features in a contemporary context.	
Swiss German Corpus	cited_context	SMS set	https://doi.org/10.1017/S1351324919000391 (2019)	https://www.semanticscholar.org/paper/f53b626eae0757b58b89e4d887aa3df6ac6cdcbd (2015)	The SMS set, containing 262,494 alignment units, is used as additional training data for Swiss language normalization, particularly for SMS messages in various Swiss languages. It is manually normalized using a web annotation tool and analyzed to study linguistic patterns and usage in informal communication contexts. This dataset enables researchers to improve normalization techniques and understand the nuances of informal language use in Switzerland.	
Swiss German Corpus	citing_context	SPC corpus	https://doi.org/10.48550/arXiv.2207.00412 (2022)	https://www.semanticscholar.org/paper/3fd70d12b556bf62a6c8011931ce039d729b0a91 (2020)	The SPC corpus is primarily used for training and evaluating models on Swiss German speech-to-text tasks, focusing on the alignment between spoken Swiss German and written Standard German. It is utilized to improve automatic alignment and transcription accuracy, particularly in parliamentary contexts. Studies often emphasize the lack of metadata, such as gender, age, and dialect, and assess system performance using metrics like BLEU scores and WER. The dataset also supports multidialectal research and the analysis of audio quality and utterance length distribution.	
Swiss German Corpus	cited_context	Sprachatlas	https://doi.org/10.5167/UZH-126461 (2015)	https://doi.org/10.1515/9783110309997.153 (2013)	The Sprachatlas dataset is mentioned in research contexts but lacks detailed descriptions of its usage, methodology, or specific research questions. There is no explicit information on how it is employed in studies or its enabling role in research. Therefore, based on the provided evidence, no specific claims can be made about its actual use in research.	
Swiss German Corpus	citing_context	SRF Meteo weather report dataset	https://doi.org/10.1109/ASRU51503.2021.9688249 (2021)	https://www.semanticscholar.org/paper/cdcd26085e3b63d7fd3e3d9c5fee5aef9dad0359 (2018)	The SRF Meteo weather report dataset is used to study linguistic and dialectal variations in Swiss German, with textual annotations in standard German. It supports the analysis of speech in 8 different Swiss German dialects, including both Swiss German and standard German transcriptions. This enables researchers to explore dialectal diversity and linguistic patterns in weather reports.	
Swiss German Corpus	cited_context | citing_context	STT4SG-350	https://doi.org/10.48550/arXiv.2505.22054 (2025), https://doi.org/10.48550/arXiv.2207.00412 (2022), https://doi.org/10.48550/arXiv.2310.09088 (2023)	https://doi.org/10.48550/arXiv.2305.18855 (2023)	The STT4SG-350 dataset is used extensively in research focused on Swiss German dialects, particularly for evaluating and training speech recognition and translation systems. It contains 350 hours of parallel data, including 35 hours of audio per dialect, and is utilized to enhance the robustness and accuracy of speech-to-text models. The dataset supports the development of phonemized n-gram models and ensures balanced gender representation, enriching training data with Standard German content to handle mixed-language scenarios.; The STT4SG-350 dataset is used to analyze speech-to-text translation in Swiss German dialects, leveraging 343 hours of speech data for model training and evaluation. This dataset enables researchers to develop and refine translation models specifically tailored to the complexities of Swiss German, enhancing accuracy and performance in dialect-specific speech recognition tasks.	
Swiss German Corpus	cited_context	Swiss German ArchiMob corpus	https://doi.org/10.13092/LO.98.5947 (2019)	https://doi.org/10.1017/S1351324915000236 (2015)	The Swiss German ArchiMob corpus is used for developing and evaluating automatic normalization techniques for Swiss German text. Specifically, it employs character-level machine translation to enhance the accuracy of normalization. This dataset facilitates research aimed at improving the processing and standardization of Swiss German text, addressing challenges in linguistic variability and dialectal differences.	
Swiss German Corpus	cited_context | citing_context	Swiss German Dialect Corpus	https://www.semanticscholar.org/paper/d464f291fbbc9ffffc35e02edc5dd8336d27b47d (2020)	https://doi.org/10.3115/v1/W14-5310 (2014)	The Swiss German Dialect Corpus is used to compile and normalize spontaneous noisy writing in Swiss German, primarily for Part-of-Speech tagging and linguistic analysis. Researchers employ this dataset to enhance the accuracy of linguistic annotations and to study the grammatical structures and usage patterns in informal, dialectal Swiss German texts. This corpus enables detailed linguistic research by providing a rich source of authentic, unedited language data.; The Swiss German Dialect Corpus is used to compile and normalize spontaneous noisy writing in Swiss German, primarily for Part-of-Speech tagging and linguistic analysis. Researchers employ this dataset to enhance the accuracy of linguistic annotations and to study the grammatical structures and usage patterns in informal, dialectal Swiss German texts. This corpus enables detailed linguistic research by providing a rich, authentic source of dialectal data.	
Swiss German Corpus	cited_context | citing_context	Swiss German dialect version of the official annual report of the Swatch company from 2012	https://doi.org/10.3115/v1/W14-5310 (2014)		The dataset 'Swiss German dialect version of the official annual report of the Swatch company from 2012' is mentioned in research citations but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided, and its relevance to Swiss German language studies or other areas is not explicitly stated.; The dataset is used to augment Swiss German text, specifically to study linguistic features and dialectal variations in corporate communications. Researchers employ this dataset to analyze how Swiss German dialects are utilized in formal business contexts, focusing on the unique linguistic characteristics and variations present in the annual report. This enables a deeper understanding of dialect use in professional settings.	
Swiss German Corpus	cited_context | citing_context	Swiss German Dictionary	https://doi.org/10.21437/interspeech.2021-1735 (2021), https://www.semanticscholar.org/paper/823c3d7684e3345535b1bcab044174fe2e0adb08 (2021)	https://www.semanticscholar.org/paper/d464f291fbbc9ffffc35e02edc5dd8336d27b47d (2020)	The Swiss German Dictionary dataset is used to map Standard German words to Swiss German pronunciations and spontaneous writings, facilitating research into linguistic variations and phonetic analysis. It supports studies on spoken and written Swiss German, enabling the normalization of dialectal forms into High German. This aids in linguistic standardization and provides a resource for comprehensive linguistic and phonetic research.; The Swiss German Dictionary dataset is used to normalize forms of common words in various Swiss German dialects into High German. This process aids in linguistic standardization and analysis, enabling researchers to compare and study dialectal variations systematically. The dataset's comprehensive coverage of dialectal forms facilitates accurate linguistic transformations, enhancing the precision of dialectal research.	
Swiss German Corpus	cited_context	Swiss German raw data	https://www.semanticscholar.org/paper/9d6021627149d28aa4b94de6cbb1c66463425f76 (2010)	https://www.semanticscholar.org/paper/0ceef6f87ab5f4300a2cd213f6d781f54c3cf822 (2010)	The Swiss German raw data dataset is used for linguistic analysis and natural language processing tasks, specifically to extract probability maps and implement word transformation rules, focusing on linguistic variations within the Swiss German dialect area. It is also utilized for machine translation and dialect parsing, with raw data accessible via an interactive web page. This dataset enables researchers to explore and process Swiss German dialects effectively, enhancing understanding and computational handling of these linguistic variations.	
Swiss German Corpus	citing_context	Swiss Parliament Corpus	https://doi.org/10.1109/ASRU51503.2021.9688249 (2021)	https://www.semanticscholar.org/paper/15e06647f706a1153d495b88d18e3f6d79f033e2 (2020)	The Swiss Parliament Corpus is used to develop a low-resource speech-to-text corpus for Swiss German, employing automatic collection methods to gather parliamentary speech data. This dataset facilitates research in speech recognition and transcription, particularly in under-resourced languages, enabling the creation of more accurate and contextually relevant speech-to-text models.	
Swiss German Corpus	cited_context | citing_context	Swiss Parliaments Corpus	https://doi.org/10.1007/978-3-031-93429-2_20 (2025), https://doi.org/10.48550/arXiv.2205.09501 (2022), https://doi.org/10.48550/arXiv.2310.09088 (2023)	https://www.semanticscholar.org/paper/3fd70d12b556bf62a6c8011931ce039d729b0a91 (2020)	The Swiss Parliaments Corpus is primarily used for training and optimizing speech-to-text (STT) models, particularly for Swiss German language processing. It provides an automatically aligned corpus of Swiss German speech to Standard German text, enabling researchers to focus on speech-to-text alignment and linguistic analysis of parliamentary speeches. The dataset includes 293 hours of speech data from the Bernese cantonal parliament and 26 hours of single-sentence samples, facilitating multidialectal research and enhancing model performance.; The Swiss Parliaments Corpus is used to study the Bernese dialect of Swiss German, focusing on the analysis of 293 hours of speech data from the Bernese cantonal parliament. It is employed for speech-to-text alignment and linguistic analysis, particularly evaluating the effectiveness of models in aligning Swiss German speech with Standard German text. This dataset enables detailed research into dialectal variations and speech processing in parliamentary settings.	
Swiss German Corpus	cited_context | citing_context	Swiss SMS Corpus	https://www.semanticscholar.org/paper/38e38bf3e6adf7060f889f5c1e54ef60ee0924c1 (2019), https://doi.org/10.5167/UZH-126460 (2016)	https://www.semanticscholar.org/paper/f53b626eae0757b58b89e4d887aa3df6ac6cdcbd (2015)	The Swiss SMS Corpus is used to validate Language Identification (LID) systems, particularly for their robustness in handling short SMS sentences, including those as brief as five words. It is also employed to compare and contrast with transcribed spoken language corpora, highlighting differences in data sources and linguistic contexts, and emphasizing the distinctions between written and spoken Swiss German. This dataset enables researchers to test and refine LID systems and to explore the unique characteristics of SMS communication in Swiss German.; The Swiss SMS Corpus is used to validate Language Identification (LID) systems, particularly focusing on their performance with short SMS sentences. The dataset's characteristic of containing brief messages, often as short as five words, is crucial for testing the robustness of these systems in real-world scenarios. This application highlights the dataset's utility in enhancing the accuracy and reliability of LID systems for short text inputs.	
Swiss German Corpus	cited_context	Syntaktischer Atlas der deutschen Schweiz (SADS)	https://www.semanticscholar.org/paper/0ceef6f87ab5f4300a2cd213f6d781f54c3cf822 (2010)		The Syntaktischer Atlas der deutschen Schweiz (SADS) is used to address the lack of syntactic data in Swiss German dialects. It provides a comprehensive syntactic atlas, enhancing linguistic research by offering detailed syntactic information. This dataset enables researchers to fill gaps in syntactic knowledge, supporting in-depth analysis and expanding the understanding of Swiss German dialects.	
Swiss German Corpus	cited_context | citing_context	TüBa-D/Z German Treebank	https://doi.org/10.3115/v1/W14-5310 (2014)	https://www.semanticscholar.org/paper/5704793e4704aaf2643f3942a455f9efdc96131e (2012)	The TüBa-D/Z German Treebank is used in conjunction with a Swiss German corpus to enhance linguistic analysis, particularly focusing on syntactic structures and token distribution in written German. It is also utilized to train and evaluate Swiss German language models, leveraging over 1,300,000 tokens. This combination allows researchers to improve the accuracy and robustness of language models and linguistic analyses.; The TüBa-D/Z German Treebank is used in conjunction with a Swiss German corpus to enhance linguistic analysis, particularly focusing on syntactic structures and token distribution in written German. It is employed to augment the Swiss German corpus, facilitating detailed annotation and analysis of syntactic patterns and lexical elements. This combination supports research into the structural and distributional aspects of written German, providing a robust foundation for linguistic studies.	
Swiss German Corpus	cited_context	TuDa	https://doi.org/10.21256/ZHAW-21550 (2020)	https://www.semanticscholar.org/paper/f6a6ae860a85875e19006cfa1e8ca65d6b2b784d (2018)	The TuDa dataset is used to train a language model for Swiss German, specifically focusing on dialectal variations. It employs a corpus of spoken and written texts to address the complexities of Swiss German dialects. This dataset enables researchers to develop more accurate and nuanced language models for Swiss German, enhancing natural language processing capabilities in this linguistic area.	
Swiss German Corpus	cited_context	Valaisan Swiss German corpus	https://www.semanticscholar.org/paper/3e6e1e480fb8711296a0056d0ea06e5de98b71f5 (2017)	https://doi.org/10.21437/Interspeech.2014-480 (2014)	The Valaisan Swiss German corpus is used to develop automatic speech recognition and translation systems for the Walliserdeutsch dialect. Research focuses on collecting and processing spoken data, enabling the creation of models that can accurately recognize and translate this specific dialect. The dataset's spoken content is crucial for training these systems, addressing the unique phonetic and linguistic characteristics of Walliserdeutsch.	
Sylheti Corpus	citing_context	a new speech database in Sylheti	https://doi.org/10.1109/CSCC49995.2020.00014 (2020)	https://doi.org/10.35940/ijrte.c5874.098319 (2019)	The Sylheti speech database is used to develop and evaluate speech recognition systems for isolated words in the Sylheti language. Researchers focus on enhancing Automatic Speech Recognition (ASR) performance for under-resourced languages, leveraging the dataset's recordings to improve system accuracy and robustness.	
Sylheti Corpus	citing_context	ONUBAD	https://doi.org/10.48550/arXiv.2505.12273 (2025)	https://doi.org/10.1016/j.dib.2025.111276 (2025)	The ONUBAD dataset is used to translate Sylheti and other Bangla regional dialects into Standard Bangla and English. It consists of an expert-translated parallel corpus with 1,540 words, 130 clauses, and 980 sentences per dialect. This dataset enables researchers to focus on translation accuracy and linguistic nuances, facilitating the development of translation models and tools for these dialects.	
Tatar Corpus	citing_context	Corpus of Written Tatar	https://doi.org/10.18653/V1/2021.CALCS-1.18 (2021)		The Corpus of Written Tatar is used to prepare and evaluate 700 shuffled sentences, both in Cyrillic and Latin scripts, for performance assessment. These sentences, totaling 8,466 words with duplicates and 5,261 without, are manually transcribed and verified by a native speaker. The dataset is also utilized to develop and train the FinTat 2 transliteration tool, specifically focusing on written Tatar language data. This enables researchers to enhance transliteration accuracy and evaluate system performance.	
Tatar Corpus	cited_context	CVC	https://doi.org/10.3390/info14020074 (2023)	https://doi.org/10.1007/978-3-030-83527-9_41 (2021)	The CVC dataset is used to fine-tune a Tatar speech recognition system, leveraging 129 hours of annotated data. It is specifically employed for training and testing, enabling researchers to achieve a Word Error Rate (WER) of 5.37%. This dataset facilitates the development and evaluation of speech recognition models tailored for the Tatar language.	
Tatar Corpus	citing_context	Kardeş-NLU	https://doi.org/10.48550/arXiv.2502.11020 (2025)	https://doi.org/10.18653/v1/2024.eacl-long.100 (2024)	The Kardeş-NLU dataset is used to evaluate multilingual language understanding models, particularly focusing on transfer learning from high-resource to low-resource Turkic languages, including Tatar. This dataset enables researchers to assess the effectiveness of these models in understanding and processing Tatar, a low-resource language, by leveraging knowledge from better-resourced languages.	
Tatar Corpus	cited_context | citing_context	SART	https://doi.org/10.1109/ICAIIC60209.2024.10463261 (2024)	https://doi.org/10.1007/978-3-031-24337-0_28 (2019)	The SART dataset is used to evaluate word embeddings for the Tatar language, focusing on tasks such as text similarity, analogies, and text relatedness. It provides benchmark datasets for classical NLP tasks, enabling researchers to assess the performance of word embeddings in these specific linguistic contexts.; The SART dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	
Tatar Corpus	citing_context	Tatar Language Corpus	https://doi.org/10.1007/978-3-031-24337-0_28 (2019)	https://doi.org/10.3115/v1/D14-1162 (2014)	The Tatar Language Corpus is used to train word embedding models, specifically focusing on the Tatar language. These models are trained with 300-dimensional vectors and a window size of 5, enabling researchers to capture semantic and syntactic relationships within the Tatar language. This methodology supports research in natural language processing and linguistic analysis, enhancing the representation of Tatar words in computational models.	
Tatar Corpus	cited_context	Tatar Speech Corpus	https://doi.org/10.3390/info14020074 (2023)	https://doi.org/10.1007/978-3-030-87802-3_40 (2021)	The Tatar Speech Corpus is used to address the lack of publicly available datasets for the Tatar language. It serves as a necessary resource for studying Tatar, particularly in the context of other Turkic language corpora. The dataset enables researchers to develop and evaluate speech processing systems and linguistic models specific to Tatar, enhancing the availability of resources for this underrepresented language.	
Tatar Corpus	cited_context | citing_context	TatarST	https://doi.org/10.48550/arXiv.2305.15749 (2023), https://doi.org/10.3390/info14020074 (2023)	https://www.semanticscholar.org/paper/cf4c9d626facb20c271ca5cbd04af5c0eb06f813 (2021), https://doi.org/10.1109/ICASSP.2015.7178964 (2015)	The TatarST dataset is used for speech recognition tasks, specifically to develop and evaluate speech processing models for the Tatar language. This involves training and testing algorithms to improve the accuracy of speech recognition systems tailored for Tatar. The dataset's focus on the Tatar language enables researchers to address the unique phonetic and linguistic challenges of this language, enhancing the performance of speech technologies in Tatar-speaking communities.; The TatarST dataset is used to compare the size of speech corpora, focusing on the availability of Tatar speech data for training and evaluation. This comparison helps researchers understand the scale and adequacy of Tatar speech datasets, which is crucial for developing and assessing speech processing systems in the Tatar language.	
Tatar Corpus	cited_context | citing_context	TATAR-TTS	https://doi.org/10.1109/IECON55916.2024.10905876 (2024), https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.1109/ICAIIC60209.2024.10463261 (2024), https://doi.org/10.1109/ICASSP40776.2020.9053512 (2019)	The TatarTTS dataset is used to develop and evaluate text-to-speech (TTS) synthesis systems for the Tatar language. Researchers focus on enhancing the naturalness and intelligibility of synthesized speech, employing methodologies that assess the quality and clarity of the generated audio. This dataset enables the creation of more effective and realistic TTS systems for Tatar, addressing specific linguistic challenges and improving user experience.; The TATAR-TTS dataset is used to train text-to-speech (TTS) models specifically for the Tatar language. It is integrated into the LJ Speech training recipe within the ESPnet-TTS toolkit, enabling researchers to develop and refine TTS systems that accurately synthesize Tatar speech. This dataset facilitates the creation of natural-sounding voice outputs, enhancing the accessibility and usability of Tatar language technologies.	TatarTTS
Tatar Corpus	citing_context	TUATURK	https://doi.org/10.48550/arXiv.2502.11020 (2025)	https://doi.org/10.48550/arXiv.2309.04766 (2023)	The TUATURK dataset is used to evaluate multilingual language understanding across Turkic languages, including Tatar. It focuses on cross-lingual alignment and cultural reasoning, employing methodologies that assess the ability of models to understand and reason about linguistic and cultural nuances across these languages. This dataset enables researchers to address specific questions related to the performance and adaptability of language models in multilingual contexts, particularly within the Turkic language family.	
Tigrigna Corpus	cited_context	Bible	https://doi.org/10.1109/IALP.2016.7875939 (2016)	https://doi.org/10.1023/A:1001798929185 (1999)	The Bible dataset is used as a parallel corpus for linguistic analysis, particularly to annotate and study the 'Book of 2000 Tongues'. Researchers focus on cross-linguistic comparisons and detailed annotations, leveraging the dataset's structured text to explore linguistic features and patterns across languages. This approach enables in-depth analysis of linguistic structures and facilitates comparative linguistics research.	
Tigrigna Corpus	citing_context	Nagaoka Tigrinya Corpus	https://doi.org/10.1109/ICAIBD49809.2020.9137443 (2020)	https://www.semanticscholar.org/paper/4f39041e2d53ee4b0c171ed836fe7136395f2ad1 (2016)	The Nagaoka Tigrinya Corpus is used to develop a part-of-speech tagged corpus for Tigrinya, specifically focusing on categorizing news articles. This enhances linguistic analysis and improves tagging accuracy, leveraging the dataset's structured content to refine natural language processing techniques for the Tigrinya language.	
Tigrigna Corpus	citing_context	Tigrinya Language Speech Corpus	https://doi.org/10.18653/V1/2020.WINLP-1.5 (2020)	https://www.semanticscholar.org/paper/cc60f276ac89eb91dd83fb1de8be698a8f8cee8b (2018)	The Tigrinya Language Speech Corpus is used to develop speech recognition systems for Tigrigna, specifically focusing on creating a medium-sized read speech corpus. This dataset supports linguistic and technological advancements by providing a resource for training and testing speech recognition models, enabling researchers to improve the accuracy and robustness of these systems for the Tigrigna language.	
Tiv Corpus	citing_context	Icighan Bibilo (2007)	https://doi.org/10.34256/ijll2111 (2021)		The Icighan Bibilo (2007) dataset is used to document and analyze graphological and lexical changes in the Tiv language, focusing on phonetic and orthographic modifications. It serves as a baseline to compare with the 2017 edition, enabling researchers to highlight and study changes in Tiv grammar and vocabulary over time. This dataset facilitates longitudinal linguistic analysis by providing a reference point for identifying and examining linguistic evolution.	
Tunisian Arabic Corpus	citing_context	aebWordNet	https://www.semanticscholar.org/paper/f289447b64f774951b8b4eaf1fd07ecbaf87f506 (2019)		The aebWordNet dataset serves as a standardized lexical resource for Tunisian Arabic, structured according to the extended WordNet-LMF model as per ISO 24613. It is utilized to provide a consistent and structured vocabulary, facilitating research in lexical semantics and natural language processing tasks specific to Tunisian Arabic. This dataset enables researchers to develop and evaluate computational models and linguistic tools tailored to the unique characteristics of the Tunisian Arabic language.	
Tunisian Arabic Corpus	cited_context	Al-Khalil-TUN	https://doi.org/10.1016/J.JKSUCI.2017.01.004 (2017)	https://doi.org/10.1007/978-3-642-37247-6_13 (2013)	The Al-Khalil-TUN dataset is primarily used for orthographic transcription and linguistic analysis of spoken Tunisian Arabic. It supports the extraction of function words and clitics, and is utilized in developing free software tools for linguistic research. The dataset is also used to train and test the TD version of Al-Khalil, focusing on improving the accuracy of morphological disambiguation in spoken Tunisian Arabic transcriptions.	
Tunisian Arabic Corpus	citing_context	apc	https://doi.org/10.18653/v1/2024.iwslt-1.25 (2024)	https://doi.org/10.3115/1073083.1073135 (2002)	The 'apc' dataset is used to evaluate the performance of fine-tuned Neural Machine Translation (NMT) and Large Language Models (LLM), specifically focusing on measuring translation quality through BLEU scores. This dataset enables researchers to assess and compare the effectiveness of different model configurations and fine-tuning techniques in translating Tunisian Arabic.	
Tunisian Arabic Corpus	cited_context	Araber corpus	https://doi.org/10.21437/Interspeech.2005-710 (2005)		The Araber corpus is used to provide speech data for Tunisian Arabic language research, specifically focusing on phonetic and phonological analysis. This dataset enables researchers to examine the acoustic properties and sound patterns of Tunisian Arabic, contributing to a deeper understanding of its linguistic structure. The corpus supports methodologies that involve detailed acoustic analysis and phonological modeling, facilitating research into the unique phonetic features of this dialect.	
Tunisian Arabic Corpus	cited_context	ArSarcasmDia	https://doi.org/10.48550/arXiv.2305.14976 (2023)	https://www.semanticscholar.org/paper/acc88786e83acc3a7fc0280bbe592706a8034c8f (2020)	The ArSarcasmDia dataset is primarily used for sarcasm detection in Arabic, with a focus on Tunisian Arabic dialects and social media contexts. Researchers employ machine learning models to classify sarcastic versus non-sarcastic content, leveraging the dataset's nuanced linguistic features. Additionally, the dataset is used for named entity recognition, enhancing the understanding of entities within Tunisian Arabic texts.	
Tunisian Arabic Corpus	citing_context	CODA Tun.	https://doi.org/10.13053/RCS-90-1-9 (2015)	https://www.semanticscholar.org/paper/a24b30b1db2ed332aae0ca77b578f9d4b94a7bab (2012)	The CODA Tun. dataset is used to propose a conventional orthography for Tunisian Arabic by extending the CODA map to support the dialect. This involves standardizing written forms of Tunisian Arabic, focusing on developing consistent and systematic orthographic rules. The dataset enables researchers to address the challenge of creating a standardized writing system for a dialect that lacks a formal orthographic tradition.	
Tunisian Arabic Corpus	citing_context	CommonVoice version 8	https://doi.org/10.48550/arXiv.2407.04533 (2024)	https://doi.org/10.21437/interspeech.2022-143 (2021)	The CommonVoice version 8 dataset is used to refine the XLS-R-128 model, focusing on cross-lingual speech representation learning across 53 languages. This enhances the model's performance and robustness by leveraging diverse linguistic data, enabling more effective and generalized speech recognition capabilities. The dataset's multilingual nature is crucial for improving the model's ability to handle various languages.	
Tunisian Arabic Corpus	cited_context	elcinema	https://doi.org/10.18653/v1/W17-1307 (2017)	https://doi.org/10.1007/978-3-319-18117-2_2 (2015)	The 'elcinema' dataset is used to build multi-domain resources for sentiment analysis in Arabic, particularly focusing on dialectal variations and multi-standard Arabic (MSA). It includes movie reviews and user comments, enabling researchers to analyze sentiment in diverse Arabic dialects. This dataset facilitates the development of more nuanced and context-specific sentiment analysis models, enhancing the understanding of user opinions and emotions in Arabic-language content.	
Tunisian Arabic Corpus	citing_context	graphemic lexicon of 88k words	https://doi.org/10.48550/arXiv.2205.01987 (2022)	https://doi.org/10.21437/ICSLP.2002-303 (2002)	The graphemic lexicon of 88k words is used to build a 3-gram language model for Tunisian Arabic, focusing on graphemic representations. The dataset employs smoothing techniques to enhance model accuracy. This research specifically addresses the creation of a robust language model for Tunisian Arabic, leveraging the extensive word list to improve graphemic representation and model performance.	
Tunisian Arabic Corpus	cited_context	LDC2022E01	https://doi.org/10.48550/arXiv.2205.01987 (2022)	https://doi.org/10.1109/SLT.2016.7846277 (2016)	The LDC2022E01 dataset is used for dialect adaptation, specifically to enhance model performance in recognizing Tunisian Arabic dialects. It provides Tunisian Arabic data that is crucial for improving the accuracy of speech recognition models tailored to local dialects. This dataset enables researchers to address the challenge of dialectal variation in speech processing, thereby enhancing the robustness and effectiveness of natural language processing systems for Tunisian Arabic.	
Tunisian Arabic Corpus	citing_context	LJSpeech	https://www.semanticscholar.org/paper/8a377a655f14e4e93e342c4ba1960368cac4d524 (2024)	https://doi.org/10.1007/978-3-030-58309-5_22 (2020)	The LJSpeech dataset is used to train and evaluate Text-to-Speech (TTS) models, particularly for low-resource languages like Tunisian Arabic. It serves as a teacher model in transfer learning experiments, enabling researchers to leverage its high-quality speech data to improve TTS performance in less-resourced linguistic contexts. This approach facilitates cross-lingual transfer, enhancing the adaptability and effectiveness of TTS systems in diverse language settings.	
Tunisian Arabic Corpus	cited_context | citing_context	MADAR	https://www.semanticscholar.org/paper/1ddde28f257127f41a816722511a8dedf317d317 (2023), https://doi.org/10.48550/arXiv.2305.14976 (2023)	https://doi.org/10.1184/R1/6373130.V1 (2014)	The MADAR dataset is used to study Tunisian Arabic dialects, particularly focusing on the parallel corpus of Tunis and Sfax within a multidialectal Arabic context. Researchers employ this dataset to analyze linguistic variations and similarities between these regions, contributing to a deeper understanding of dialectal differences and their contextual usage.; The MADAR dataset is used to study Tunisian Arabic dialects, particularly through the analysis of a parallel corpus comparing Tunis and Sfax within a multidialectal Arabic context. It is also employed to detect offensive language in Arabic, focusing on social media content and analyzing linguistic features associated with hate speech. This dataset enables researchers to explore dialectal variations and linguistic markers of offensive content, providing valuable insights into both sociolinguistic and computational linguistics research.	
Tunisian Arabic Corpus	cited_context	MSA database	https://www.semanticscholar.org/paper/1ddde28f257127f41a816722511a8dedf317d317 (2023)	https://www.semanticscholar.org/paper/995ec006ac98a697ea38bd4eea8c1f3170a8adb4 (2020)	The MSA database is used for morphological analysis and root identification in Tunisian Arabic language processing. Researchers employ the dataset to extract roots in Modern Standard Arabic, focusing on the linguistic structures and patterns specific to Tunisian Arabic. This enables detailed morphological studies and enhances language processing techniques.	
Tunisian Arabic Corpus	cited_context	Multi-dialectal Parallel Corpus (MDPC)	https://doi.org/10.48550/arXiv.2305.14976 (2023)	https://doi.org/10.1184/R1/6373130.V1 (2014)	The Multi-dialectal Parallel Corpus (MDPC) is used for machine translation tasks, specifically translating five Arabic dialects and Modern Standard Arabic to English. It facilitates the development and evaluation of translation models, as well as the assessment of ChatGPT's performance on dialectal generation tasks. The dataset's parallel structure supports robust model training and testing, enabling researchers to address challenges in cross-dialectal translation accuracy and fluency.	
Tunisian Arabic Corpus	cited_context | citing_context	OTTA	https://doi.org/10.13053/RCS-90-1-9 (2015), https://www.semanticscholar.org/paper/cdb2ed8b103bfe711b635ba10bcd09d816667ab8 (2013)	https://doi.org/10.1007/978-3-642-37247-6_13 (2013)	The OTTA dataset is used for orthographic transcription of spoken Tunisian Arabic, focusing on converting spoken language into written form to enhance speech-to-text systems. It is employed to analyze conversational speech patterns, aiding in the development and improvement of speech recognition technologies for Tunisian Arabic.; The OTTA dataset is used for orthographic transcription of spoken Tunisian Arabic, providing consistent guidelines for converting spoken language into written form. It facilitates linguistic studies by enabling the analysis of conversational speech, incorporating transcriptions of dialogues between at least two speakers. This dataset supports research focused on improving language processing, understanding, and the study of spoken language characteristics in Tunisian Arabic.	
Tunisian Arabic Corpus	cited_context | citing_context	PADIC	https://doi.org/10.18653/v1/2024.iwslt-1.25 (2024), https://www.semanticscholar.org/paper/a0f99c51f431ca8e6fc1d502840eb6fb4810d17d (2015)	https://www.semanticscholar.org/paper/a0f99c51f431ca8e6fc1d502840eb6fb4810d17d (2015), https://www.semanticscholar.org/paper/ebdcfcd11c8da6e20e4486b65bed5d23f931dc63 (2012)	The PADIC dataset is used to train and evaluate models for text normalization in Dialectal Arabic, particularly addressing the challenge of limited training data by providing multi-dialectal examples. This enables researchers to improve the performance of text normalization systems in handling diverse dialects, enhancing their applicability in natural language processing tasks.; The PADIC dataset is used to compile a corpus of 5 Arabic dialects, each containing 6400 sentences, specifically for machine translation research. This dataset enables researchers to develop and evaluate machine translation models across these dialects, enhancing the accuracy and effectiveness of translation systems. The large, balanced corpus supports robust training and testing methodologies.	
Tunisian Arabic Corpus	citing_context	PREMS corpus	https://doi.org/10.1017/cnj.2018.20 (2018)	https://doi.org/10.1075/z.152.09ch8 (2009)	The PREMS corpus is used to study Tunisian Arabic, focusing on linguistic patterns and structures in spoken language. It supports comparative analysis with other languages and examines linguistic development, particularly the transition from babbling to first words and individual differences. This dataset enables detailed linguistic research through its rich, structured content.	
Tunisian Arabic Corpus	cited_context | citing_context	STAC corpus	https://doi.org/10.1007/s10579-021-09538-4 (2021), https://doi.org/10.1016/J.JKSUCI.2017.01.004 (2017)	https://doi.org/10.13053/RCS-90-1-9 (2015)	The STAC corpus is used to study the spontaneous Tunisian Arabic dialect, focusing on transcription and annotation of spoken data. It is also employed to develop systems for detecting sentence boundaries in transcribed Tunisian Arabic, utilizing specific transcription and annotation methodologies. This dataset enables detailed linguistic analysis and the development of natural language processing tools tailored to the Tunisian Arabic dialect.; The STAC dataset is used for morphological disambiguation and transcription of spoken Tunisian Arabic, focusing on enhancing the accuracy of morphological analysis and linguistic annotation. It is employed to train and test methods, particularly addressing paragraph-level interventions to improve the quality of annotated transcriptions. This dataset enables detailed linguistic research by providing annotated spoken data, facilitating the development and evaluation of morphological analysis tools.	STAC
Tunisian Arabic Corpus	citing_context	TACA-TA testing corpus	https://www.semanticscholar.org/paper/f289447b64f774951b8b4eaf1fd07ecbaf87f506 (2019)	https://doi.org/10.1007/978-3-540-76336-9_3 (2007)	The TACA-TA testing corpus is used to evaluate and compare machine transliteration systems, specifically focusing on performance metrics in Tunisian Arabic transliteration. Researchers employ this dataset to benchmark their proposed systems against existing ones like EiKtub, ensuring rigorous assessment of transliteration accuracy and efficiency. This dataset enables precise and standardized evaluation, facilitating advancements in Tunisian Arabic transliteration technology.	
Tunisian Arabic Corpus	cited_context	TA lexicographic database	https://www.semanticscholar.org/paper/1ddde28f257127f41a816722511a8dedf317d317 (2023)		The TA lexicographic database is used to compile and analyze Tunisian Arabic lexical entries, integrating historical and contemporary sources. It supports linguistic research by providing a searchable Latin and Arabic script-based transcription, enabling corpus-based analysis of language patterns, structures, and usage. This dataset facilitates the development of language encoding systems and the study of linguistic dynamics in the Greater Tunis Area.	
Tunisian Arabic Corpus	cited_context	Tharwa	https://doi.org/10.1109/AICCSA.2017.115 (2017)	https://www.semanticscholar.org/paper/ee8393a4579b04c76275d51dd325b3a5580c7b6e (2014)	The Tharwa dataset is used as a multidialectal, multilingual lexical resource to enhance representation and translation of various Arabic dialects, particularly Tunisian Arabic. It is employed to create comprehensive dictionaries, explore semantic relationships, and find item names across dialects and languages. This dataset enables researchers to improve lexical resources and support linguistic studies focused on Arabic dialects.	
Tunisian Arabic Corpus	cited_context | citing_context	TSAC	https://www.semanticscholar.org/paper/1ddde28f257127f41a816722511a8dedf317d317 (2023)	https://doi.org/10.18653/v1/W17-1307 (2017)	The TSAC dataset is used for sentiment analysis of Tunisian dialects, offering linguistic resources and experimental data to evaluate sentiment classification models. It also serves as a parallel corpus of Tunisian Arabic and Modern Standard Arabic, extracted from social networks, to support translation and cross-lingual research. This dataset enables researchers to develop and test models for both sentiment analysis and translation tasks, leveraging its unique bilingual and social media-derived content.; The TSAC dataset is used for sentiment analysis of Tunisian dialects, providing linguistic resources and experimental data. It supports research by enabling the development and evaluation of models that can accurately analyze sentiments expressed in Tunisian Arabic, thus enhancing natural language processing capabilities for this specific dialect.	
Tunisian Arabic Corpus	citing_context	Tunisian Arabic CODA	https://www.semanticscholar.org/paper/787b37843e0629f936a4b9fe7803321ad12fbacc (2014)	https://www.semanticscholar.org/paper/e8551be410daf4525a26d87f3c867b6db6955076 (2014)	The Tunisian Arabic CODA dataset is used to standardize a web corpus by applying a conventional orthography for Tunisian Arabic, ensuring consistency in writing standards. This process involves methodically aligning diverse textual data to a unified orthographic system, addressing research questions related to linguistic standardization and digital language resources. The dataset's focus on orthographic consistency enables researchers to create more reliable and standardized corpora for further linguistic analysis and applications.	
Tunisian Arabic Corpus	cited_context	Tunisian Arabic Corpus	https://doi.org/10.48550/arXiv.2305.14976 (2023)	https://doi.org/10.18653/v1/2020.fever-1.2 (2020)	The Tunisian Arabic Corpus is used for stance prediction and claim verification in Arabic, particularly focusing on the linguistic and contextual aspects of Tunisian Arabic claims from news and social media. Researchers employ this dataset to predict the stance and factuality of claims, leveraging its rich linguistic data to enhance the accuracy of these predictions.	
Tunisian Arabic Corpus	citing_context	Tunisian Arabic Peace Corps Dictionary	https://doi.org/10.1109/ACLING.2015.7 (2015)		The Tunisian Arabic Peace Corps Dictionary is used as a bilingual English-Tunisian Arabic dictionary to create a Wordnet, expanding lexical resources through an approach similar to EuroWordnet building. This methodology focuses on enhancing lexical coverage and structure, enabling research in computational linguistics and natural language processing, particularly in developing linguistic resources for under-resourced languages like Tunisian Arabic.	
Tunisian Arabic Corpus	cited_context	Tunisian Arabic Treebank	https://doi.org/10.1109/AICCSA.2017.115 (2017)	https://doi.org/10.1109/AICCSA.2013.6616508 (2013)	The Tunisian Arabic Treebank is part of the broader Arabic Treebank project. While its specific usage and research context are not detailed in the provided citations, it is implied to be used within the scope of the larger Arabic Treebank initiative, likely contributing to linguistic and computational linguistics research. However, the exact methodologies, research questions, and specific applications are not explicitly mentioned in the provided descriptions.	
Tunisian Arabic Corpus	citing_context	Tunisian Media Corpus	https://doi.org/10.1007/s10579-021-09538-4 (2021)	https://www.semanticscholar.org/paper/2c14719336ce6aa55db49e6afdcd15066e25d397 (2015)	The Tunisian Media Corpus is used as a linguistic resource for merging with other datasets to study Tunisian Arabic, particularly in media contexts. It supports research in linguistic processing, automatic speech recognition, and dialectal Arabic analysis. The dataset's textual content enables detailed examination of Tunisian Arabic, enhancing understanding and technological applications in these areas.	
Tunisian Arabic Corpus	cited_context	TUN-MSA lexicon	https://doi.org/10.3115/v1/W14-5311 (2014)	https://www.semanticscholar.org/paper/cdb2ed8b103bfe711b635ba10bcd09d816667ab8 (2013)	The TUN-MSA lexicon dataset is used to generate a deverbal lexicon for Tunisian Arabic, specifically focusing on mapping rules to build a lexicon of 1500 verbs with root and verbal pattern pairs. This dataset enables researchers to systematically create and analyze verb structures, enhancing the understanding and processing of Tunisian Arabic verbs.	
Turkmen Corpus	citing_context	Common Voice	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.1109/BigComp57234.2023.00037 (2023)	The Common Voice dataset is used to enhance Turkic Automatic Speech Recognition (ASR) systems and develop machine translation systems for Turkmen, leveraging cross-lingual transfer learning from Kazakh. It also supports the development and evaluation of text-to-speech systems for Turkmen, extending the dataset to over 270 hours of recorded audio. This extensive audio data improves model performance and robustness across multiple Turkic languages.	
Turkmen Corpus	citing_context	Turkmen-ERD	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://www.semanticscholar.org/paper/cf4c9d626facb20c271ca5cbd04af5c0eb06f813 (2021)	The Turkmen-ERD dataset is used for Named Entity Recognition (NER) in the Turkmen language, supporting the development and improvement of NER models. This dataset enables researchers to train and evaluate NER systems, enhancing their performance in identifying and classifying named entities within Turkmen text. The dataset's relevance lies in its application to Turkmen language processing, contributing to the broader field of natural language processing (NLP) for under-resourced languages.	
Turkmen Corpus	citing_context	Turkmen Language Dataset	https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.1109/icisct52966.2021.9670140 (2021)	The Turkmen Language Dataset is used to develop and evaluate concatenative speech synthesizers for the Turkmen language. Researchers focus on improving phonetic accuracy and the naturalness of synthesized speech. This dataset enables the testing and refinement of speech synthesis techniques, ensuring they are effective for Turkmen, a less commonly studied language.	
Turkmen Corpus	citing_context	Turkmen Wikipedia	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.48550/arXiv.2404.04487 (2024)	The Turkmen Wikipedia dataset is used to develop and evaluate Turkmen language processing tools, focusing on the quality and coverage of Turkmen content within a large-scale, multilingual environment. This dataset enables researchers to assess and improve the performance of these tools in handling Turkmen language data, ensuring they are effective in real-world applications.	
Zarma Corpus	citing_context	Feriji Dataset	https://doi.org/10.48550/arXiv.2410.15539 (2024)	https://doi.org/10.48550/arXiv.2406.05888 (2024)	The Feriji Dataset is used to address the scarcity of annotated data in the Zarma language by generating synthetic datasets through custom corruption scripts. It evaluates system performance on linguistic errors by manually corrupting sentences from a French-Zarma parallel corpus and assesses spelling correction accuracy by introducing typographical errors into 3,539 sentences. This enables research on error detection and correction in Zarma, enhancing the development of language processing systems for under-resourced languages.	
