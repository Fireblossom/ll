ID	language	Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Homepage	Note	Label	Details
8664	Sicilian Corpus	citing_context	Mparamu lu sicilianu	https://www.semanticscholar.org/paper/318e3ced09aa6ce5fc6e457768e270a9714eb518 (2021)		The 'Mparamu lu sicilianu' dataset is used to translate homework exercises into Sicilian, English, and Italian, facilitating linguistic analysis and educational applications. It provides authentic and structured language samples, enhancing research on translation accuracy and language bridging. The dataset supports studies in linguistic content and grammatical structures, aiding both academic and pedagogical research.		not a dataset	3	a book
9676	Swahili Corpus	cited_context | citing_context	sawa corpus	https://doi.org/10.48550/arXiv.2211.00046 (2022), https://doi.org/10.1007/S10579-011-9159-7 (2011), https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)	https://doi.org/10.1007/S10579-011-9159-7 (2011), https://doi.org/10.3115/1564508.1564511 (2009)	The SAWA dataset is used to source parallel sentences in Swahili and English, facilitating translation and cross-lingual analysis across various domains. Researchers employ this dataset to enhance translation accuracy and explore linguistic similarities and differences between the two languages, leveraging its parallel structure to support robust cross-lingual studies.; The SAWA corpus is used to compile and analyze comprehensive resources of 21,000 lemmas for English-Swahili translations, supporting linguistic and translation research. It is employed to develop and evaluate machine translation systems, providing a parallel corpus for training and testing models. Additionally, it is used to construct a trilingual parallel corpus (English-Luo-Swahili) for linguistic alignment and computational morphological analysis, leveraging chapter and verse indications for paragraph alignment.	https://aflat.org/	not a dataset	3	cannot find the exactly one dataset
9677	Swahili Corpus	citing_context	sawa corpus	https://doi.org/10.48550/arXiv.2410.14289 (2024)	https://doi.org/10.1007/S10579-011-9159-7 (2011)	The Sawa Corpora dataset is used to develop and evaluate question answering systems in Swahili, leveraging stories in the Swahili language. It also addresses the lack of appropriate data for English-Swahili translation by providing a two-million-word parallel corpus, enabling detailed language pair analysis and enhancing translation models.	https://aflat.org/	not a dataset	3	cannot find the exactly one dataset
9651	Swahili Corpus	cited_context	freedict	https://doi.org/10.18653/v1/2022.naacl-main.293 (2021)	https://www.semanticscholar.org/paper/562c09c112df56c5696c010d90a815d6018a86c8 (2017)	The FreeDict dataset is primarily used to align word embeddings for Swahili, providing bilingual dictionaries that support word translation without parallel data. It is also utilized to align word embeddings for other languages like French, German, and Chinese, with a focus on enhancing Swahili language research. This dataset enables researchers to improve cross-lingual word representation and translation accuracy, particularly in scenarios where parallel corpora are scarce.	https://freedict.org	not a dataset	3	it is a dictionary
5620	Indonesian Corpus	citing_context	Huggingface Community Datasets	https://doi.org/10.14569/ijacsa.2022.0131110 (2022)	https://doi.org/10.18653/v1/N18-1149 (2018)	The Huggingface Community Datasets is a collection of various NLP datasets contributed by the community, utilized for a wide range of natural language processing tasks. Researchers employ these datasets to develop and evaluate NLP models, addressing diverse research questions and applications. The dataset's community-driven nature ensures a broad and dynamic resource, enabling robust testing and validation across multiple NLP domains.		not a dataset	3	it is not a dataset
10	Akan Corpus	cited_context	Twi Bible	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.1023/A:1001798929185 (1999)	The Twi Bible dataset is used as a base text for linguistic studies in the Akan language, particularly in low-resourced and endangered language contexts. Researchers leverage its widespread availability to conduct linguistic analyses, focusing on the preservation and documentation of the Akan language. The dataset's extensive use in these contexts enables detailed linguistic research and supports efforts to maintain linguistic diversity.	-	not a dataset	3	it is the bible itself
9	Akan Corpus	cited_context	JW300	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.18653/v1/P19-1310 (2019)	The JW300 dataset is primarily used to enhance and expand vocabulary in low-resource languages, particularly in the Akan language, by increasing token counts to over 10,000. It serves as a parallel corpus for training and evaluating machine translation and language modeling tasks. Researchers use it to compare the quality and dialect mix between Twi and Yoruba corpora, and to study the Akuampem dialect of Akan. The dataset provides clean, wide-coverage data, crucial for improving translation quality and addressing low-resource language challenges.	https://opus.nlpl.eu/JW300.php	not a dataset	3	JW300 is part of the OPUS project. The OPUS site hosts JW300.
185	Bavarian Corpus	cited_context	Glottolog	https://doi.org/10.18653/V1/2021.NAACL-MAIN.197 (2021)	https://doi.org/10.5617/OSLA.75 (2011)	The Glottolog dataset is used to provide information on language families, particularly for linguistic studies and language documentation. It focuses on linguistic classification, including the Bavarian language family. Researchers use this dataset to support studies involving language documentation and classification, enhancing understanding of linguistic relationships and structures.	https://glottolog.org/	not a dataset	3	Comprehensive reference information for the world's languages, especially the lesser known languages.
768	Cameroon Pidgin Corpus	citing_context	Creole Language Structures (APiCS)	https://doi.org/10.1177/13670069211019126 (2021)	https://doi.org/10.1075/CLL.20 (1999)	The Creole Language Structures (APiCS) dataset is used to investigate linguistic structures in creole languages, particularly to demonstrate the presence of split systems in English- and Portuguese-lexifier contact languages. Researchers employ this dataset to support their analyses of linguistic features, focusing on how these systems emerge and function within creole languages. The dataset's comprehensive coverage of structural data enables detailed comparative studies, enhancing understanding of creole language formation and typology.	https://apics-online.info/	not a dataset	3	not a dataset
767	Cameroon Pidgin Corpus	citing_context	APiCS	https://doi.org/10.1177/13670069211019126 (2021)	https://doi.org/10.5944/ENDOXA.37.2016.16613 (2016)	The APiCS dataset is used to analyze the presence of split systems in English- and Portuguese-lexifier contact languages of West Africa and the Americas. It supports research on linguistic structures and patterns in creole languages, employing a comparative approach to demonstrate the presence of these systems. This dataset enables detailed linguistic analysis by providing structured data on various contact languages.		not a dataset	3	not a dataset
769	Cameroon Pidgin Corpus	citing_context	request strategy types in cameroon Pidgin English	https://doi.org/10.30564/JLER.V1I1.226 (2019)	https://doi.org/10.1093/APPLIN/5.3.196 (1984)	The dataset 'request strategy types in Cameroon Pidgin English' is used to analyze the distribution and frequency of various request strategies, such as explicit performative and locution derivable, within the language. Containing 38 instances, the dataset supports research focused on understanding the linguistic mechanisms and social functions of requests in Cameroon Pidgin English. This analysis helps in identifying the predominant types of request strategies and their contextual usage, providing insights into the communicative norms and pragmatic features of the language.		not a dataset	3	not a dataset
834	Central Kurdish Corpus	citing_context	Kurdish BLARK	https://doi.org/10.48550/arXiv.2501.14528 (2025)	https://doi.org/10.1007/s10579-017-9400-0 (2017)	The Kurdish BLARK dataset is used to provide foundational resources for Kurdish Natural Language Processing (NLP), specifically supporting tokenization for both Sorani and Kurmanji dialects. This enables the development of language models, enhancing the processing and understanding of the Kurdish language in computational linguistics.	https://kurdishblark.github.io/	not a dataset	3	not a dataset
830	Central Kurdish Corpus	cited_context	Dicto	https://doi.org/10.1109/AICCSA.2013.6616470 (2013)	https://doi.org/10.1007/BFb0034731 (1996)	The Dicto dataset is used for cross-language information retrieval, specifically to automatically translate query terms from English to Sorani, a dialect of Central Kurdish. This enables researchers to enhance the accuracy and efficiency of information retrieval systems when dealing with multilingual data. The dataset's focus on precise translation supports the development of more effective cross-language search capabilities.		not a dataset	3	not a dataset
837	Central Kurdish Corpus	citing_context	Kurdish Language Processing Toolkit (KLPT)	https://doi.org/10.37652/juaps.2022.176501 (2022)	https://doi.org/10.1177/0165551516683617 (2018)	The Kurdish Language Processing Toolkit (KLPT) is used for text pre-processing tasks such as tokenization, stemming, transliteration, and lemmatization in Central Kurdish language processing. These functions enhance the toolkit's capabilities for linguistic analysis, enabling researchers to conduct detailed linguistic studies and improve the accuracy of language processing systems for Central Kurdish.	https://github.com/sinaahmadi/klpt	not a dataset	3	not a dataset
1976	Dholuo Corpus	cited_context	Crúbadán project archive	https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)	https://www.semanticscholar.org/paper/ef9f81a662aa9618173d033e95d46d74e14aeafc (2007)	The Crúbadán project archive is used to access a small web-mined corpus for the Dholuo language, supporting research on under-resourced languages. This dataset enables researchers to study linguistic features and develop computational tools for languages with limited digital resources. The corpus provides essential data for analyzing and preserving under-documented languages.	https://kevinscannell.com/files/wac3.pdf	not a dataset	3	not a dataset
2140	Eastern Punjabi Corpus	citing_context	FIRE dataset	https://doi.org/10.48550/arXiv.2312.09508 (2023)	https://www.semanticscholar.org/paper/43f2ad297941db230c089ba353efc3f281ab678c (2020)	The FIRE dataset is mentioned in research citations but lacks detailed descriptions of its usage, methodology, or specific research applications. There is no evidence to suggest it is used for Eastern Punjabi language research or any other specific area. Its role and impact in research are not clearly defined based on the provided information.	https://link.springer.com/chapter/10.1007/978-3-642-40087-2_1	not a dataset	3	not a dataset
4283	Ghanaian Pidgin English Corpus	citing_context	Atlas of Pidgin and Creole Language Structures (APiCS)	https://doi.org/10.1177/13670069211019126 (2021)	https://doi.org/10.5944/ENDOXA.37.2016.16613 (2016)	The Atlas of Pidgin and Creole Language Structures (APiCS) is used to analyze split systems in English- and Portuguese-lexifier contact languages of West Africa and the Americas. It provides structured linguistic data for comparative studies, enabling researchers to demonstrate the presence and distribution of specific linguistic structures across these regions. This dataset facilitates detailed analyses of linguistic features and their variations, supporting research into the structural complexities of pidgin and creole languages.	https://apics-online.info/	not a dataset	3	not a dataset
4285	Ghanaian Pidgin English Corpus	citing_context	GhaP	https://doi.org/10.1177/13670069211019126 (2021)	https://www.semanticscholar.org/paper/3ee12dec281cc490e6b626076a2fc8402a7dcd24 (1958)	The GhaP dataset is used to analyze the polysemy of 'make-cop.nfact' in Ghanaian Pidgin English, comparing its usage with other languages in the corpus. This involves examining linguistic data to understand semantic variations and cross-linguistic similarities, providing insights into the structure and function of Ghanaian Pidgin English.		not a dataset	3	not a dataset
4286	Ghanaian Pidgin English Corpus	cited_context	GloVe	https://www.semanticscholar.org/paper/880c3a51450b9a4c9f3c0de44352b2ae0f8a3e63 (2019)	https://doi.org/10.3115/v1/D14-1162 (2014)	The GloVe dataset is used to initialize word embeddings for Pidgin, providing global context to word vectors before fine-tuning on a Pidgin corpus. This approach helps in enhancing the semantic richness of the embeddings, which are then utilized in natural language processing tasks specific to the Pidgin language.		not a dataset	3	not a dataset
4479	Hausa Corpus	citing_context	HausaNLP Catalogue	https://doi.org/10.48550/arXiv.2505.14311 (2025)	https://doi.org/10.58414/scientifictemper.2024.15.3.27 (2024)	The HausaNLP Catalogue serves as a centralized repository to enhance accessibility and advance Hausa NLP research. It compiles datasets, tools, and research papers, facilitating the development and evaluation of NLP models and methodologies specific to the Hausa language. This resource supports researchers in addressing challenges and advancing the state-of-the-art in Hausa language processing.	https://catalog.hausanlp.org/	not a dataset	3	not a dataset
5539	Indonesian Corpus	citing_context	ALFFA	https://doi.org/10.1109/ASRU57964.2023.10389730 (2023)	https://www.semanticscholar.org/paper/7b07ef9ba4419314fc3a569d9b63361ed87932f7 (2020)	The ALFFA dataset is utilized for Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) tasks, providing multilingual speech corpora, particularly for under-resourced languages. It offers diverse, crowd-sourced, and high-quality speech data, enabling the training and evaluation of ASR and TTS models. The dataset is also applied to evaluate universal speech representations in few-shot learning scenarios, focusing on low-resource languages.		not a dataset	3	not a dataset
5602	Indonesian Corpus	citing_context	Gojali’s automatic summarization test dataset	https://doi.org/10.1109/ICAICTA59291.2023.10390436 (2023)		The Gojali’s automatic summarization test dataset is used to evaluate the performance of Gojali’s automatic summarization system, specifically focusing on summarization accuracy and coherence. It consists of 736 sentences and is employed to assess how well the system generates summaries that are both accurate and coherent. This dataset enables researchers to systematically test and improve the summarization capabilities of the system.		not a dataset	3	not a dataset
5724	Indonesian Corpus	citing_context	Katadata Insight Center 2020 survey	https://doi.org/10.1109/ICISS59129.2023.10291599 (2023)		The Katadata Insight Center 2020 survey is used to analyze the prevalence and recognition of online hoaxes among Indonesian internet users. It provides quantitative data on user awareness and behavior, enabling researchers to assess how effectively users identify and respond to misinformation. This dataset supports studies focusing on digital literacy and the impact of online hoaxes in Indonesia.		not a dataset	3	not a dataset
5548	Indonesian Corpus	cited_context	big dictionary of Indonesian	https://doi.org/10.1109/ICOMITEE.2019.8920923 (2019)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003)	The 'big dictionary of Indonesian' dataset is used for sentiment analysis and information retrieval in Indonesian. It provides 3,587 entries for positive and negative word references and 28,533 entries for root word data. This dataset supports research in understanding and processing Indonesian text by enabling accurate sentiment classification and enhancing information retrieval systems.		not a dataset	3	it is a dictionary
5681	Indonesian Corpus	cited_context	Indonesian summarisation dataset	https://doi.org/10.1080/09540091.2021.1937942 (2021)	https://aclanthology.org/L16-1129 (2016)	The Indonesian summarisation dataset is used to advance research in Indonesian text summarization by providing a publicly accessible resource. It addresses the limitation of previous datasets being proprietary, offering a chat dataset with two summary versions. This enables broader access and development in the field, facilitating the creation and evaluation of summarization models.		not a dataset	3	not a dataset
5767	Indonesian Corpus	citing_context	NusaCatalogue	https://doi.org/10.48550/arXiv.2207.10524 (2022)	https://www.semanticscholar.org/paper/96b8611c3e8d887362df292522debe2c3cfa1def (2021)	The NusaCatalogue dataset serves as a public datasheet catalogue website, listing and organizing datasets collected in NusaCrowd. It provides a comprehensive resource for researchers and practitioners, focusing on the Indonesian language. This dataset supports documentation and accessibility, facilitating research in NLP and linguistics by offering a centralized and organized collection of relevant datasets.	https://indonlp.github.io/nusa-catalogue/	not a dataset	3	not a dataset
7189	Marathi Corpus	cited_context	Health and Tourism domain sense-tagged corpus of Hindi and Marathi languages	https://www.semanticscholar.org/paper/8c9ff9dcdcec41f3c1f967b9ca63074ba6381a58	https://www.semanticscholar.org/paper/887d8d4bae488941d9a3a26ec39122abc84fad79 (2010)	The 'Health and Tourism domain sense-tagged corpus of Hindi and Marathi languages' is used for studying sense tagging in these domains, focusing on linguistic analysis and annotation for both Hindi and Marathi. It also serves as a raw, untagged resource for Hindi, supporting natural language processing tasks. This dual use highlights its versatility in enhancing understanding and processing of domain-specific language data.	http://sanskrit.jnu.ac.in/ilci/	not a dataset	3	not a dataset
7209	Marathi Corpus	citing_context	iNLTK headlines	https://doi.org/10.1007/978-3-031-58495-4_4 (2024)	https://doi.org/10.18653/v1/2020.nlposs-1.10 (2020)	The iNLTK headlines dataset is used for classification tasks, including hate speech detection and sentiment analysis, specifically focusing on Marathi text from news articles and various sources. It enables researchers to develop and evaluate models that can accurately classify and identify specific types of content in Marathi language datasets.	https://github.com/goru001/inltk	not a dataset	3	not a dataset
7214	Marathi Corpus	cited_context | citing_context	L3Cube-MahaNLP	https://doi.org/10.48550/arXiv.2306.14030 (2023), https://doi.org/10.48550/arXiv.2211.11418 (2022), https://doi.org/10.48550/arXiv.2506.00863 (2025)	https://doi.org/10.48550/arXiv.2203.13778 (2022), https://doi.org/10.1007/978-981-19-6088-8_56 (2022)	The L3Cube-MahaNLP dataset is used for various NLP tasks in the Marathi language, including hate speech detection, named entity recognition, and multi-domain sentiment analysis. It supports the training and evaluation of language models, particularly transformer models, and provides a comprehensive collection of datasets, models, and libraries. This enables researchers to address classification and sentiment analysis tasks, as well as compare monolingual and multilingual model performance.; The L3Cube-MahaNLP dataset is used to develop and evaluate monolingual models for Marathi, focusing on named entity recognition tasks. It enables researchers to compare the performance of these monolingual models against multilingual models, providing insights into the effectiveness of language-specific approaches in natural language processing.	https://github.com/l3cube-pune/MarathiNLP	not a dataset	3	not a dataset
7219	Marathi Corpus	cited_context | citing_context	MahaCorpus	https://doi.org/10.48550/arXiv.2205.14728 (2022)	https://www.semanticscholar.org/paper/7ebbb9f14a08fda8d76b3f299254c2b0d2c59d9a (2022)	The MahaCorpus dataset, comprising 24.8M sentences and 289M tokens, is used to develop and train Marathi BERT language models and support other NLP tasks. It provides a large-scale monolingual corpus essential for language modeling, enabling researchers to enhance the performance and accuracy of Marathi language processing systems through robust training and evaluation.; The MahaCorpus dataset, comprising 24.8M sentences and 289M tokens, is used to develop and evaluate Marathi BERT language models. It serves as a large monolingual corpus, supporting research in Marathi language processing and modeling. This dataset enables the training and evaluation of advanced language models, enhancing their performance and applicability in the Marathi language.	https://github.com/l3cube-pune/MarathiNLP	not a dataset	3	not a dataset
7347	Nepali Corpus	cited_context	Nepali OSCAR	https://doi.org/10.1109/ASONAM49781.2020.9381292 (2020)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019)	The Nepali OSCAR dataset is primarily used to train multilingual embeddings, enhancing representation in low-resource settings like Nepali. It complements Nepali data with a large corpus of English text, extracted from 169,899 recent articles, which aids in training and evaluation. This approach improves the quality and robustness of multilingual models, particularly for underrepresented languages.	https://language-resources-nepal.github.io/data	not a dataset	3	not a dataset
7403	Northeastern Thai Corpus	citing_context	Northeastern (NE) dialect dictionary	https://doi.org/10.17507/tpls.1411.05 (2024)	https://doi.org/10.1017/9781787445086.003 (2019)	The Northeastern (NE) dialect dictionary is used to document sound changes in Pali and Sanskrit loanwords within the Northeastern Thai dialect. This linguistic resource facilitates the study of dialectal variations and historical borrowings, enabling researchers to analyze the evolution and integration of these loanwords into the dialect. The dataset provides a structured reference for linguistic analysis, supporting research into the phonological and lexical aspects of the Northeastern Thai dialect.		not a dataset	3	not a dataset
7425	Northern Sotho Corpus	citing_context	Northern Sotho dictionary (Ziervogel and Mokgokong 1985)	https://doi.org/10.5788/24-1-1254 (2014)		The Northern Sotho dictionary (Ziervogel and Mokgokong 1985) is used to create a structured data collection for the Northern Sotho language. Researchers employ OCR technology to digitize and transform the printed dictionary into a digital format, enabling easier access and manipulation of linguistic data. This process facilitates the study and preservation of the Northern Sotho language by providing a digital resource for linguistic analysis and educational purposes.		not a dataset	3	not a dataset
7427	Northern Sotho Corpus	citing_context	Oxford bilingual school dictionary: Northern Sotho and English	https://www.semanticscholar.org/paper/3a0cd6f8f197ace84ecf4652448db8dc959621cf (2016)		The Oxford bilingual school dictionary: Northern Sotho and English is used to analyze Northern Sotho consonant clusters, identifying additional possible clusters not previously documented. The dictionary serves as a comprehensive lexical resource, enabling detailed linguistic research through its extensive vocabulary and phonetic information. This dataset facilitates the exploration of phonological patterns and contributes to the understanding of Northern Sotho's phonetic structure.		not a dataset	3	not a dataset
7428	Northern Sotho Corpus	citing_context	PPVT–4 Form A	https://doi.org/10.1080/02702711.2023.2276444 (2023)	https://doi.org/10.4102/RW.V6I1.77 (2015)	The PPVT–4 Form A dataset is used to assess receptive vocabulary in Northern Sotho-English bilingual children. Researchers translate the English version into Northern Sotho to evaluate vocabulary comprehension in both languages, focusing on the linguistic abilities and cognitive development of bilingual children. This translation approach ensures cultural and linguistic relevance, enabling accurate assessment of vocabulary skills in a bilingual context.		not a dataset	3	not a dataset
7429	Northern Sotho Corpus	cited_context	PUKU 2	https://doi.org/10.1093/IJL/ECP009 (2009)	https://doi.org/10.1080/02572117.2000.10587438 (2000)	The PUKU 2 dataset is used to apply rule-based methods for information retrieval in the Northern Sotho language. It specifically aims to reduce redundancy and support inexperienced learners by enhancing the accuracy and efficiency of information retrieval systems. This dataset enables researchers to develop and test algorithms that improve the educational experience for Northern Sotho speakers.		not a dataset	3	not a dataset
10903	Uyghur Corpus	cited_context	DARPA LORELEI corpus	https://doi.org/10.18653/v1/D19-1143 (2019)	https://doi.org/10.1007/s10590-017-9212-4 (2018)	The DARPA LORELEI corpus is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. Therefore, based on the provided evidence, it cannot be accurately described how this dataset is used in research.	https://www.darpa.mil/research/programs/low-resource-languages-for-emergent-incidents	not a dataset	3	not a dataset
5609	Indonesian Corpus	citing_context	Handling Imbalanced Dataset on Hate Speech Detection in Indonesian Online News Comments	https://doi.org/10.1109/SIML65326.2025.11081166 (2025)	https://doi.org/10.1109/ICoICT55009.2022.9914883 (2022)	The dataset 'Handling Imbalanced Dataset on Hate Speech Detection in Indonesian Online News Comments' is used to address the imbalance in hate speech detection, particularly focusing on underrepresented hate speech data in Indonesian online news comments. Researchers employ methodologies aimed at improving the detection of minority classes, enhancing the model's ability to identify hate speech in imbalanced datasets. This enables more accurate and fair hate speech detection in online platforms, contributing to safer digital environments.		not a dataset	3	not dataset
5610	Indonesian Corpus	citing_context	hashtag distribution dataset	https://doi.org/10.48550/arXiv.2310.11258 (2023)	https://www.semanticscholar.org/paper/e705a87b501e0d6f50c43b1d042cc1ef7bf29de3 (2021)	The hashtag distribution dataset is used to construct a sentiment classification task, focusing on the distribution of hashtags to understand their role in sentiment analysis. Researchers employ this dataset to analyze how hashtags influence sentiment, using methods that examine the frequency and context of hashtag usage. This enables insights into the nuanced ways hashtags shape public opinion and emotional content in social media.		not a dataset	3	not dataset
5802	Indonesian Corpus	citing_context	Sastrawi stopword list	https://doi.org/10.34148/teknika.v14i1.1207 (2025)	https://doi.org/10.17977/um018v3i22020p106-111 (2020)	The Sastrawi stopword list is used to filter out common Indonesian stop words in text processing, which enhances the accuracy of natural language analysis. This dataset is employed in methodologies that require cleaning and preprocessing of Indonesian text data, improving the quality and relevance of subsequent analyses. It is particularly useful in research focusing on natural language processing tasks, such as sentiment analysis, topic modeling, and text classification, where removing stop words is crucial for reducing noise and improving model performance.		not a dataset	3	not dataset
5813	Indonesian Corpus	citing_context	Spoken Wikipedia Corpus (swc)	https://doi.org/10.1109/ASRU57964.2023.10389730 (2023)	https://doi.org/10.18653/v1/2021.eacl-main.96 (2021)	The Spoken Wikipedia Corpus (swc) is used for Text-to-Speech (TTS) tasks, providing spoken versions of Wikipedia articles in multiple languages. Researchers use this dataset to train and evaluate TTS systems, leveraging its multilingual content to enhance system performance and robustness across different languages. The dataset's extensive and diverse text corpus enables the development of more natural and accurate speech synthesis models.	https://dl.acm.org/doi/abs/10.1007/s10579-017-9410-y	not a dataset	3	not dataset
5815	Indonesian Corpus	cited_context	Indonesian Stemmer	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003)	The dataset is used to study the impact of stemming on information retrieval in Indonesian, specifically focusing on the effectiveness of the Porter Stemmer algorithm. Research employs this dataset to evaluate how stemming improves search accuracy and efficiency, addressing questions related to the performance and applicability of the Porter Stemmer in the Indonesian language context.		not a dataset	3	not dataset
9646	Swahili Corpus	citing_context	D Swatok	https://doi.org/10.1109/BigComp60711.2024.00067 (2024)	https://doi.org/10.18653/v1/2022.naacl-main.23 (2022)	The D Swatok dataset is used for tokenizer training in the SwahBERT project, focusing on the Swahili language. It is compiled from news websites, Wikipedia, and forums, providing a diverse corpus for enhancing the tokenizer's performance. This dataset enables researchers to improve the accuracy and efficiency of natural language processing tasks specific to Swahili.		not a dataset	3	this is not a dataset
10206	Tiv Corpus	citing_context	Icighan Bibilo (2007)	https://doi.org/10.34256/ijll2111 (2021)		The Icighan Bibilo (2007) dataset is used to document and analyze graphological and lexical changes in the Tiv language, focusing on phonetic and orthographic modifications. It serves as a baseline to compare with the 2017 edition, enabling researchers to highlight and study changes in Tiv grammar and vocabulary over time. This dataset facilitates longitudinal linguistic analysis by providing a reference point for identifying and examining linguistic evolution.		not a dataset	3	this is not a dataset
8671	Sindhi Corpus	cited_context	multi-domain NH, ST, FN tweet corpus		https://aclanthology.org/2021.wassa-1.20/ (2021)	The multi-domain NH, ST, FN tweet corpus is used for sentiment subjectivity analysis (SSA). It is annotated using Doccano and evaluated with machine learning models such as SVM, LSTM, BiLSTM, and CNN. This dataset enables researchers to analyze and classify tweets based on their sentiment and subjectivity across various domains, providing insights into public opinion and emotional content in social media data.	https://github.com/AliWazir/SdSenti-lexicon	not found	0	404
8675	Sindhi Corpus	citing_context	Sindhi language dataset	https://doi.org/10.26615/issn.2603-2821.2021_004 (2021)		The Sindhi language dataset is mentioned in various citation contexts but lacks detailed descriptions of its usage, methodology, or specific research applications. Therefore, there is insufficient evidence to provide a comprehensive account of how this dataset is actually used in research.	https://github.com/AliWazir/SiPOS-Dataset	not found	0	404
8682	Sindhi Corpus	cited_context | citing_context	SiPOS	https://doi.org/10.1109/ACCESS.2025.3576853 (2025), https://doi.org/10.26615/issn.2603-2821.2021_004 (2021), https://doi.org/10.48550/arXiv.2408.15720 (2024), https://doi.org/10.1145/3457682.3457718 (2021)	https://www.semanticscholar.org/paper/ff9d2c6fa90e51c6795d7ed56bb5bdadcee82085 (2021)	The SiPOS dataset is primarily used for evaluating and developing part-of-speech (POS) tagging models in the Sindhi language. It serves as a benchmark for comparing the performance of various NLP models and tools, particularly in POS tagging and sequence labeling tasks. The dataset includes annotated data, which is utilized for training and evaluating these models. Additionally, it has been used for syntactic parsing and sentiment analysis of Sindhi tweets, employing supervised machine learning techniques. The dataset's annotated corpus enables researchers to improve and refine language processing models for the Sindhi language.; The SiPOS dataset is primarily used for part-of-speech (POS) tagging and named entity recognition (NER) experiments in the Sindhi language, following the suggested standard split. It provides a benchmark for evaluating the performance of POS tagging and NER models in a low-resource setting. The dataset includes annotated news corpora from Awami-Awaz and Kawish Sindhi newspapers, containing over 293K tokens, which supports research in Sindhi language processing. Additionally, it has been used to evaluate sequence classification models using a Conditional Random Field (CRF).	https://github.com/AliWazir/SiPOS-Dataset	not found	0	404
7528	Odia Corpus	citing_context	ODDB	https://www.semanticscholar.org/paper/0c10ef5c9c74ea81ae7bf012bf212c7b3b2379de (2016)		The ODDB dataset is primarily used for training and testing models focused on recognizing Odia language data, particularly in character recognition. It is employed to evaluate the performance and accuracy of character recognition algorithms and machine learning models, with a focus on feature extraction and classification accuracy for Odia handwritten characters.	https://researchportal.helsinki.fi/en/publications/oddb-an-interactive-database-for-clinical-and-translational-resea	not found	0	cannot find. 
9686	Swahili Corpus	cited_context	TshwaneDJe Kiswahili Internet Corpus	https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)		The TshwaneDJe Kiswahili Internet Corpus is used to train a Swahili language model, leveraging a twenty million word corpus to enhance language understanding and generation capabilities. This dataset enables researchers to develop more sophisticated models for processing and generating Swahili language content, focusing on improving the accuracy and fluency of language models in this specific linguistic context.		not found	0	cited article is "TshwaneDJe Kiswahili internet corpus. GM de Schryver, D Joffe, 2009", but the paper's full text is not found.
8637	Setswana Corpus	citing_context	JW300 bible	https://doi.org/10.55492/dhasa.v3i03.3822 (2021)	https://doi.org/10.18653/v1/P19-1310 (2019)	The JW300 bible dataset is used to train word vectors for low-resource languages, leveraging its biblical-domain parallel corpus to enhance cross-lingual representation. This methodology specifically supports research in improving linguistic models for languages with limited resources, enabling better cross-lingual understanding and representation.	https://zeljkoagic.github.io/	not found	0	data no longer available
5641	Indonesian Corpus	cited_context | citing_context	Indo_MultiModal_LAION	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df (2021)	The Indo_MultiModal_LAION dataset is used for vision-language pretraining in Indonesian, containing 70M image-text pairs translated from LAION-400M. It focuses on enhancing multilingual capabilities and improving multimodal understanding in the Indonesian language. The dataset enables researchers to develop models with better performance in vision-language tasks, specifically tailored for Indonesian content.; The Indo_MultiModal_LAION dataset is used for vision-language pretraining in Indonesian, containing 70M image-text pairs translated from LAION-400M. It focuses on improving multimodal understanding and multilingual capabilities in vision-language models, particularly in the Indonesian language. The dataset is filtered by CLIP, enhancing its utility for large-scale, multilingual or cross-lingual research.		not found	0	no longer avaliable
5751	Indonesian Corpus	cited_context | citing_context	Multilingual Open Relations	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.3115/v1/N15-1151 (2015)	The Multilingual Open Relations dataset is used for multilingual open relation extraction, providing a set of automatically extracted relations through cross-lingual annotation projection. It enables researchers to focus on extracting relations across multiple languages, enhancing the scope and applicability of relation extraction models in multilingual contexts.; The Multilingual Open Relations dataset is used for multilingual open relation extraction, providing automatically extracted relations through a cross-lingual annotation projection method. It enables researchers to explore and analyze relational data across multiple languages, facilitating the development and evaluation of multilingual relation extraction systems. The dataset's key feature is its cross-lingual capability, which supports the extraction of relations in various languages, enhancing the robustness and applicability of natural language processing models.	https://aclanthology.org/N15-1151.pdf	not found	0	no longer avaliable
5753	Indonesian Corpus	cited_context | citing_context	multilingual paraphrase pairs dataset	https://doi.org/10.48550/arXiv.2205.04651 (2022)	https://www.semanticscholar.org/paper/fee52611a082ab136cfd03bd73b838a2e6db20c9 (2014)	The multilingual paraphrase pairs dataset is used to explore phrase-level paraphrases across multiple languages, examining the limitations and potential of such data for multilingual NLP tasks. Researchers employ this dataset to analyze how phrase-level data can enhance or constrain cross-lingual understanding and processing, focusing on the nuances and challenges in multilingual contexts. This dataset enables detailed investigations into the effectiveness of paraphrase detection and generation across languages, providing insights into the robustness and adaptability of NLP models in multilingual settings.; The multilingual paraphrase pairs dataset is used to explore phrase-level paraphrases across languages, examining the limitations and potential of phrase-based multilingual data in natural language processing tasks. Researchers employ this dataset to analyze cross-lingual phrase equivalences, enhancing understanding of multilingual NLP models and their performance on paraphrase detection and generation.	https://callison-burch.github.io/publications/ppdb-multilingual.pdf	not found	0	not avaliable
5722	Indonesian Corpus	citing_context	kamus dari Taudata Analytics	https://doi.org/10.35912/jisted.v1i1.1509 (2022)		The 'kamus dari Taudata Analytics' dataset is used to normalize and standardize Indonesian language data by replacing slang and abbreviations in text processing. This aids in improving the accuracy and consistency of language analysis, particularly in contexts where informal language use is prevalent. The dataset's focus on slang and abbreviations makes it a valuable resource for enhancing the quality of text data in various linguistic and computational research applications.		not found	0	not exist
5674	Indonesian Corpus	cited_context	Indonesian phonetically balanced speech corpus	https://doi.org/10.5614/ITBJ.ICT.RES.APPL.2014.8.2.2 (2014)	https://doi.org/10.4156/IJIIP.VOL3.ISSUE4.7 (2012)	The Indonesian phonetically balanced speech corpus, comprising 44,000 utterances from 400 speakers, is used for speech recognition experiments to evaluate model performance on phonetically diverse data. It is also utilized to adapt and test a segmentation method originally designed for Farsi, specifically focusing on clean speech to assess its effectiveness on Indonesian speech. This dataset's phonetic balance and clean quality enable robust evaluation and adaptation of speech processing techniques.		not found	0	not exist
5727	Indonesian Corpus	cited_context	dataset komentar	https://www.semanticscholar.org/paper/68068fd11c0fe5c82afa50a6dcfb3756f07e1bd7 (2020)	https://doi.org/10.2200/s00416ed1v01y201204hlt016 (2012)	The 'dataset komentar' is used to identify opinion polarization among citizens through sentiment analysis, focusing on extracting information from unstructured data. This dataset enables researchers to analyze public opinions and sentiments, providing insights into how different groups perceive various topics. The methodology involves applying sentiment analysis techniques to understand the nuances and polarizations within the dataset.		not found	0	not exist
5731	Indonesian Corpus	cited_context	korpus	https://www.semanticscholar.org/paper/7562204c80b95b7d5dfb3d1138fda76d903e7e41 (2019)	https://doi.org/10.1007/978-3-319-67217-5_26 (2017)	The 'korpus' dataset is used in Indonesian language research for developing and annotating tweet data for sentiment analysis. It also supports the creation and validation of a lexicon of sentiment-bearing terms. This dataset enables researchers to build and refine tools for analyzing sentiments in social media content, enhancing the accuracy and reliability of sentiment detection in Indonesian.		not found	0	not exist
5817	Indonesian Corpus	cited_context	stop words dictionary	https://doi.org/10.1088/1742-6596/1008/1/012011 (2018)		The stop words dictionary dataset is used to enhance text processing in movie review analysis by filtering out less important words, improving efficiency and relevance. It also aids in obtaining root words for Indonesian movie reviews, thereby enhancing the accuracy of text analysis. This dataset supports methodologies focused on refining and optimizing text data for more precise sentiment and content analysis.		not found	0	not exist
5834	Indonesian Corpus	citing_context	tweet dataset of tweeter users in Indonesia	https://doi.org/10.23919/EECSI56542.2022.9946530 (2022), https://doi.org/10.1145/3625821 (2023)	https://doi.org/10.1109/ICAICTA.2015.7335365 (2015)	The tweet dataset of Twitter users in Indonesia is used to enhance the accuracy of word meanings related to locations and events during the COVID-19 pandemic and to build a corpus for training datasets focused on Indonesian language usage. It is also utilized to classify Indonesian Instagram posts as hate speech or not, employing linguistic pattern analysis and context-specific indicators. This dataset supports research in natural language processing and social media content analysis, specifically addressing the nuances of the Indonesian language and context.		not found	0	not exist
5835	Indonesian Corpus	cited_context	tweet pengguna menggunakan Bahasa Indonesia dengan kata kunci “Ibu Kota Nusantara”	https://doi.org/10.29408/edumatic.v8i1.25667 (2024)	https://doi.org/10.55338/jikomsi.v7i1.2846 (2024)	The dataset 'tweet pengguna menggunakan Bahasa Indonesia dengan kata kunci “Ibu Kota Nusantara”' is used to analyze public sentiment regarding the new capital city of Indonesia. Researchers employ K-Nearest Neighbors (KNN) and Naïve Bayes algorithms to classify tweets, focusing on public opinion and reactions. The dataset is collected through keyword-based crawling, enabling quantitative analysis of social media reactions and opinions about Ibu Kota Nusantara.		not found	0	not exist
5838	Indonesian Corpus	citing_context	Twitter Data	https://doi.org/10.31358/techne.v23i1.446 (2024)	https://doi.org/10.1109/ISRITI54043.2021.9702784 (2021)	The Twitter Data dataset is used to analyze public sentiment towards Indonesian government policies during the COVID-19 pandemic. Researchers employ Twitter as a real-time source of public opinion, leveraging the dataset to capture and assess immediate reactions and attitudes. This approach helps in understanding the public's response to policy changes and health measures, providing insights into the effectiveness and acceptance of governmental actions.		not found	0	not exist
7232	Marathi Corpus	cited_context	MarathiNLI	https://doi.org/10.48550/arXiv.2204.08776 (2022)	https://doi.org/10.18653/v1/N18-1101 (2017)	The dataset 'MarathiNLI' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for any particular research purpose or methodology based on the provided evidence.		not found	0	not exist
7244	Marathi Corpus	cited_context	MID-197	https://doi.org/10.48550/arXiv.2204.08776 (2022)	https://www.semanticscholar.org/paper/92343cecdc990380de362b969eec60081959f507 (2019)	The MID-197 dataset is used for training and evaluating monolingual contextualized word embeddings for the Marathi language, particularly to improve performance in low-resource and mid-resource settings. It is also utilized for pre-training MuRIL on multiple Indian languages, including Marathi, to develop robust contextualized word embeddings. This dataset enables researchers to enhance the linguistic understanding and representation capabilities of models in the Marathi language.		not found	0	not exist
7255	Marathi Corpus	cited_context	optimize_prime	https://doi.org/10.1145/3574318.3574326 (2022)	https://doi.org/10.48550/arXiv.2212.10039 (2022)	The 'optimize_prime' dataset is used to enhance offensive language detection in Marathi by augmenting training data, thereby improving model performance. This dataset specifically supports machine learning approaches aimed at detecting offensive content, contributing to more effective and accurate models in this domain.		not found	0	not exist
11221	Yoruba Corpus	citing_context	YORLEXVE	https://doi.org/10.32473/ysr.8.1.134096 (2023)		The YORLEXVE dataset is used to represent Yorùbá lexical verbs in a lexicon module for linguistic analysis. It supports the study of Yorùbá parts of speech, particularly verbs, enabling detailed linguistic research. While its potential use in NLP applications is noted, specific methodologies and research contexts beyond linguistic analysis are not explicitly detailed.	http://purl.org/net/yorno_and_yorlexverb	not found	0	not exist
5	Akan Corpus	citing_context	Akan	https://doi.org/10.48550/arXiv.2502.10973 (2025), https://doi.org/10.1016/j.dib.2025.111880 (2025)	https://doi.org/10.18653/v1/P19-1050 (2018)	The Akan dataset is used to assess inter-annotator agreement for emotion recognition, ensuring reliable annotations in the Akan language. It is also utilized to explore linguistic aspects of Akan, particularly in the context of financial inclusion, using an audio dataset. These applications focus on enhancing the reliability and understanding of Akan language data in specific social and emotional contexts.		not found	0	not found
11	Akan Corpus	citing_context	wage Data from Akan	https://www.semanticscholar.org/paper/e3b567a6f6121fc1e9325d570ac10159ef3bd06b (2025)		The 'wage Data from Akan' dataset is used to develop and evaluate Akan speech models, particularly for financial applications. It focuses on enhancing language resources and improving speech recognition systems by incorporating specific financial vocabulary and context. The dataset addresses the impact of recording conditions, dialect variations, and acoustic environments on model performance, thereby improving the robustness and accuracy of Akan speech models in real-world scenarios.		not found	0	not found
153	Assamese Corpus	cited_context | citing_context	Indian Languages Corpora Initiative (ILCI)	https://doi.org/10.1109/AICCSA56895.2022.10017934 (2022)		The Indian Languages Corpora Initiative (ILCI) dataset is used to preprocess and prepare part-of-speech (POS) tagged data for training, enhancing the accuracy and quality of POS tagging in the Assamese language. It involves manual tagging by language experts to create a high-quality linguistic resource, which is then used to train and evaluate machine learning models through multiple rounds of experiments. This dataset enables researchers to improve the usability and performance of linguistic tools for the Assamese language.; The Indian Languages Corpora Initiative (ILCI) dataset is used to enhance the quality and performance of models in the Assamese language domain. It involves preprocessing and preparing part-of-speech tagged data, focusing on improving data structure and accuracy. Manual tagging by language experts ensures high-quality linguistic resources, enabling robust training and evaluation of machine learning models for tasks such as POS tagging.	http://sanskrit.jnu.ac.in/ilci	not found	0	not found
165	Assamese Corpus	cited_context	parallel transliteration corpora	https://doi.org/10.48550/arXiv.2205.03018 (2022)	https://doi.org/10.1145/2629489 (2014)	The 'parallel transliteration corpora' dataset is used to mine transliteration pairs from Wikidata, specifically focusing on the Assamese language. It is employed to analyze the accuracy of transliteration tasks and to identify cross-lingual name equivalences, enhancing research in Assamese language transliteration and cross-lingual information retrieval.		not found	0	not found
169	Assamese Corpus	citing_context	Wikipedia dataset	https://doi.org/10.48550/arXiv.2410.11291 (2024)		The Wikipedia dataset is used to obtain textual content for research, leveraging the comprehensive and diverse information available in the 2021 Wikimedia dumps. This dataset provides a rich source of data for various research questions, enabling studies that require extensive and varied textual information. The dataset's broad coverage and detailed content facilitate in-depth analysis and support methodologies that rely on large-scale, high-quality textual data.		not found	0	not found
719	Bhojpuri Corpus	citing_context	LIMMITS	https://www.semanticscholar.org/paper/3a67a93afc0b0c3caf580ebb73d66d9a1b575c21 (2025)	https://syspin.iisc.ac.in/ (2025)	The LIMMITS dataset is used to train monolingual Bhojpuri models, focusing on enhancing speech recognition and language modeling. It includes 133.4 hours of diverse speech data, such as news broadcasts and conversational speech, which improves the robustness and performance of the models in understanding and processing spoken Bhojpuri.	https://syspin.iisc.ac.in/	not found	0	not found
819	Cebuano Corpus	citing_context	CEBUDataset	https://www.semanticscholar.org/paper/1e949b343ebfe45657b20a168d38437d24389da2 (2024)	https://doi.org/10.48550/arXiv.2406.10118 (2024)	The CEBUDataset is used to provide a multilingual corpus for Southeast Asian languages, specifically supporting research and development in the Cebuano language. It offers annotated data for various NLP tasks, enabling researchers to develop and test algorithms tailored to the linguistic features of Cebuano. This dataset facilitates the advancement of NLP technologies in the region by providing essential linguistic resources.		not found	0	not found
845	Central Kurdish Corpus	cited_context | citing_context	text corpus	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)		The text corpus dataset is used to analyze the frequency of forms in the Central Kurdish language, focusing specifically on the written language. Researchers employ a methodology that prioritizes standard forms through majority voting. This enables detailed linguistic analysis, addressing research questions related to the standardization and usage patterns of Central Kurdish in written contexts.; The text corpus dataset is used to analyze the frequency of linguistic forms in the Central Kurdish language. Researchers employ this dataset to focus on the frequency distribution of specific forms within the corpus, providing insights into the usage patterns and linguistic structure of Central Kurdish. This analysis helps in understanding the language's morphology and syntax, contributing to linguistic research and language documentation.		not found	0	not found
2132	Eastern Oromo Corpus	cited_context	Eastern Oromo Dataset	https://doi.org/10.7176/nmmc/92-01 (2020)		The Eastern Oromo Dataset is primarily used for sentiment analysis of Afaan Oromoo text, focusing on opinion mining and classification of sentiments in reviews and social media posts. Researchers employ various machine learning models, including Naive Bayes, Maximum Entropy, Support Vector Machines, MNB, LSTM, and CNN, to analyze and classify positive, negative, and neutral sentiments. This dataset enables detailed understanding of public opinion and social media reactions in the Eastern Oromo language.		not found	0	not found
4476	Hausa Corpus	cited_context	HausaCC	https://doi.org/10.18653/v1/2021.emnlp-main.99 (2021)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The HausaCC dataset is used to construct monolingual corpora for the Hausa language, leveraging large-scale web crawl data. This dataset provides extensive textual content that supports linguistic research, enabling the development and evaluation of natural language processing models specific to Hausa. It facilitates the creation of language resources and tools by offering a rich, diverse set of textual data.		not found	0	not found
5625	Indonesian Corpus	citing_context	IDF-01	https://doi.org/10.1109/ACCESS.2020.3027619 (2020)	https://www.semanticscholar.org/paper/21b786b3f870fc7fa247c143aa41de88b1fc6141 (2018)	The IDF-01 dataset is used to fine-tune the WaveGlow vocoder for both multi-speaker multilingual and single-speaker monolingual text-to-speech (TTS) models. It enhances the model's ability to handle multiple speakers and languages, as well as improves performance specifically for the Indonesian language. This dataset enables researchers to address challenges in speech synthesis, such as naturalness and speaker variability, by providing a robust set of audio samples.		not found	0	not found
5673	Indonesian Corpus	cited_context	Indonesian novel corpus	https://doi.org/10.1109/IALP.2018.8629122 (2018)		The Indonesian novel corpus is used to create a male-female dialogue dataset, focusing on the linguistic aspects of spoken Indonesian in a literary context. This involves extracting and analyzing dialogues to study gender-specific language use and interactions. The dataset's literary nature provides rich, contextually diverse examples of spoken Indonesian, enabling detailed linguistic analysis.		not found	0	not found
5740	Indonesian Corpus	cited_context | citing_context	manually tagged Indonesian corpus	https://doi.org/10.1109/IALP.2018.8629262 (2018)	https://doi.org/10.1109/IALP.2014.6973519 (2014)	The manually tagged Indonesian corpus is used to develop and validate a part-of-speech tagset for Indonesian, focusing on enhancing linguistic annotation accuracy and consistency. This dataset supports research in computational linguistics by providing a standardized resource for tagging Indonesian text, enabling more reliable natural language processing applications.; The manually tagged Indonesian corpus is used to develop and validate an Indonesian part-of-speech tagset, focusing on enhancing linguistic annotation accuracy and consistency. This dataset supports research in computational linguistics by providing a standardized resource for evaluating tagging algorithms and improving the reliability of linguistic annotations in Indonesian language processing tasks.		not found	0	not found
5788	Indonesian Corpus	cited_context | citing_context	parallel corpus of 3000 Lampung language sentence pairs and their translation in Indonesian	https://doi.org/10.1109/IC3INA64086.2024.10732202 (2024), https://doi.org/10.29407/INTENSIF.V5I1.14670 (2021), https://www.semanticscholar.org/paper/41713cdd3af12f4fe8e50fee0d71ffb1a904a49e (2018)	https://www.semanticscholar.org/paper/cb58c7d42958f468019294ff9af5c398ab167d96 (1995), https://www.semanticscholar.org/paper/d96dcd94a4b9ba06972eecd390ea409fc5904414 (2017)	The parallel corpus dataset for Makassar-Indonesian sentences is used to collect and analyze sentence pairs from dictionaries and folklore books. Researchers focus on linguistic and cultural aspects, employing the dataset to explore the nuances and contextual usage of both languages. This enables detailed analysis of translation patterns and cultural expressions, enhancing understanding of the Makassar and Indonesian languages.; The dataset of 3000 Lampung-Indonesian sentence pairs is used to train and evaluate a Neural Machine Translation (NMT) model for translating Lampung to Indonesian, specifically focusing on the Api dialect without using the Attention mechanism. It is also utilized to derive translation equivalents for automatic sense-tagging and disambiguation tasks, enhancing linguistic analysis through computational methods. This dataset supports research in both machine translation and computational linguistics, providing a valuable resource for developing and refining translation models and sense disambiguation techniques.		not found	0	not avaliable
5689	Indonesian Corpus	cited_context | citing_context	Indonesian Twitter dataset for abusive language and hate speech detection	https://doi.org/10.14569/ijacsa.2023.01412108 (2023), https://doi.org/10.18653/v1/W19-3506 (2019), https://www.semanticscholar.org/paper/1ec9cab33a92a80e22ccb49bb1655a9a2342658d (2020), https://doi.org/10.1109/ICITDA60835.2023.10427046 (2023), https://doi.org/10.1145/3373477.3373495 (2019)	https://doi.org/10.1109/IALP.2018.8629262 (2018), https://doi.org/10.18653/v1/W19-3518 (2018)	The Indonesian Twitter dataset for abusive language and hate speech detection is primarily used for training and evaluating models to identify and classify abusive language and hate speech in Indonesian social media. It supports the development of hate speech detection models by focusing on target, category, and level of hate speech, using machine learning techniques. Additionally, the dataset is utilized for emotion classification tasks, enhancing the identification of emotional content in Indonesian tweets through feature engineering and the use of stop words for preprocessing.; The Indonesian Twitter dataset is primarily used for detecting and classifying hate speech and abusive language in Indonesian social media. Researchers employ machine learning models, including LSTM neural networks, to identify multi-label hate speech, abusive language, and adult content. Studies focus on linguistic features, context-specific indicators, and reducing bias in annotation processes. The dataset supports the development of classification and annotation methodologies, providing a gold standard for benchmarking models.		not found	0	not found
5763	Indonesian Corpus	cited_context	NewsGroup dataset	https://doi.org/10.24843/LKJITI.2017.V08.I03.P08 (2017)	https://doi.org/10.1016/j.ipm.2017.07.005 (2017)	The NewsGroup dataset is used to enhance the performance of Naive Bayes classifiers by integrating negative class information, leading to improved classification accuracy. This dataset facilitates research in text classification, specifically focusing on methods to refine and optimize classifier performance through the strategic use of negative examples.		not found	0	not found
5542	Indonesian Corpus	cited_context	Anindya Knowledge Graph	https://doi.org/10.26418/jlk.v4i2.62 (2021)		The Anindya Knowledge Graph dataset is used to construct a knowledge graph in the Indonesian language, focusing on entity linking and relationship extraction. This dataset enables researchers to build a comprehensive linguistic resource, enhancing the representation and understanding of semantic relationships within the Indonesian language. The dataset's focus on linguistic structures supports the development of more accurate and contextually rich language models.		not found	0	not found
5851	Indonesian Corpus	cited_context	UNBK	https://doi.org/10.26499/ICEAP.V0I0.228 (2019)	https://doi.org/10.26499/ICEAP.V2I1.95 (2018)	The UNBK dataset is used to evaluate student answers in Bahasa Indonesia, specifically to assess the accuracy and efficiency of the UKARA platform in an educational context. This involves analyzing student responses to determine the platform's effectiveness in educational settings. The dataset enables researchers to focus on improving automated assessment tools for Indonesian language education.		not found	0	not found
5658	Indonesian Corpus	citing_context	Indonesian General Sentiment Analysis Data set	https://doi.org/10.29207/resti.v6i4.4179 (2022)		The Indonesian General Sentiment Analysis Data set is used to develop sentiment analysis resources for the Indonesian language. It provides labeled data for training and evaluating models on general topics. This dataset enables researchers to build and test algorithms that can accurately classify sentiments in Indonesian text, enhancing natural language processing capabilities in this language.		not found	0	not found
5650	Indonesian Corpus	cited_context	Indonesian Corpus website	https://doi.org/10.29207/resti.v7i3.4830 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	The Indonesian Corpus website is primarily used to train IndoBERT, a language model tailored for the Indonesian language. It provides a large, diverse, and curated collection of current Indonesian texts, which enhances the model's robustness, language understanding, and generation capabilities. This dataset supports research focused on improving the performance of language models in handling modern Indonesian language.		not found	0	not found
5538	Indonesian Corpus	citing_context	AFFIN	https://doi.org/10.31937/ti.v14i1.2540 (2022)	https://www.semanticscholar.org/paper/d38763b4b0bbf1ed91b55107c658fcc96f8ef82d (2011)	The AFFIN dataset is used to derive value weights for the InSet Lexicon, facilitating sentiment analysis in the Indonesian language. Weights range from -5 to +5, enabling researchers to quantify sentiment polarity accurately. This dataset supports the development and enhancement of sentiment analysis tools, specifically tailored for Indonesian text data.	https://arxiv.org/pdf/1103.2903	not found	0	not found
5671	Indonesian Corpus	cited_context	Indonesian news articles	https://doi.org/10.32890/jict2022.21.1.4 (2021)		The Indonesian news articles dataset is used to train and evaluate models on Indonesian language processing tasks, specifically focusing on the content and structure of news articles. This dataset enables researchers to develop and test natural language processing techniques tailored to the Indonesian language, enhancing the accuracy and relevance of these models in processing news-related text.		not found	0	not found
5593	Indonesian Corpus	citing_context	FNID-Indonesian	https://doi.org/10.3390/informatics10040086 (2023)	https://doi.org/10.32604/cmc.2022.021449 (2022)	The FNID-Indonesian dataset is used to evaluate the accuracy of fake news detection models, particularly through the BERT–BiLSTM architecture. It contains diverse text types, enabling comprehensive model evaluation. This dataset supports research focused on improving the reliability of fake news detection systems in the Indonesian context.		not found	0	not found
7381	Nigerian Pidgin Corpus	cited_context	Naija-PIE	https://doi.org/10.48550/arXiv.2302.08956 (2023)	https://www.semanticscholar.org/paper/f5808755486ad4fe58d832e752e624382c5a2d5a (2020)	The Naija-PIE dataset is used for part-of-speech tagging and parsing in Nigerian Pidgin, facilitating linguistic analysis and natural language processing tasks. This dataset supports the development and evaluation of computational models for understanding and processing Nigerian Pidgin, enhancing the accuracy of linguistic annotations and syntactic structures.		not found	0	not found
7385	Nigerian Pidgin Corpus	cited_context	Nigerian Pidgin	https://doi.org/10.48550/arXiv.2212.10785 (2022)	https://doi.org/10.18653/V1/2020.COLING-MAIN.480 (2020)	The Nigerian Pidgin dataset is not explicitly mentioned in the provided usage descriptions, and therefore, there is no evidence to support its use in any specific research. As such, it cannot be accurately described how this dataset is utilized in research based on the given information.	https://arxiv.org/pdf/2003.12450	not found	0	not found
7422	Northern Sotho Corpus	citing_context	Multilingual Natural Science and Technology Dictionary Grade 4–6 (2013)	https://doi.org/10.4102/lit.v45i1.2031 (2024)		The Multilingual Natural Science and Technology Dictionary Grade 4–6 (2013) is used to align and translate scientific and technical terminology in the Northern Sotho language. It provides example terms and clarifies linguistic complexities, serving as a reference to enhance understanding of technical vocabulary. This dataset supports research by offering precise and contextually appropriate terminology, facilitating accurate communication and education in natural science and technology.		not found	0	not found
7423	Northern Sotho Corpus	citing_context	Naidoo (1994)	https://doi.org/10.4102/SAJCD.V44I1.223 (1997)	https://www.semanticscholar.org/paper/c569e43792ce618922781b7ab765a1eb4ba36827 (1994)	The Naidoo (1994) dataset is used to study linguistic norms among Northern Sotho pupils in the Pretoria / Mamelodi area. It focuses on linguistic patterns within educational settings, employing descriptive and analytical methods to understand how language use varies in these contexts. This dataset enables researchers to explore specific linguistic behaviors and their implications for education.		not found	0	not found
7434	Northern Thai Corpus	citing_context	Northern Thai language dataset	https://doi.org/10.48550/arXiv.2502.09042 (2025)	https://doi.org/10.48550/arXiv.2501.19393 (2025)	The Northern Thai language dataset is used to train and evaluate models for Northern Thai language translation, employing fine-tuning techniques. This dataset supports research focused on improving translation accuracy and linguistic processing for Northern Thai, enabling the development of more effective language models and systems.		not found	0	not found
7516	Odia Corpus	cited_context	benchmark database of Odia handwritten characters from NIT, Rourkela	https://doi.org/10.2139/ssrn.3708702 (2020)	https://doi.org/10.1049/iet-ipr.2015.0146 (2015)	The benchmark database of Odia handwritten characters from NIT, Rourkela is primarily used for training and evaluating machine learning models focused on recognizing Odia handwritten numerals and characters. Research emphasizes improving recognition rates, handling variability, enhancing performance, and ensuring robustness and generalization across different numeral and character styles. This dataset enables researchers to develop and refine algorithms that can accurately process and interpret Odia handwriting, contributing to advancements in Optical Character Recognition (OCR) technology for the Odia language.		not found	0	not found
7529	Odia Corpus	cited_context	ODIA	https://www.semanticscholar.org/paper/0c10ef5c9c74ea81ae7bf012bf212c7b3b2379de (2016), https://doi.org/10.1007/s11042-020-09457-6 (2020)	https://doi.org/10.1023/A:1022627411411 (1995)	The ODIA dataset is primarily used to develop and evaluate machine learning models for character recognition in the Odia script. It focuses on assessing the effectiveness and performance of these models in recognizing Odia characters. The dataset enables researchers to test and refine their methods specifically within the context of the Odia language, contributing to advancements in Odia script processing and recognition.		not found	0	not found
8838	Southern Sotho Corpus	cited_context	Sotho-SothoNLP	https://www.semanticscholar.org/paper/2ac19d63e1adba20473a6d1122c598f81efc3c58 (2022)	https://www.semanticscholar.org/paper/995ec006ac98a697ea38bd4eea8c1f3170a8adb4 (2020)	The Sotho-SothoNLP dataset is used to evaluate the performance of Arabic named entity recognition models in identifying entities within Southern Sotho language texts. This involves applying these models to Southern Sotho data to assess their accuracy and effectiveness in a cross-lingual context. The dataset enables researchers to explore the challenges and potential solutions for named entity recognition in under-resourced languages.		not found	0	not found
8839	Southern Sotho Corpus	cited_context	Southern Sotho dataset	https://www.semanticscholar.org/paper/2ac19d63e1adba20473a6d1122c598f81efc3c58 (2022)	https://www.semanticscholar.org/paper/a88c353124fd71c2f6af4b3387d0607878bc923c (2021)	The Southern Sotho dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how this dataset is used in research based on the provided evidence.		not found	0	not found
8841	Southern Sotho Corpus	cited_context	Southern Sotho Sotho	https://www.semanticscholar.org/paper/33776a7747eb157c2dd70b1283d83f0533569ce9 (2014)	https://doi.org/10.21437/Interspeech.2009-760 (2009)	The 'Southern Sotho Sotho' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how this dataset is used in research based on the provided evidence.		not found	0	not found
9664	Swahili Corpus	cited_context	MaReCa	https://doi.org/10.18653/v1/2021.emnlp-main.818 (2021)	https://doi.org/10.18653/v1/2021.findings-acl.106 (2021)	The MaReCa dataset is used to evaluate cross-lingual transfer performance, particularly by translating test sets into English. It serves as a benchmark for assessing how well models trained on one language can generalize to another, focusing on the accuracy and effectiveness of these translations. This enables researchers to explore the robustness and adaptability of cross-lingual models.		not found	0	not found
10911	Uyghur Corpus	citing_context	Uyghur stemming corpus	https://doi.org/10.1007/978-3-030-84186-7_19 (2021)	https://doi.org/10.3115/1596276.1596305 (2006)	The Uyghur stemming corpus is used to build a corpus of 10,000 sentence sets for Uyghur language research, specifically focusing on morphological analysis and stemming. This dataset enables researchers to develop and test algorithms for handling Uyghur language morphology, enhancing the accuracy of linguistic processing tasks. The large size and structured nature of the dataset support robust validation of stemming techniques.		not found	0	not found
8830	Southern Kurdish Corpus	citing_context	Southern Kurdish language dataset	https://www.semanticscholar.org/paper/2293b20b8721bcffeb97e2a94a3415ca4238d252 (2021)	https://www.semanticscholar.org/paper/a18b2736041977700500b63ba80c1e24e8f6f061 (2019)	The Southern Kurdish language dataset, produced by Ahmadi et al. (2019), is used to align with the Southern Kurdish lexicon at the sense level for translation inference tasks. It focuses on exploring lexical and semantic relationships, enabling researchers to enhance translation accuracy and infer meanings across languages.		not found	0	not mentioned
8831	Southern Kurdish Corpus	citing_context	Warmawa	https://doi.org/10.1515/flin-2024-2049 (2024)	https://doi.org/10.11647/obp.0307 (2022)	The Warmawa dataset is used to study Southern Kurdish folklore and linguistic patterns, specifically focusing on traditional stories and dialect variations. Researchers employ the dataset to analyze and document the rich oral traditions and linguistic nuances of Southern Kurdish, enabling a deeper understanding of cultural heritage and language diversity.		not found	0	not mentioned
5606	Indonesian Corpus	cited_context	Great Dictionary of the Indonesia Language (KBBI)	https://doi.org/10.1088/1742-6596/1008/1/012011 (2018)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003)	The Great Dictionary of the Indonesia Language (KBBI) is used in text processing research, specifically for filtering out less important words and obtaining root words from Indonesian movie reviews. This enhances the efficiency, relevance, and accuracy of text analysis. The dataset's comprehensive vocabulary and morphological data enable these improvements in natural language processing tasks.		not found	0	not opened
179	Bavarian Corpus	cited_context	Corpus of Bavarian Dialects	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://www.semanticscholar.org/paper/cf862b168e9c0f5e544c10a03eee588efb0c09b1 (2020)	The Corpus of Bavarian Dialects is used to study linguistic features of the Bavarian language, focusing on dialectal variations and phonological patterns in both spoken and written texts. It supports the creation of a syntactically annotated corpus, enabling detailed linguistic analysis and natural language processing tasks. This dataset facilitates research into the structural and phonological aspects of Bavarian dialects, enhancing understanding of regional linguistic diversity.	https://www.gcnd.ugent.be/	not in Bavarian	0	not Bavarian Corpus
187	Bavarian Corpus	cited_context	MADAR	https://www.semanticscholar.org/paper/733f61bfa5bffa1cebeceb0d22dfc654b3a7b911 (2024)	https://www.semanticscholar.org/paper/ad23a0651b0eeb87bb2bb856bea26c6b0c2f15af (2018)	The MADAR dataset is used to evaluate Domain Identification (DID) models, specifically for the Bavarian language and its dialects. It provides a comparative analysis with standard German, facilitating the assessment of model performance across these linguistic variations. This dataset enables researchers to focus on the nuances of Bavarian dialects, enhancing the accuracy and applicability of DID models in this context.	https://sites.google.com/nyu.edu/madar/home?authuser=0#h.rn99xfktiaey	not in Bavarian	0	not in Bavarian
178	Bavarian Corpus	cited_context	CC-100	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://www.semanticscholar.org/paper/c20c68c45127439139a08adb0b1f2b8354a94d6c (2019)	The CC-100 dataset is used to study high-quality monolingual data, specifically for training and evaluation in the context of Bavarian language research. It provides a robust resource for developing and assessing models that can handle the nuances of the Bavarian language, enabling researchers to improve language processing and understanding in this specific linguistic domain.		not in Bavarian	0	not in Bavarian
182	Bavarian Corpus	citing_context	English EWT	https://doi.org/10.48550/arXiv.2403.10293 (2024)	https://www.semanticscholar.org/paper/82cf69e48ede65b9d1f419da786c0349342d449d (2014)	The English EWT dataset is used to study the Bavarian language by analyzing grammatical structures similar to English. Researchers employ annotated text corpora and dependency parsing methodologies to explore these structures. This dataset enables detailed linguistic analysis, facilitating comparisons between English and Bavarian grammatical elements and enhancing understanding of their syntactic similarities.		not in Bavarian	0	not in Bavarian
188	Bavarian Corpus	cited_context	MultiCoNLLoB	https://doi.org/10.48550/arXiv.2403.12749 (2024)	https://doi.org/10.18653/v1/2022.semeval-1.196 (2022)	The MultiCoNLLoB dataset is used to train and evaluate models for Bavarian language processing, specifically focusing on named entity recognition and part-of-speech tagging. It is also utilized for multilingual complex named entity recognition, enhancing cross-lingual performance. This dataset supports research in improving the accuracy and robustness of natural language processing models for less-resourced languages like Bavarian.		not in Bavarian	0	not in Bavarian
190	Bavarian Corpus	cited_context	Snips data	https://doi.org/10.18653/V1/2021.NAACL-MAIN.197 (2021)	https://doi.org/10.18653/v1/N19-1380 (2018)	The Snips data is used to train and evaluate models for a Bavarian language task, focusing on the analysis of 400 random English utterances from the test and dev splits. This dataset enables researchers to assess model performance in translating or processing English to Bavarian, contributing to the development of more effective natural language processing systems for under-resourced languages.		not in Bavarian	0	not in Bavarian
192	Bavarian Corpus	cited_context	Universal Dependencies	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://doi.org/10.18653/v1/2020.acl-main.560 (2020)	The Universal Dependencies dataset is used to create annotated linguistic resources for multiple languages, supporting research on linguistic structures and features. It enables researchers to analyze and compare grammatical structures across languages, facilitating the development of cross-linguistic computational models and tools. This dataset provides a standardized framework for linguistic annotation, enhancing the consistency and comparability of linguistic research.		not in Bavarian	0	not in Bavarian
191	Bavarian Corpus	cited_context	SXUCorpus	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://www.semanticscholar.org/paper/b0a621b75a6cd4796be9a04eb2ee8a68ded4c7a1 (2016)	The SXUCorpus dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for any particular research area or purpose based on the provided evidence.	https://aclanthology.org/L16-1736/	not in Bavarian	0	not in Bavarian
831	Central Kurdish Corpus	citing_context	FarsDat	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	https://www.semanticscholar.org/paper/3911933c247f705b2488fdd067330820e8db07bf (2012)	FarsDat is used to analyze phonological and phonetic aspects of Central Kurdish, specifically focusing on di-phone probabilities and acoustic-phonetic features. Researchers employ this dataset to examine the sound patterns and speech characteristics of the language, enabling detailed phonetic analysis and contributing to linguistic understanding.		not in Central Kurdish	0	not in Central Kurdish
846	Central Kurdish Corpus	citing_context	TIMIT	https://doi.org/10.1007/s10579-022-09594-4 (2022)	https://www.semanticscholar.org/paper/3911933c247f705b2488fdd067330820e8db07bf (2012)	The TIMIT dataset is used to measure probabilities based on di-phones, which provides a foundation for acoustic-phonetic analysis in the Central Kurdish language study. This involves analyzing phonetic segments and their transitions, enabling researchers to understand the acoustic properties and patterns of speech sounds in Central Kurdish.		not in Central Kurdish	0	not in Central Kurdish
4424	Gujarati Corpus	citing_context	Linguistic Data for Gujarati	https://doi.org/10.1109/TCSS.2024.3360378 (2022)	https://doi.org/10.18653/v1/W16-5812 (2016)	The 'Linguistic Data for Gujarati' dataset is used to train and evaluate part-of-speech tagging models, specifically focusing on linguistic code-switching phenomena in Gujarati. This involves employing computational linguistics methods to analyze and tag parts of speech in mixed-language contexts, enhancing the understanding and processing of code-switched Gujarati text.		not in Gujarati	0	not in Gujarati
4462	Hakka Chinese Corpus	citing_context	THUYG-20	https://doi.org/10.3934/era.2023255 (2023)	https://doi.org/10.1109/TITS.2022.3203800 (2023)	The THUYG-20 dataset is used to develop and evaluate acoustic models for South Chinese dialects, specifically focusing on optimizing recognition rates. Researchers employ this dataset to improve the baseline model released by Tsinghua University, enhancing the accuracy and performance of speech recognition systems for these dialects.	https://www.openslr.org/22/	not in Hakka Chinese	0	not in Hakka Chinese
4498	Hausa Corpus	citing_context	NusaX	https://doi.org/10.48550/arXiv.2305.10971 (2023)	https://doi.org/10.48550/arXiv.2205.15960 (2022)	The NusaX dataset is used as a reference for creating a new parallel sentiment corpus for Nigerian languages, specifically focusing on human translations from English to Nigerian languages in the movie domain. It facilitates the development of sentiment analysis resources by providing accurate translations, enhancing the quality and relevance of the new corpus.		not in Hausa	0	not Hausa
4503	Hausa Corpus	cited_context | citing_context	Twitter corpus for the Hausa language	https://doi.org/10.18653/v1/2023.semeval-1.208 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.91 (2020)	The Twitter corpus for the Hausa language is used to build a sentiment analysis classifier, leveraging mixed Hausa and English features to enhance classification accuracy. This dataset enables researchers to develop more effective models for understanding sentiment in multilingual social media content, specifically addressing the nuances of Hausa language usage on Twitter.; The Twitter corpus for the Hausa language is used to train and evaluate machine learning models, particularly focusing on the performance of classifiers that utilize mixed Hausa and English features. This dataset enables researchers to assess how effectively these models can handle code-switching phenomena, providing insights into the challenges and opportunities in multilingual natural language processing.		not in Hausa	0	not Hausa
5712	Indonesian Corpus	citing_context	IVAW dataset	https://doi.org/10.1109/CITSM.2018.8674370 (2018)	https://www.semanticscholar.org/paper/1b3265dd92f441ec565a83207e3bc6acc1273b9a (2015)	The IVAW dataset is used to crawl and analyze sentiment statements in social media, specifically focusing on identifying and categorizing emotional content in posts. This involves employing natural language processing techniques to extract and classify sentiments, enabling researchers to understand the emotional landscape of online discussions.		not in Indonesian	0	not Indonesian
5611	Indonesian Corpus	cited_context	HASOC at FIRE 2019 dataset	https://doi.org/10.1145/3582768.3582771 (2022)	https://www.semanticscholar.org/paper/421b9e3f18202b757f0de42ca4a1d2de7dbe29ba (2019)	The HASOC at FIRE 2019 dataset is used to identify hate speech and offensive language in multilingual contexts. Researchers employ deep learning models and attention mechanisms to analyze social media content, focusing on the detection and classification of harmful language. This dataset enables the development and evaluation of algorithms designed to mitigate online toxicity and improve content moderation.	https://hasocfire.github.io/hasoc/2019/dataset.html	not in Indonesian	0	not Indonesian
5563	Indonesian Corpus	citing_context	CoNLL-2003 English NER dataset	https://doi.org/10.1145/3592854 (2023)	https://doi.org/10.3115/1119176.1119195 (2003)	The CoNLL-2003 English NER dataset is used to create pseudo-data for Indonesian Named Entity Recognition (NER) by translating and projecting token labels. This approach focuses on developing language-independent NER systems, leveraging the dataset's labeled entities to enhance cross-lingual NER performance. The dataset's structured annotations enable researchers to train models that can generalize across languages, specifically aiding in the development of NER systems for under-resourced languages like Indonesian.		not in Indonesian	0	not Indonesian
5783	Indonesian Corpus	cited_context	hate speech in local languages spoken in Indonesia (Javanese, Sundanes)	https://doi.org/10.1145/3639233.3639247 (2023)	https://doi.org/10.18178/wcse.2021.02.011 (2021)	Pa-mungkas et al.'s dataset is used to detect abusive language and hate speech in Javanese and Sundanese tweets. The dataset combines data from two prior studies to analyze linguistic patterns and social implications of abusive language. It employs methodologies focused on identifying and classifying hate speech, enabling researchers to explore the nuances and impacts of such language in social media contexts.	https://github.com/Shofianina/local-indonesian-abusive-hate-speech-dataset	not in Indonesian	0	The language is Javanese
5679	Indonesian Corpus	cited_context	Indonesian social media data	https://doi.org/10.11591/ijai.v12.i4.pp1928-1937 (2023)	https://doi.org/10.11591/ijai.v11.i3.pp895-904 (2022)	The Indonesian social media data dataset is used to identify abusive comments by employing a hybrid RNN-LSTM model. Research focuses on enhancing the F1 score through data balancing techniques, addressing the challenge of imbalanced datasets in detecting abusive content. This dataset enables the development and evaluation of machine learning models tailored to the nuances of Indonesian social media language.		not in Indonesian	0	not Indonesian
5583	Indonesian Corpus	cited_context	Ethnologue: Languages of the World	https://doi.org/10.48550/arXiv.2203.13357 (2022)	https://doi.org/10.1108/ERR.1999.3.11.129.120 (1999)	The Ethnologue: Languages of the World dataset is used to provide speaker counts for Indonesian and top-10 local languages, supporting research on linguistic diversity in Indonesia. This dataset enables researchers to quantify and analyze the distribution and prevalence of languages, facilitating studies on language use and preservation.		not in Indonesian	0	not Indonesian
5569	Indonesian Corpus	citing_context	DARPA LORELEI	https://doi.org/10.48550/arXiv.2205.15960 (2022)	https://www.semanticscholar.org/paper/LORELEI-Language-Packs%3A-Data%2C-Tools%2C-and-Resources-Strassel-Tracey/195351aee7cfae3a80fa77b16e7eece9f9afc894 (2016)	The DARPA LORELEI dataset is extensively used for low-resource language processing and machine translation, providing multilingual parallel texts and annotated treebanks. It supports research in syntactic and semantic analysis, enhancing translation quality and coverage in under-resourced languages. The dataset's wide coverage across over 20 languages, including Asian languages, facilitates cross-lingual studies and improves machine translation performance in less-studied language pairs.		not in Indonesian	0	not Indonesian
5632	Indonesian Corpus	cited_context | citing_context	IMDb Javanese	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICACSIS53237.2021.9631331 (2021)	The IMDb Javanese dataset is used to train and evaluate sentiment analysis and language modeling tasks, particularly for the Javanese language. It includes movie reviews and is suitable for deep learning models. Additionally, it is utilized for Indonesian language tasks, such as paragraph, question, and answer pairs, with the dataset divided into training, validation, and test sets. This enables researchers to assess model performance in both sentiment analysis and language understanding contexts.; The IMDb Javanese dataset is used to train and evaluate language models, particularly for sentiment analysis and language modeling in the Javanese language. It is also utilized for Indonesian language tasks, including paragraph, question, and answer pairs. The dataset supports deep learning approaches and is divided into training, validation, and test sets, enabling researchers to assess model performance on Javanese movie reviews and other linguistic tasks.		not in Indonesian	0	not Indonesian
5588	Indonesian Corpus	citing_context	FineWeb-Edu corpus	https://doi.org/10.48550/arXiv.2502.13252 (2025)	https://doi.org/10.1613/JAIR.1.12007 (2019)	The FineWeb-Edu corpus is used to translate English data into multiple target languages, including Indonesian, to bridge resource gaps and enhance multilingual LLMs. It is employed in creating a multilingual corpus by segmenting, translating, and reconstructing documents, with a focus on evaluating translation quality, coherence, and educational content. This dataset facilitates the transfer of rich knowledge representations, enabling more inclusive and high-quality multilingual models.		not in Indonesian	0	not Indonesian
5541	Indonesian Corpus	citing_context	AMADI_LontarSet	https://doi.org/10.48550/arXiv.2502.18148 (2025)	https://doi.org/10.1109/ICFHR.2016.0042 (2016)	The AMADI_LontarSet dataset is used to study and recognize handwritten Balinese palm leaf manuscripts, focusing on the digital analysis and preservation of Balinese script. This dataset supports research in recognizing and analyzing local Indonesian scripts, contributing to their preservation and scholarly study. The dataset's focus on Balinese script in digital form enables detailed examination and documentation of these historical documents.	https://seacrowd.github.io/seacrowd-catalogue/dataset?id=11	not in Indonesian	0	not Indonesian
5715	Indonesian Corpus	citing_context	Javanese Honorific Corpus	https://doi.org/10.48550/arXiv.2502.20864 (2025)		The Javanese Honorific Corpus is used to measure lexical diversity in Javanese honorifics by comparing Yule’s K value with the Japanese Honorific Corpus. It provides a comparative framework for integrating honorific distinctions into linguistic corpora, offering insights specific to the Javanese context. This dataset enables researchers to analyze and understand the complexity and variability of honorific usage in Javanese, facilitating cross-linguistic comparisons and enhancing corpus linguistics methodologies.		not in Indonesian	0	not Indonesian
5578	Indonesian Corpus	citing_context	Do-Not-Answer dataset	https://doi.org/10.48550/arXiv.2506.02573 (2025)	https://www.semanticscholar.org/paper/f9f23c63e2822687096b86edf4ae9435cb579b8c (2024)	The Do-Not-Answer dataset is used to enhance the safety of large language models by constructing general safety instances in Indonesian. This involves translating and selecting subsets of the dataset to create safety-focused training data. The dataset's specific focus on Indonesian language and safety improvements makes it valuable for researchers aiming to mitigate risks and improve ethical standards in language model applications.		not in Indonesian	0	not Indonesian
5773	Indonesian Corpus	citing_context	Okapi	https://www.semanticscholar.org/paper/3f9531505ff36d7ce363318d7312296f62bdb090 (2025)	https://doi.org/10.48550/arXiv.2307.16039 (2023)	The Okapi dataset is used to train and evaluate instruction-tuned large language models in 26 languages, focusing on multilingual settings. It incorporates human preference annotations to improve model performance through reinforcement learning. Research using this dataset evaluates reward models, highlighting issues such as cultural mismatches and translation artifacts, which can degrade model performance. The dataset enables the assessment of cross-lingual natural language inference models, emphasizing the challenges posed by translated content.		not in Indonesian	0	not Indonesian
5774	Indonesian Corpus	cited_context | citing_context	Old Javanese Wordnet	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/93f0902b4267919b63ccd81542affb7abbb4b518 (2020)	The Old Javanese Wordnet dataset is used to construct a lexical database for Old Javanese, focusing on mapping word relationships and semantic structures. Researchers employ this dataset to build comprehensive lexical resources, which enable detailed analyses of the language's semantic properties and lexical organization. This supports research into the historical and linguistic aspects of Old Javanese, enhancing understanding of its vocabulary and semantic networks.; The Old Javanese Wordnet dataset is used to build a lexical resource for Old Javanese, specifically focusing on word sense disambiguation and semantic relations. Researchers employ this dataset to enhance understanding and processing of Old Javanese texts by accurately identifying and categorizing word meanings and their relationships. This resource supports linguistic and computational analyses of historical texts, improving the accuracy of natural language processing tasks in this ancient language.	https://aclanthology.org/2020.lrec-1.359/	not in Indonesian	0	not Indonesian
5799	Indonesian Corpus	citing_context	RODICA	https://doi.org/10.3390/fi17010038 (2025)		RODICA, initially released in 2016, serves as the foundational dataset that has been expanded into the ERH dataset for subsequent research. It is primarily used to support the development and enhancement of larger datasets, facilitating more comprehensive studies. The dataset's expansion into ERH indicates its role in enabling more detailed and extensive research, though specific research questions and methodologies are not detailed in the provided descriptions.		not in Indonesian	0	not Indonesian
5808	Indonesian Corpus	citing_context	SMD	https://doi.org/10.48550/arXiv.2311.00958 (2023)	https://doi.org/10.18653/V1/E17-1042 (2016)	The SMD dataset is used as an original English dataset for transforming into Indonesian task-oriented dialogue systems, covering domains such as navigation, scheduling, and restaurants. It is translated and adapted to create a parallel Indonesian end-to-end task-oriented dialogue dataset, with a focus on delexicalization and lexicalization processes. This enables research on developing multilingual dialogue systems and improving cross-lingual adaptation techniques.		not in Indonesian	0	not Indonesian
5831	Indonesian Corpus	cited_context	Translated Liu Lexicon	https://doi.org/10.1109/IALP.2017.8300625 (2017)	https://doi.org/10.1145/1060745.1060797 (2005)	The Translated Liu Lexicon is used for sentiment analysis to analyze and compare opinions on the Web, particularly in cross-lingual studies. It provides a translated version of opinion words, enabling researchers to conduct comparative analyses across languages. This dataset facilitates the examination of sentiment in multilingual contexts, enhancing the accuracy and scope of opinion mining and sentiment analysis research.		not in Indonesian	0	not Indonesian
5833	Indonesian Corpus	cited_context | citing_context	Translated SQuAD 2.0	https://doi.org/10.48550/arXiv.2210.13778 (2022)	https://doi.org/10.1109/ICAICTA49861.2020.9429032 (2020)	The Translated SQuAD 2.0 dataset is used to evaluate and compare the performance of machine reading comprehension (MRC) models trained on Indonesian data. Research focuses on cross-lingual question answering and understanding, assessing model accuracy and robustness in handling questions and answers in the Indonesian language. This dataset enables researchers to benchmark and improve MRC systems for Indonesian, highlighting the importance of linguistic nuances and context in model performance.; The Translated SQuAD 2.0 dataset is used to evaluate and compare the performance of Machine Reading Comprehension (MRC) models trained on the Indonesian language. It focuses on assessing the models' ability to process and understand Indonesian text and their cross-lingual question answering capabilities. This dataset enables researchers to benchmark and improve the effectiveness of MRC systems in handling Indonesian and cross-lingual tasks.		not in Indonesian	0	not Indonesian
5521	Indonesian Corpus	cited_context	16K annotated tweet dataset	https://doi.org/10.29207/resti.v5i6.3521 (2021)	https://doi.org/10.1145/3041021.3054223 (2017)	The 16K annotated tweet dataset is used to train and evaluate deep learning models for hate speech detection. Researchers focus on semantic word embeddings, which have been shown to outperform N-gram methods. This dataset enables the development and testing of more effective hate speech detection algorithms by providing a large, annotated corpus of tweets.		not in Indonesian	0	not Indonesian Corpus
5547	Indonesian Corpus	citing_context	Berlin EmoDB	https://doi.org/10.14569/IJACSA.2021.0120422 (2021)	https://doi.org/10.1371/journal.pone.0196391 (2018)	The Berlin EmoDB dataset is primarily used for emotion recognition and classification tasks, particularly in the context of emotional speech. It has been employed to study emotional expression in the Indonesian language, balance data amounts for each emotion class (ranging from 60 to 174 samples per class), and compare the performance of combined corpora against single corpora to assess the impact of corpus size on average accuracy. This dataset enables researchers to enhance the robustness and generalizability of emotion recognition models by providing a balanced and diverse set of emotional speech samples.		not in Indonesian	0	this is not a Indonesian Corpus
7190	Marathi Corpus	citing_context	Hindi dataset used in the HASOC 2020 shared task	https://doi.org/10.1007/s13278-022-00906-8 (2022)	https://doi.org/10.1145/3368567.3368584 (2019)	The Hindi dataset from the HASOC 2020 shared task is used for transfer learning to identify hate speech and offensive content in Marathi, leveraging its close linguistic relationship with Hindi. This approach employs the Hindi dataset to train models that can then be fine-tuned for Marathi, addressing the challenge of limited annotated data in the latter language.		not in Marathi	0	not Marathi
7231	Marathi Corpus	cited_context	Marathi News Corpus	https://doi.org/10.5121/ijait.2013.3203 (2013)	https://doi.org/10.17562/PB-37-3 (2008)	The Marathi News Corpus is used to develop Part-of-Speech (POS) taggers for the Marathi language, employing Brill transformation-based methods. Researchers compare the performance of these taggers with those of other South Asian languages, focusing on improving the accuracy and effectiveness of POS tagging in Marathi. This dataset enables detailed linguistic analysis and enhances natural language processing capabilities for Marathi.		not in Marathi	0	not Marathi
7321	Najdi Arabic Corpus	citing_context	RMS survey	https://www.semanticscholar.org/paper/77c3f3c7c9341c3dc2e41494510cb4402582adec (2012)	https://doi.org/10.1093/APPLIN/20.4.481 (1999)	The RMS survey dataset is used to compile the first Romani dialectological survey, focusing on linguistic variations in Najdi Arabic. It provides data for analyzing dialectal features and their distribution, enabling researchers to study specific linguistic patterns and variations within the Najdi Arabic dialect.		not in Najdi Arabic	0	not in Najdi Arabic
7392	Nigerian Pidgin Corpus	citing_context	Pidgin English text-to-text corpus	https://www.semanticscholar.org/paper/2f40d3db2a626e77f0fc70a53e099b410ebef8a9 (2020)	https://www.semanticscholar.org/paper/880c3a51450b9a4c9f3c0de44352b2ae0f8a3e63 (2019)	The Pidgin English text-to-text corpus is primarily used to train unsupervised neural machine translation models for translating West African Pidgin to English. It facilitates the creation of Pidgin word vectors and cross-lingual embeddings aligned with English, enabling researchers to develop and evaluate translation models using text from news websites. This dataset supports advancements in unsupervised neural machine translation, focusing on text-to-text tasks.		not in Nigerian Pidgin	0	not in Nigerian Pidgin
8845	Southern Uzbek Corpus	citing_context	Southern Uzbek language dataset	https://doi.org/10.18653/v1/2024.eacl-long.100 (2024)		The Southern Uzbek language dataset is not explicitly described in the provided usage information, and no specific research applications, methodologies, or characteristics related to this dataset are mentioned. Therefore, there is no evidence to support any particular use of this dataset in research.		not in Southern Uzbek	0	not in Southern Uzbek
9630	Sudanese Arabic Corpus	citing_context	Sudanese Arabic Hotel Reviews Dataset	https://doi.org/10.1109/ICCCEEE49695.2021.9429651 (2021)	https://doi.org/10.1007/978-3-319-67056-0_3 (2018)	The Sudanese Arabic Hotel Reviews Dataset is used for sentiment analysis applications, focusing on hotel reviews in both Modern Standard Arabic and Sudanese Arabic. Researchers analyze linguistic and cultural nuances specific to Sudanese Arabic, employing methods that capture these unique aspects. This dataset enables the exploration of how regional dialects influence sentiment expression, enhancing the accuracy of sentiment analysis models in diverse Arabic contexts.		not in Sudanese Arabic	0	not Sudanese Arabic
10560	Turkmen Corpus	citing_context	Turkmen Language Dataset	https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.1109/icisct52966.2021.9670140 (2021)	The Turkmen Language Dataset is used to develop and evaluate concatenative speech synthesizers for the Turkmen language. Researchers focus on improving phonetic accuracy and the naturalness of synthesized speech. This dataset enables the testing and refinement of speech synthesis techniques, ensuring they are effective for Turkmen, a less commonly studied language.		not in Turkmen	0	not in Turkmen
10902	Uyghur Corpus	cited_context	ASPEC corpus	https://doi.org/10.18653/v1/D19-1143 (2019)	https://www.semanticscholar.org/paper/d88c756b301859880d263230f8e2e0438a499bde (2010)	The ASPEC corpus is primarily used for training unsupervised Neural Machine Translation (NMT) models, particularly in high-resource language pairs like Japanese and English, achieving a BLEU score of 0.6. It has also been applied to low-resource scenarios, such as Uyghur and English, though with limited success (BLEU score of 0.0), highlighting the challenges in these contexts. The dataset's large size (400k sentences) and multilingual nature enable researchers to explore the effectiveness of unsupervised NMT approaches across different language resources.	https://jipsti.jst.go.jp/aspec/	not in Uyghur	0	not in Uyghur
8827	Southern Kurdish Corpus	citing_context	audio samples for Southern Kurdish	https://doi.org/10.48550/arXiv.2304.01319 (2023)	https://doi.org/10.1007/978-3-030-87802-3_5 (2021)	The dataset of audio samples for Southern Kurdish is used to collect 11 hours of audio data, primarily from radio and television content, for spoken dialect recognition. This dataset enables researchers to focus on the specific characteristics of Southern Kurdish dialects, enhancing the accuracy of speech recognition systems tailored to this language.	https://doi.org/10.1007/978-3-030-87802-3_5	not released	4	audio samples for Kurdish
7536	Odia Corpus	cited_context | citing_context	OHCS	https://doi.org/10.1109/ESIC64052.2025.10962622 (2025), https://doi.org/10.14569/ijacsa.2024.0150271 (2024), https://doi.org/10.1007/s11042-020-09457-6 (2020)	https://doi.org/10.1109/NCVPRIPG.2015.7490020 (2015)	The OHCS dataset is used for recognizing handwritten atomic Odia characters, evaluating the performance of character recognition algorithms. It consists of 47 classes of Odia alphabets, enabling researchers to focus on the accuracy and efficiency of these algorithms in processing and recognizing specific Odia characters.; The dataset 'OHCS' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	https://doi.org/10.1109/NCVPRIPG.2015.7490020 (2015)	not released	4	did not provided url in the paper
7537	Odia Corpus	citing_context	OHCSv1	https://doi.org/10.1080/13682199.2022.2163348 (2022)	https://doi.org/10.1109/NCVPRIPG.2015.7490020 (2015)	The OHCSv1 dataset is used to evaluate the performance of SVM, kNN, and BPNN classifiers in recognizing handwritten atomic Odia characters. Research focuses on classification accuracy and performance, utilizing the dataset's characteristics to assess and compare these machine learning models.	https://doi.org/10.1109/NCVPRIPG.2015.7490020 (2015)	not released	4	did not provided url in the paper
a5552	Indonesian Corpus	cited_context | citing_context	SmSA	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://doi.org/10.1109/ICAICTA.2019.8904199 (2019)			not released	4	it is a dataset but did not provide url
5552	Indonesian Corpus	cited_context | citing_context	CASA	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://doi.org/10.1109/IALP.2018.8629181 (2018)	The CASA dataset is primarily used for aspect-based sentiment analysis and part-of-speech (POS) tagging in Indonesian text. It is employed to train and evaluate models for detecting aspects and classifying sentiments in reviews, particularly car reviews, using deep neural networks. Additionally, it enhances fine-grained sentiment analysis and improves syntactic analysis, supporting various downstream NLP tasks.; The CASA dataset is primarily used for training and evaluating natural language processing (NLP) models in Indonesian. It supports POS tagging, enhancing syntactic analysis, and is applied for aspect-based sentiment analysis in reviews, particularly car reviews. Deep neural networks are used to detect aspects and classify sentiments, improving fine-grained and overall sentiment analysis. The dataset enables researchers to develop more accurate and contextually relevant NLP models for Indonesian text.		not released	4	it is a dataset but did not provide url
5617	Indonesian Corpus	cited_context | citing_context	HoASA	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020), https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICEEI47359.2019.8988898 (2019)	The HoASA dataset is primarily used for aspect-based sentiment analysis and part-of-speech tagging in Indonesian text. It is applied to identify and classify sentiments associated with specific aspects in reviews, using methods like convolutional neural networks and extreme gradient boosting. Additionally, it trains and evaluates sentiment analysis models for classifying positive, negative, and neutral sentiments, as well as POS taggers for accurate labeling of parts of speech.; The HoASA dataset is primarily used for aspect-based sentiment analysis of hotel reviews in Indonesian, employing methodologies such as convolutional neural networks and extreme gradient boosting to categorize aspects and sentiments. It is also utilized for training and evaluating POS tagging and NER systems, enhancing the accuracy of part-of-speech labeling and named entity recognition in Indonesian text. This dataset supports research in improving sentiment analysis, aspect categorization, and linguistic processing for Indonesian language applications.		not released	4	it is a dataset but did not provide url
5616	Indonesian Corpus	citing_context	Hate Speech Twitter Expert Manual	https://doi.org/10.14569/ijacsa.2023.01406125 (2023)	https://doi.org/10.29207/resti.v5i6.3521 (2021)	The 'Hate Speech Twitter Expert Manual' dataset is used to detect hate speech on Twitter in Indonesia. Researchers employ GloVe embeddings for feature expansion to enhance classification accuracy. This dataset specifically supports the development and evaluation of machine learning models aimed at identifying hate speech, leveraging its annotated content to improve model performance.		not released	4	it is a dataset, but the paper did not provide url
5585	Indonesian Corpus	cited_context | citing_context	FactQA	https://doi.org/10.48550/arXiv.2310.04928 (2023)	https://doi.org/10.1162/tacl_a_00317 (2020), https://www.semanticscholar.org/paper/57dff08efcb9e2d6417a1e851a23109e03bf0c56 (2007)	The FactQA dataset is used to evaluate and benchmark machine reading comprehension and question-answering systems in Indonesian, focusing on factual questions over news and Wikipedia documents. It assesses model accuracy, the ability to handle complex questions, and cross-lingual transfer, enabling researchers to compare performance across typologically diverse languages.; The FactQA dataset is primarily used to assess and benchmark machine reading comprehension and question-answering systems in Indonesian, focusing on factual questions and context understanding. It is applied over news and Wikipedia documents to evaluate cross-lingual performance and information extraction capabilities, enabling researchers to test complex question types and multilingual model performance.		not released	4	it is a dataset, but the paper did not provide URL
5549	Indonesian Corpus	cited_context | citing_context	BPPT Medical Speech Corpus	https://doi.org/10.11591/ijai.v13.i2.pp1762-1772 (2024), https://doi.org/10.1109/CENIM56801.2022.10037479 (2022)	https://doi.org/10.1109/ICoICT52021.2021.9527450 (2021)	The BPPT Medical Speech Corpus is used to train and evaluate Automatic Speech Recognition (ASR) models specifically for transcribing medical records in Indonesian. Researchers assess the corpus's effectiveness in enhancing ASR accuracy within medical contexts, focusing on the unique linguistic and technical challenges of medical speech data. This dataset enables the development and refinement of ASR systems tailored to the Indonesian healthcare sector.; The BPPT Medical Speech Corpus is used to evaluate Automatic Speech Recognition (ASR) systems for medical record transcription, specifically focusing on the accuracy and performance of speech recognition in Indonesian medical contexts. This dataset enables researchers to assess how well ASR systems can transcribe medical speech, providing insights into their effectiveness in real-world healthcare settings.		not released	4	it is a speech dataset, but the paper did not provide the url
6	Akan Corpus	citing_context	Akan language dataset	https://doi.org/10.48550/arXiv.2502.10973 (2025), https://www.semanticscholar.org/paper/d8e6c45b1504089cdde501164d0bb557ac544dfb (2025)	https://doi.org/10.48550/arXiv.2503.19642 (2025)	The Akan language dataset is used to study cultural variation in emotion perception within a broader set of 15 African languages and to evaluate the performance of a base model in Akan compared to an English model, focusing on model generalization capabilities. This dataset enables researchers to explore linguistic and cultural nuances in emotion perception and machine learning model effectiveness across languages.	https://doi.org/10.48550/arXiv.2503.19642	not released	4	not released
138	Assamese Corpus	cited_context	Assamese-English corpus	https://doi.org/10.1109/GCAT55367.2022.9972085 (2022)	https://doi.org/10.1007/978-981-33-4084-8_4 (2021)	The Assamese-English corpus is used for evaluating and training neural machine translation (NMT) models, specifically in the context of low-resource Assamese-English translation. Researchers employ this dataset to compare the performance of different NMT systems, including a sequence-to-sequence RNN with attention and a transformer model. The dataset's focus on low-resource settings enables the assessment of these models' effectiveness in handling less common language pairs.	https://link.springer.com/chapter/10.1007/978-981-33-4084-8_4	not released	4	not released
133	Assamese Corpus	citing_context	AHTID-MW (2015)	https://doi.org/10.1109/ACCESS.2023.3301564 (2023)	https://www.semanticscholar.org/paper/05582b1f331e736677f601625ef8668032984ea2 (2005)	The AHTID-MW (2015) dataset is used to train and evaluate machine learning models for Assamese language processing, specifically focusing on handwriting recognition and text classification tasks. This dataset enables researchers to develop and test algorithms that can accurately recognize handwritten Assamese text and classify Assamese text into various categories, enhancing the performance and reliability of Assamese language processing systems.	https://ieeexplore.ieee.org/document/6424426	not released	4	not released
132	Assamese Corpus	citing_context	8,000 Tourism domain parallel sentences	https://doi.org/10.1145/3469721 (2021)	https://doi.org/10.5120/17522-8084 (2014)	The 8,000 Tourism domain parallel sentences dataset is used to train and evaluate Assamese to English statistical machine translation systems. Research focuses on assessing the impact of dataset size and domain specificity on translation performance. This dataset enables researchers to investigate how domain-specific data influences the accuracy and effectiveness of machine translation models.	https://doi.org/10.5120/17522-8084	not released	4	not released
141	Assamese Corpus	cited_context	Assamese OCR corpus	https://doi.org/10.1109/ICFHR-2018.2018.00063 (2018)	https://doi.org/10.1145/2432553.2432566 (2012)	The Assamese OCR corpus is used to develop and improve OCR systems for the Assamese language. It focuses on collecting data for 182 unique, frequently occurring words to enhance OCR accuracy. Researchers analyze the frequency of selected BU sets in Assamese text, which helps in refining the OCR system's performance on common linguistic elements. This dataset enables targeted improvements in Assamese language processing and recognition.	https://dl.acm.org/doi/10.1145/2432553.2432566	not released	4	not released
140	Assamese Corpus	cited_context | citing_context	Assamese language dataset	https://doi.org/10.1088/1757-899X/1070/1/012055 (2021), https://doi.org/10.3115/1667583.1667595 (2009)	https://doi.org/10.1007/978-3-030-42363-6_15 (2019), https://doi.org/10.1145/1386869.1386871 (2008)	The Assamese language dataset is used to extract event information from Facebook posts by leveraging reports from Assamese news portals. This involves detecting and analyzing events from social media content, employing natural language processing techniques to identify and categorize events. The dataset's relevance lies in its ability to bridge social media data with traditional news sources, enhancing the accuracy of event detection in regional languages.; The Assamese language dataset is used to study morphological patterns and train a morphological tagger, specifically utilizing a large text corpus from the Assamese daily Asomiya Pratidin. This dataset enables researchers to analyze morphological structures and develop tagging models tailored to the genre of daily newspaper articles.	https://dl.acm.org/doi/10.1145/1386869.1386871	not released	4	not released
166	Assamese Corpus	cited_context | citing_context	PMIndia	https://doi.org/10.18653/v1/2023.wmt-1.56 (2023), https://doi.org/10.18653/v1/2020.loresmt-1.9 (2020)	https://www.semanticscholar.org/paper/81ed0e757ae2d66a43d73407ad6f7e0359adf6d7 (2020)	The PMIndia dataset is used to explore parallel corpora of Indian languages, particularly Assamese, supporting language processing tasks and resource development. It provides parallel data for Assamese and other Indian languages, facilitating machine translation and cross-lingual research. This dataset enables researchers to develop and improve language technologies by leveraging its rich parallel text resources.; The PMIndia dataset is used to provide parallel data for Assamese and other Indian languages, enabling research in multilingual natural language processing and machine translation. It facilitates the development and evaluation of models that can handle multiple Indian languages, enhancing the accuracy and efficiency of translation systems.	https://arxiv.org/abs/2001.09907	not released	4	not released
164	Assamese Corpus	citing_context	parallel corpus 20,000 (approx) sentences	https://doi.org/10.1145/3469721 (2021)	https://www.semanticscholar.org/paper/c794da8601b9d77635d7204b49aa10decd4cacd7 (2015)	The dataset 'parallel corpus 20,000 (approx) sentences' is mentioned in the citation context but lacks detailed descriptions of its usage in specific research. Therefore, there is no evidence to describe its application, methodology, research questions, or enabling capabilities in any particular study.	https://arxiv.org/abs/1504.01182	not released	4	not released
157	Assamese Corpus	citing_context	L3Cube-IndicQuest	https://doi.org/10.1109/RAIT65068.2025.11089369 (2025)	https://doi.org/10.48550/arXiv.2409.08706 (2024)	The L3Cube-IndicQuest dataset is used to evaluate the performance of large language models on question answering tasks specific to Indic languages. It focuses on assessing context sensitivity and knowledge accuracy, enabling researchers to analyze how well these models handle linguistic nuances and factual information in Indic languages.	https://aclanthology.org/2024.paclic-1.93/	not released	4	not released
151	Assamese Corpus	citing_context	GUIT Assamese Corpus	https://doi.org/10.1109/ICMLANT59547.2023.10372865 (2023)		The GUIT Assamese Corpus is used to provide 30,000 sequences of Assamese text for linguistic analysis, focusing on the structure and usage of the Assamese language. Researchers employ this dataset to examine linguistic patterns and features, enabling detailed studies on the grammatical and syntactical aspects of Assamese.	https://aclanthology.org/2024.paclic-1.89.pdf	not released	4	not released
146	Assamese Corpus	citing_context	COCO-Assamese	https://doi.org/10.48550/arXiv.2503.01453 (2025)	https://www.semanticscholar.org/paper/0b6df9e9b7d3ca332600bb8ac24ff2f957b80768 (2023)	The COCO-Assamese dataset is used to develop and evaluate image caption synthesis models for the Assamese language. Researchers employ a bilinear attention mechanism to enhance the performance of these models, focusing on improving translation accuracy through manual corrections. This dataset enables the assessment of model performance and the refinement of caption generation techniques specific to the Assamese language.	https://aclanthology.org/2023.paclic-1.74.pdf	not released	4	not released
144	Assamese Corpus	citing_context	Assamese UPoS Tagged Dataset	https://doi.org/10.48550/arXiv.2410.11291 (2024)	https://doi.org/10.52783/jes.1506 (2024)	The Assamese UPoS Tagged Dataset serves as a gold standard resource, supporting various NLP tasks for the Assamese language. It enables the training of machine learning models, facilitating research in areas such as part-of-speech tagging and other linguistic analyses. This dataset's annotated tags provide essential data for developing and evaluating NLP systems tailored to Assamese.	https://aclanthology.org/2023.icon-1.38/	not released	4	not released
137	Assamese Corpus	citing_context	Assamese caption dataset	https://doi.org/10.1109/IALP61005.2023.10337310 (2023), https://doi.org/10.1145/3717612 (2025)	https://www.semanticscholar.org/paper/b51ce2126eadf25586b118b284cc5dfa0065b557 (2022)	The Assamese caption dataset is used to generate image captions in Assamese by translating English captions from Flickr30K and MSCOCO using Microsoft Azure translation service. It addresses the lack of resources in the low-resource Assamese language, enhancing training data availability for natural language processing tasks, particularly in image caption generation.	https://aclanthology.org/2022.rocling-1.33/	not released	4	not released
136	Assamese Corpus	citing_context	AsNER	https://doi.org/10.1109/RAIT65068.2025.11089369 (2025), https://doi.org/10.48550/arXiv.2410.11291 (2024)	https://doi.org/10.48550/arXiv.2207.03422 (2022)	The AsNER dataset is used to develop and evaluate Named Entity Recognition (NER) models for the Assamese language, focusing on entity extraction from text. It provides annotated data for training and evaluating NER systems, supporting research in Assamese language processing and NLP tasks. This dataset enables researchers to improve the accuracy and performance of NER models specific to Assamese, addressing the need for robust language resources in this domain.	https://aclanthology.org/2022.lrec-1.706/	not released	4	not released
150	Assamese Corpus	cited_context	English-Assamese (eng-asm)	https://doi.org/10.18653/v1/2023.wmt-1.56 (2023)	https://www.semanticscholar.org/paper/bfdc43beebd1f0d2b24b8f1564a9072bb21a6a64 (2021)	The English-Assamese (eng-asm) dataset is used to compile a bilingual corpus for language research, focusing on text alignment and translation quality. It also supports studies on linguistic patterns in Assamese, particularly phonetic and morphological structures. This dataset enables researchers to analyze and improve bilingual text processing and understand the unique characteristics of the Assamese language.	https://aclanthology.org/2021.mtsummit-loresmt.9/	not released	4	not released
149	Assamese Corpus	cited_context | citing_context	EnAsCorp1.0	https://doi.org/10.18653/v1/2023.wmt-1.56 (2023), https://doi.org/10.18653/v1/2024.wmt-1.54 (2024)	https://doi.org/10.18653/v1/2020.loresmt-1.9 (2020)	The EnAsCorp1.0 dataset is used to compile and develop English-Assamese parallel and monolingual corpora, focusing on bilingual text alignment and translation quality. It enhances machine translation systems and linguistic resources, specifically addressing the improvement of translation accuracy and linguistic alignment in the context of English-Assamese translation.; The EnAsCorp1.0 dataset is used to compile an English-Assamese corpus for language research, focusing on bilingual text alignment and translation quality. It is also utilized to study linguistic patterns in Assamese, particularly phonetic and morphological structures. The dataset supports the development and evaluation of English-Assamese translation systems by providing parallel and monolingual data for training and evaluation.	https://aclanthology.org/2020.loresmt-1.9/	not released	4	not released
176	Bavarian Corpus	citing_context	bilingual Bavarian-German lexicon	https://doi.org/10.48550/arXiv.2304.09957 (2023)	https://www.semanticscholar.org/paper/56051929213235139795ed282f1ad0b30c549ac8 (2016)	The bilingual Bavarian-German lexicon is used to train and evaluate translation systems, specifically focusing on the accuracy and coverage of the lexicon. This dataset enables researchers to assess the performance of translation models in converting between Bavarian and German, ensuring that the lexicon's comprehensive vocabulary supports high-quality translations.	https://arxiv.org/pdf/2304.09957	not released	4	not released
189	Bavarian Corpus	cited_context	NoStaD	https://doi.org/10.48550/arXiv.2403.12749 (2024)	https://www.semanticscholar.org/paper/0a1cf8dbb859c13cbdb40788d7e69060155f9d77 (2014)	The dataset 'NoStaD' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how it is actually used in research.	https://aclanthology.org/L14-1251/	not released	4	not released
177	Bavarian Corpus	cited_context | citing_context	BIOfid	https://doi.org/10.48550/arXiv.2403.12749 (2024)	https://www.semanticscholar.org/paper/ed20e4ca1741b8b3a74a049010435cb6bf815c2d (2016)	The BIOfid dataset is used to analyze historical biodiversity literature in the Bavarian language, focusing on named entity recognition. Researchers employ natural language processing techniques to identify and categorize entities within the texts. This enables detailed analysis of historical biological data, enhancing understanding of biodiversity over time. The dataset's focus on Bavarian language sources provides unique insights into regional biodiversity history.; The BIOfid dataset is used to train models for NLP tasks, specifically focusing on the Bavarian language. It enables the analysis of linguistic patterns and entity recognition in historical biodiversity literature. This dataset supports research into the linguistic structure and content of historical texts, enhancing understanding and processing of specialized historical documents.	https://aclanthology.org/K19-1081/	not released	4	not released
173	Bavarian Corpus	cited_context	Bavarian language dataset	https://doi.org/10.48550/arXiv.2403.12749 (2024)		The Bavarian language dataset is used for training and evaluating models in named entity recognition (NER) and part-of-speech tagging, particularly in non-standard German genres such as historical, chat, spoken, learner, and literary prose. It supports multilingual NER, enhancing cross-lingual performance. This dataset enables researchers to develop and test models that can accurately process and recognize entities in the Bavarian language, contributing to advancements in natural language processing for under-resourced languages.	https://aclanthology.org/2024.lrec-main.1262/	not released	4	not released
717	Bhojpuri Corpus	cited_context	IITKGP-MLILSC	https://doi.org/10.21437/s4sg.2022-1 (2022)	https://doi.org/10.1109/NCC.2012.6176831 (2012)	The IITKGP-MLILSC dataset is used to develop and evaluate language identification systems, focusing on 27 Indian languages, including Bhojpuri. It employs speech data to enhance the accuracy of language recognition models. This dataset enables researchers to address the challenge of distinguishing between closely related Indian languages, contributing to advancements in multilingual speech processing and language technology.	https://ieeexplore.ieee.org/document/6176831/metrics#metrics	not released	4	not released
760	Burmese Corpus	citing_context	in-house diphone database	https://www.semanticscholar.org/paper/fdb63df962f8411726d277480699564321a2648d (2020)	https://www.semanticscholar.org/paper/901b36e56d80f2e36eab150d85d631450b23785e (2013)	The in-house diphone database is used to develop diphone-based concatenative synthesis systems for Burmese TTS applications, focusing on enhancing speech synthesis quality and naturalness through phonetic units and concatenation techniques. It also serves as a speech corpus for training and evaluating automatic speech recognition (ASR) models in Burmese, facilitating advancements in both TTS and ASR technologies.	https://www.semanticscholar.org/paper/Diphone-Concatenation-Speech-Synthesis-for-Myanmar-Phyu-Soe/901b36e56d80f2e36eab150d85d631450b23785e	not released	4	not released
765	Burmese Corpus	citing_context	UCSY-SC1	https://www.semanticscholar.org/paper/fdb63df962f8411726d277480699564321a2648d (2020)	https://doi.org/10.11591/ijece.v9i4.pp3194-3202 (2019)	The UCSY-SC1 dataset is used for automatic speech recognition (ASR) in Burmese, serving as a speech corpus to train and evaluate ASR models. This dataset enables researchers to develop and refine ASR systems specifically tailored for the Burmese language, enhancing accuracy and performance in speech-to-text applications.	https://ijece.iaescore.com/index.php/IJECE/article/view/17232	not released	4	not released
764	Burmese Corpus	cited_context	speech corpus for Myanmar language	https://doi.org/10.11591/ijece.v9i4.pp3194-3202 (2019)	https://doi.org/10.1109/ICSDA.2017.8384451 (2017)	The 'speech corpus for Myanmar language' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, based on the provided evidence, it cannot be accurately described how this dataset is used in research.	https://ieeexplore.ieee.org/document/8384451	not released	4	not released
758	Burmese Corpus	citing_context	Basic Travel Expression Corpus (BTEC)	https://www.semanticscholar.org/paper/fdb63df962f8411726d277480699564321a2648d (2020)	https://doi.org/10.21437/Eurospeech.2003-150 (2003)	The Basic Travel Expression Corpus (BTEC) is used to construct foundational travel-related expressions for the Myanmar language, specifically for the PBC. It is employed to train and evaluate neural network-based TTS systems and ASR models for Burmese, focusing on phonetic balance, coverage, and accuracy in speech recognition and synthesis.	https://aclanthology.org/2020.lrec-1.777/	not released	4	not released
828	Central Kurdish Corpus	cited_context | citing_context	CK verb database	https://doi.org/10.1007/s42803-022-00062-7 (2021)	https://www.semanticscholar.org/paper/4ddb51690435785cdadbe3ce3907a5f3495acb46 (1966)	The CK verb database is used to compile a comprehensive and accurate list of Central Kurdish verbs, leveraging dictionary entries and linguistic studies. It provides about 7K main entries and 18.9K subentries, serving as an essential resource for linguistic research by enabling the collection of a robust Central Kurdish lexicon. This dataset supports research requiring precise and extensive verb data, enhancing the accuracy and completeness of linguistic analyses.; The CK verb database is used to compile a comprehensive list of Central Kurdish verbs, focusing on their linguistic structure and usage within the Central Kurdish language. This dataset enables researchers to analyze and document the verb system, contributing to linguistic studies and the understanding of Central Kurdish grammar.	https://www.semanticscholar.org/paper/A-Kurdish-English-Dictionary-Wahby-Edmonds/4ddb51690435785cdadbe3ce3907a5f3495acb46	not released	4	not released
821	Central Kurdish Corpus	citing_context	An extensive dataset of handwritten central Kurdish isolated characters	https://doi.org/10.11591/ijeecs.v35.i3.pp1865-1875 (2024)	https://doi.org/10.1016/j.dib.2021.107479 (2021)	The dataset of handwritten central Kurdish isolated characters is used to develop and evaluate deep learning models for character recognition. It focuses on enhancing e-government services through automation, specifically by improving the accuracy and efficiency of processing handwritten documents. The dataset's extensive coverage of isolated characters supports robust model training and evaluation.	https://www.sciencedirect.com/science/article/pii/S2352340921007605	not released	4	not released
833	Central Kurdish Corpus	citing_context	KSLexicon	https://www.semanticscholar.org/paper/364d25c670ab7d8cab8a05e7521aae0e342f23b0 (2022)		The KSLexicon dataset is used to support linguistic analysis and natural language processing tasks in the Central Kurdish language, specifically the Sorani dialect. It involves tagging 35,000 Sorani entries with 28 part-of-speech tags and annotating data according to six categories. This enables detailed linguistic research and enhances the development of NLP tools for Central Kurdish.	https://www.researchgate.net/publication/333856055_KSLexicon_Kurdish-Sorani_Generative_Lexicon	not released	4	not released
832	Central Kurdish Corpus	citing_context	Jira speech corpus	https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	The Jira speech corpus is utilized in various aspects of Kurdish language processing. It is employed to design and build speech recognition systems, correct common text errors, and construct a parallel corpus for Central Kurdish-English translation. Additionally, it is used to train GloVe embeddings and collect textual data for NLP experiments, leveraging its extensive content from diverse sources such as TED Talks, digital books, and academic texts.	https://link.springer.com/article/10.1007/s10579-022-09594-4	not released	4	not released
820	Central Kurdish Corpus	citing_context	15,000 text documents	https://doi.org/10.48550/arXiv.2304.04703 (2023)	https://doi.org/10.21928/juhd.v1n4y2015.pp393-397 (2015)	The dataset of 15,000 text documents is used to develop sentiment analyzers for Kurdish, specifically focusing on social network texts. Researchers employ a naive Bayes classifier with a bag-of-words approach, using 8000 positive and 7000 negative reviews to train the model. This enables the analysis of sentiment in Kurdish online communications, enhancing understanding of public opinion and social dynamics.	https://journals.uhd.edu.iq/index.php/juhd/article/view/618	not released	4	not released
835	Central Kurdish Corpus	cited_context	Kurdish Corpus	https://doi.org/10.37652/juaps.2022.176501 (2022)	https://doi.org/10.1177/0165551516683617 (2018)	The Kurdish Corpus is used for developing and evaluating tools for text pre-processing, tokenization, stemming, transliteration, and lemmatization in Central Kurdish language processing. These methodologies focus on enhancing the accuracy and efficiency of linguistic tools, addressing specific challenges in Central Kurdish, such as morphological complexity and script variations. The dataset enables researchers to test and refine algorithms, improving natural language processing capabilities for this under-resourced language.	https://journals.sagepub.com/doi/10.1177/0165551516683617	not released	4	not released
822	Central Kurdish Corpus	citing_context	annotated corpus for abstractive Kurdish text summarization	https://doi.org/10.48550/arXiv.2504.14630 (2025)		The annotated corpus for abstractive Kurdish text summarization is used to develop and evaluate models that generate concise summaries from longer texts in the Kurdish language. This dataset enables researchers to focus on improving the accuracy and coherence of abstractive summarization techniques, specifically tailored for the linguistic nuances of Kurdish.	https://en.civilica.com/doc/1675574/	not released	4	not released
836	Central Kurdish Corpus	citing_context	Kurdish Language (2100 words)	https://doi.org/10.24271/psr.2022.351832.1149 (2022)	https://doi.org/10.1109/ICDT.2009.29 (2009)	The Kurdish Language (2100 words) dataset is used to develop a text-to-speech (TTS) system for the Kurdish language. Researchers focus on the phonetic and quality aspects of the data, employing methodologies that enhance the accuracy and naturalness of TTS output. This dataset enables the creation of more effective and realistic speech synthesis for Kurdish, addressing the need for high-quality TTS systems in this language.	https://ieeexplore.ieee.org/abstract/document/5205223	not released	4	not released
843	Central Kurdish Corpus	cited_context	raw corpus of Sorani Kurdish		https://www.semanticscholar.org/paper/b103e1ec19f19780c2b172be85b2f0863fa20c8b (2010)	The 'raw corpus of Sorani Kurdish' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. The dataset's role in enabling research is not clearly defined based on the provided evidence.		not released	4	not released
849	Central Pashto Corpus	cited_context	KPTI	https://doi.org/10.1109/ICDAR.2017.359 (2017)	https://doi.org/10.1109/ICFHR.2016.0090 (2016)	The KPTI dataset is used to provide a text imagebase and deep learning benchmark for the Pashto language, focusing on character recognition and text analysis. It contains real Pashto calligraphic contents and is employed in deep learning benchmarking and text image analysis, enabling researchers to develop and evaluate algorithms for processing and recognizing Pashto script.	https://ieeexplore.ieee.org/document/7814106	not released	4	not released
852	Central Pashto Corpus	citing_context	synthetic dataset	https://doi.org/10.1109/ACCESS.2022.3216881 (2022)	https://doi.org/10.1109/ICET.2009.5353160 (2009)	The synthetic dataset is used to analyze the shape of Pashto script ligatures, focusing on 1000 unique ligatures across four different font sizes. This dataset supports OCR development by providing a structured set of ligatures, enabling researchers to test and improve recognition algorithms specifically for the Pashto script.	https://ieeexplore.ieee.org/document/5353160	not released	4	not released
1719	Chittagonian Corpus	citing_context	Chittagonian-Bangla Dataset	https://doi.org/10.3390/electronics13091677 (2024)	https://doi.org/10.1109/icccnt45670.2019.8944868 (2019)	The Chittagonian-Bangla Dataset, comprising over 5000 samples, is used to validate linguistic distinctions between Chittagonian and Bangla languages. This dataset fills a gap in the availability of resources for these languages, enabling researchers to conduct empirical analyses and contribute to the understanding of their unique linguistic features.	https://www.sciencedirect.com/science/article/pii/S2352340925001453	not released	4	not released
1977	Dholuo Corpus	citing_context	Dholuo	https://doi.org/10.21248/jlcl.36.2023.243 (2022)	https://doi.org/10.1162/tacl_a_00317 (2020)	The Dholuo dataset is used to investigate question answering in the Dholuo language, specifically focusing on information-seeking queries. Researchers employ this dataset to explore typological diversity in language processing, utilizing it to analyze and improve the handling of diverse linguistic structures in computational models. This enables more effective and culturally relevant information retrieval systems for under-resourced languages like Dholuo.	https://jlcl.org/article/view/243	not released	4	not released
1978	Dholuo Corpus	cited_context	SAWA corpus	https://www.semanticscholar.org/paper/ead82aa52dd095cede73531344a382b2b867c79b (2010)	https://doi.org/10.5788/18-0-488 (2009)	The SAWA corpus is used to construct a trilingual parallel corpus (English-Luo-Swahili) for computational and linguistic analysis. It leverages chapter and verse indications for paragraph alignment, facilitating cross-linguistic studies, particularly focusing on the New Testament data. This enables detailed morphological and linguistic analyses across the three languages.	https://aclanthology.org/W09-0702/	not released	4	not released
2137	Eastern Oromo Corpus	citing_context	Unsupervised OPDO official Facebook page	https://doi.org/10.7176/nmmc/92-01 (2020)		The Unsupervised OPDO official Facebook page dataset is used for unsupervised opinion mining to analyze sentiments in Afaan Oromoo, specifically focusing on positive, negative, and neutral reviews from the OPDO official Facebook page. This methodology involves extracting and categorizing textual data to understand public opinions and reactions to political content. The dataset's relevance lies in its ability to capture real-time, unfiltered public sentiment, enabling researchers to study the dynamics of political discourse and public perception in the Afaan Oromoo-speaking community.	https://www.researchgate.net/publication/334318286_Unsupervised_Opinion_Mining_Approach_for_Afaan_Oromoo_Sentiments	not released	4	not released
2141	Eastern Punjabi Corpus	citing_context	NER corpus for the Shahmukhi	https://doi.org/10.4018/IJORIS.20210701.OA1 (2021)	https://doi.org/10.1145/3383306 (2020)	The 'NER corpus for the Shahmukhi' dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. Therefore, based on the provided evidence, it cannot be definitively stated how this dataset is used in research.	https://www.sciencedirect.com/science/article/abs/pii/S0957417423009910	not released	4	not released
4403	Gujarati Corpus	citing_context	10,000 pictures	https://doi.org/10.62441/nano-ntp.v20is6.27 (2024)		The '10,000 pictures' dataset is used to train and evaluate CNN and MLP models for recognizing handwritten Gujarati characters. It achieves success rates of 97.21% and 64.48%, respectively, highlighting its effectiveness in machine learning applications for character recognition. The dataset's large size and diverse set of images enable robust model training and evaluation, facilitating advancements in Gujarati language processing.	https://www.ijert.org/research/gujarati-handwritten-character-recognition-using-hybrid-method-based-on-binary-tree-classifier-and-k-nearest-neighbour-IJERTV2IS60509.pdf	not released	4	not released
4404	Gujarati Corpus	citing_context	5000 images	https://doi.org/10.1109/CISCT55310.2022.10046543 (2022)	https://doi.org/10.1007/s40012-014-0059-z (2015)	The '5000 images' dataset is used for classifying Gujarati alphabets using KNN and SVM classifiers. The primary focus is on achieving high accuracy in character recognition. This dataset enables researchers to develop and evaluate machine learning models specifically tailored for recognizing Gujarati characters, contributing to advancements in optical character recognition (OCR) technology for the Gujarati language.	https://ieeexplore.ieee.org/document/10046543	not released	4	not released
4405	Gujarati Corpus	citing_context	bilingual corpus	https://doi.org/10.1109/iSSSC50941.2020.9358837 (2020)	https://doi.org/10.1109/ic-ETITE47903.2020.410 (2020)	The bilingual corpus is used to perform language identification in English and Gujarati code-mixed data. Researchers focus on developing methods for accurate language detection in mixed-language texts, employing techniques that leverage the dataset's code-mixed characteristics to enhance the precision of language identification algorithms.	https://ieeexplore.ieee.org/document/9077734	not released	4	not released
4407	Gujarati Corpus	citing_context	custom speech corpus	https://doi.org/10.1145/3483446 (2021)	https://doi.org/10.1007/978-981-10-3920-1_46 (2018)	The custom speech corpus is used to train and evaluate an HMM-based Automatic Speech Recognition (ASR) system for Gujarati, containing 650 regular Gujarati words. It supports research in Gujarati speech recognition by enabling the assessment of system performance, as evidenced by achieving a Word Error Rate (WER) of 12.7%. This dataset facilitates the development and refinement of ASR models tailored for the Gujarati language.	https://link.springer.com/chapter/10.1007/978-981-10-3920-1_46	not released	4	not released
4408	Gujarati Corpus	citing_context	DeitY Datasets	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)	https://www.semanticscholar.org/paper/1d480a2ae449ee625aa543610ed2574e3c87643e (2016)	The DeitY Datasets are used to provide resources for Indian languages, with a specific focus on Gujarati. These datasets support language processing tasks and linguistic analysis in Gujarati, enabling researchers to develop and evaluate computational tools and methods tailored to the Gujarati language. The dataset's comprehensive coverage of linguistic features facilitates in-depth analysis and enhances the accuracy of language processing systems.	https://www.tsdconference.org/tsd2016/download/cbblr16-850.pdf	not released	4	not released
4414	Gujarati Corpus	citing_context	Gujarati Handwritten Dataset		https://doi.org/10.1109/ICSSIT53264.2022.9716483 (2022)	The Gujarati Handwritten Dataset is used to extract features from Gujarati handwritten characters using pre-trained deep learning models such as VGG-16, VGG-19, ResNet50, ResNet101, InceptionV3, and EfficientNet. This methodology supports research focused on improving character recognition accuracy and enhancing the performance of machine learning models in recognizing Gujarati handwriting. The dataset's relevance lies in its provision of a diverse set of handwritten samples, enabling researchers to train and validate their models effectively.	https://ieeexplore.ieee.org/document/9716483	not released	4	not released
4415	Gujarati Corpus	cited_context	Gujarati language dataset	https://doi.org/10.1145/3483446 (2021)	https://doi.org/10.1007/978-981-10-3920-1_46 (2018)	The Gujarati language dataset is used to train Hidden Markov Model (HMM)-based Automatic Speech Recognition (ASR) systems, specifically focusing on recognizing 650 regular Gujarati words. The primary research goal is to improve speech recognition performance, targeting a Word Error Rate (WER) of 12.7%. This dataset enables researchers to evaluate and enhance ASR accuracy for Gujarati, contributing to advancements in speech technology for this language.	https://link.springer.com/chapter/10.1007/978-981-10-3920-1_46	not released	4	not released
4427	Gujarati Corpus	citing_context	dataset of 3000 images	https://doi.org/10.1109/CISCT55310.2022.10046543 (2022)	https://doi.org/10.1016/j.patcog.2010.01.008 (2010)	The dataset of 3000 images is used to classify Gujarati handwritten numerals. Researchers employ projection profiles and a neural network classifier, achieving 81.66% accuracy. This dataset specifically supports the development and evaluation of machine learning models for recognizing Gujarati numerals, enhancing the accuracy and reliability of numeral classification systems.	https://www.sciencedirect.com/science/article/pii/S0031320310000403	not released	4	not released
4431	Gujarati Corpus	citing_context	data set provided by Goswami et. al	https://doi.org/10.1109/ICAITPR51569.2022.9844182 (2022)	https://doi.org/10.1504/IJAPR.2015.075955 (2015)	The dataset provided by Goswami et al. is used to evaluate the performance of a proposed model on recognizing variable length numeral strings in Gujarati, specifically focusing on offline handwritten numeral recognition. It employs synthetically generated samples to test the model's accuracy and robustness in this domain. This dataset enables researchers to assess the effectiveness of their models in handling handwritten numerals, contributing to advancements in Gujarati language processing and recognition technologies.	https://www.inderscience.com/info/inarticle.php?artid=75955	not released	4	not released
4470	Hausa Corpus	cited_context	enhanced Hausa dataset	https://doi.org/10.4314/swj.v19i1.13 (2024)	https://www.semanticscholar.org/paper/bbaa0815c38e1be942b64bf21b378e6c59f8d64a (2022)	The enhanced Hausa dataset is used to train and evaluate a classifier for sentiment analysis of Hausa language tweets. The dataset focuses on improving classification accuracy by incorporating acronyms and abbreviations, which are common in social media communications. This approach addresses the specific challenge of accurately analyzing sentiment in informal, abbreviated text, enhancing the robustness of sentiment analysis models for the Hausa language.	https://www.semanticscholar.org/paper/Sentiment-Analysis-of-Hausa-Language-Tweet-Using-Sani-Page/bbaa0815c38e1be942b64bf21b378e6c59f8d64a	not released	4	not released
4501	Hausa Corpus	cited_context | citing_context	ten Hausa news articles	https://doi.org/10.1371/journal.pone.0285376 (2023)	https://www.semanticscholar.org/paper/022bd7119eefbb248f8315b2546462bb6fb37627	The 'ten Hausa news articles' dataset is used to train and test a Naïve-Bayes classifier for automatic text summarization in Hausa. This research focuses on evaluating the classifier's performance with limited data, highlighting the dataset's utility in developing summarization techniques for low-resource languages.; The 'ten Hausa news articles' dataset is used to train and test a Naïve-Bayes classifier for automatic text summarization in Hausa. This research focuses on evaluating the model's performance with a small dataset, highlighting the challenges and potential of using limited data for text summarization tasks in the Hausa language.	https://www.researchgate.net/publication/322152904_Automatic_Hausa_LanguageText_Summarization_Based_on_Feature_Extraction_using_Naive_Bayes_Model	not released	4	not released
5516	Indonesian Corpus	citing_context	1000 news data in the Indonesian language	https://doi.org/10.1109/ICOIACT55506.2022.9972086 (2022)	https://doi.org/10.1016/J.PROCS.2021.01.059 (2021)	The '1000 news data in the Indonesian language' dataset is used to evaluate the performance of hoax news classification models. Researchers employ both deep learning and traditional machine learning algorithms, testing various parameter settings to assess model accuracy. This dataset enables the comparison of different methodologies in detecting hoaxes, contributing to the development of more effective classification systems.	https://www.sciencedirect.com/science/article/pii/S1877050921000739?via%3Dihub	not released	4	not released
5518	Indonesian Corpus	citing_context	11 millions Indonesian News	https://doi.org/10.1007/978-3-031-24337-0_29 (2019)	https://doi.org/10.21437/Interspeech.2014-564 (2013)	The '11 millions Indonesian News' dataset is used to pre-train bidirectional language models (bi-LMs) for monolingual ELMo, specifically focusing on the Indonesian language. It provides a large corpus of news articles, which enhances the language model's understanding of current events and formal language, thereby improving its overall performance and contextual awareness.	https://eprints.illc.uva.nl/id/eprint/740/1/MoL-2003-02.text.pdf	not released	4	not released
5527	Indonesian Corpus	cited_context	50k formal words of KBBI	https://doi.org/10.1007/s10772-018-09569-3 (2018)	https://doi.org/10.14569/IJACSA.2016.070358 (2016)	The '50k formal words of KBBI' dataset is primarily used to develop, train, and evaluate algorithms for syllabification and grapheme-to-phoneme (G2P) conversion in Indonesian language processing. Researchers employ this dataset to enhance phonemic accuracy through modified grapheme encoding, phonemic rules, and cross-validation techniques. Specific applications include improving syllable segmentation in personal names, evaluating phonemic syllabification performance, and comparing graphemic and phonemic methods. The dataset's formal nature and structured five-fold division facilitate robust model training and evaluation, particularly in PNNR-based G2P systems.		not released	4	not released
5649	Indonesian Corpus	cited_context	Indonesian Corpora for Automatic Abstractive and Extractive Chat Summarization	https://doi.org/10.1080/09540091.2021.1937942 (2021), https://doi.org/10.15676/ijeei.2021.13.4.10 (2021)	https://www.semanticscholar.org/paper/e0e68cedb8d1cd2643da97fe223b096ad851aa4c (2016)	The Indonesian Corpora for Automatic Abstractive and Extractive Chat Summarization dataset is used to develop and evaluate summarization systems for the Indonesian language, focusing on both abstractive and extractive techniques. It is applied to conversational data from WhatsApp and online news content, supporting research into summarization models tailored for Indonesian linguistic structures. This dataset enables the creation and assessment of chat and news summarization models, enhancing the accuracy and relevance of summaries in Indonesian.	https://aclanthology.org/L16-1129/	not released	4	not released
5738	Indonesian Corpus	cited_context	Low Level Descriptor of Interspeech 2010	https://doi.org/10.1088/1742-6596/971/1/012048 (2018)	https://doi.org/10.1109/SLT.2014.7078619 (2014)	The Low Level Descriptor of Interspeech 2010 dataset is primarily used for detecting emotions in Indonesian spoken language. It serves as a baseline for comparison and is employed to evaluate the effectiveness and performance of different feature sets in emotion detection tasks. This dataset enables researchers to systematically compare various feature extraction methods, enhancing the accuracy and reliability of emotion recognition systems in Indonesian speech.	https://ieeexplore.ieee.org/abstract/document/7078619	not released	4	not released
5662	Indonesian Corpus	cited_context	Indonesian Labor Strike Tweets	https://doi.org/10.1088/1757-899X/403/1/012067 (2018)	https://doi.org/10.1145/2348283.2348380 (2012)	The Indonesian Labor Strike Tweets dataset is used to train and evaluate a supervised entity tagger for named entity recognition in low-resource settings, specifically focusing on labor strike tweets in Indonesian. This dataset enables researchers to improve the accuracy of identifying key entities in tweets related to labor strikes, enhancing the understanding and analysis of social movements and labor dynamics in Indonesia.	https://telkomnika.uad.ac.id/index.php/TELKOMNIKA/article/view/3876	not released	4	not released
5642	Indonesian Corpus	cited_context | citing_context	Indonesia-English code-mixed Tweets		https://doi.org/10.1109/I2CT57861.2023.10126234 (2023)	The 'Indonesia-English code-mixed Tweets' dataset is utilized for normalizing and detecting multi-label hate speech and abusive language in code-mixed content. Research focuses on the linguistic and contextual aspects of mixed-language tweets, employing methods to normalize the data and identify abusive language. This dataset enables detailed analysis of code-mixed language in social media, enhancing understanding of linguistic characteristics and the prevalence of hate speech in Indonesian-English tweets.; The Indonesia-English code-mixed Tweets dataset is primarily used for detecting and normalizing multi-label hate speech and abusive language in Indonesian-English mixed content on Twitter. Researchers employ this dataset for collecting and classifying tweets, focusing on linguistic and contextual aspects to improve normalization techniques and enhance multi-label classification models. This dataset supports studies on code-mixed language in social media, addressing specific challenges in hate speech and abusive language detection.	https://ieeexplore.ieee.org/document/10126234	not released	4	not released
5670	Indonesian Corpus	cited_context	Indonesian NER corpus		https://doi.org/10.1109/IALP.2016.7876005 (2016)	The Indonesian NER corpus is used for training and evaluating named entity recognition (NER) models in the Indonesian language. It contains approximately 600 instances and focuses on linguistic, orthographic, and lookup list features. This dataset enables researchers to develop and test NER models, addressing specific challenges in recognizing named entities within Indonesian text.	https://ieeexplore.ieee.org/document/7876005	not released	4	not released
5696	Indonesian Corpus	cited_context | citing_context	Indonesian Wikipedia dump data	https://doi.org/10.1109/IALP.2018.8629109 (2018)	https://www.semanticscholar.org/paper/2c08e64edebbeafd4e85d68c08ba272a2455f75e (2003)	The Indonesian Wikipedia dump data is used to precompute Term Frequency (TF) and Inverse Document Frequency (IDF) tables for information retrieval in Bahasa Indonesia. The dataset focuses on evaluating the impact of stemming techniques on retrieval performance, enabling researchers to optimize search algorithms and improve the accuracy of information retrieval systems in the Indonesian language.	https://www.researchgate.net/publication/254921793_The_impact_of_stemming_on_information_retrieval_in_Bahasa_Indonesia	not released	4	not released
5654	Indonesian Corpus	cited_context | citing_context	Indonesian Emotion Words List	https://doi.org/10.1109/IALP.2018.8629262 (2018)	https://doi.org/10.1111/1467-839X.00086 (2001)	The Indonesian Emotion Words List dataset is used to classify and analyze 94 Indonesian emotion words into five main categories: love, joy, anger, sadness, and fear. It supports research on the structure of the Indonesian emotion lexicon by forming significant features in combination with Bag-of-Words and FastText models. This enables detailed analysis of emotional content in Indonesian text, focusing on the presence and distribution of emotion-related words.; The Indonesian Emotion Words List dataset is used to categorize 94 emotion words into five main classes: love, joy, anger, sadness, and fear. It supports research on the structure of the Indonesian emotion lexicon and analyzes the emotional content in Indonesian text by examining the presence and frequency of these emotion-related words in various contexts. This dataset enables detailed studies on emotional expression and categorization in the Indonesian language.	https://onlinelibrary.wiley.com/doi/epdf/10.1111/1467-839X.00086	not released	4	not released
5524	Indonesian Corpus	citing_context	400 twitter posts	https://doi.org/10.1109/IALP51396.2020.9310514 (2020)	https://doi.org/10.1016/j.procs.2019.11.155 (2019)	The '400 twitter posts' dataset is used to normalize abbreviations and acronyms in Indonesian microtext. Researchers employ dictionary-based and Longest Common Subsequence (LCS) approaches to enhance text processing accuracy. This dataset specifically supports research aimed at improving the handling of abbreviated and acronymic content in Indonesian social media text, facilitating more effective natural language processing tasks.	https://www.sciencedirect.com/science/article/pii/S1877050919318666	not released	4	not released
5667	Indonesian Corpus	cited_context	Indonesian Lyrics Dataset (ILD) 		https://doi.org/10.1109/ic2ie47452.2019.8940826 (2019)	The Indonesian Lyrics Dataset is used for lyrics classification in Music Information Retrieval, specifically to enhance music genre detection accuracy through feature level fusion. This approach involves combining various features extracted from the lyrics to improve classification performance. The dataset's relevance lies in its ability to provide a rich source of textual data for training and testing machine learning models aimed at genre detection.	https://ieeexplore.ieee.org/document/8940826	not released	4	not released
5645	Indonesian Corpus	citing_context	Indonesia Named Entity Recognition (NER) dataset		https://doi.org/10.1109/IC2IE60547.2023.10331336 (2023)	The Indonesia Named Entity Recognition (NER) dataset is used to categorize formal type sentences in Indonesian, specifically focusing on named entity recognition within the context of Indonesian language processing. This dataset enables researchers to develop and evaluate NER models tailored for the Indonesian language, enhancing the accuracy of identifying and classifying named entities in formal Indonesian text.	https://ieeexplore.ieee.org/document/10331336	not released	4	not released
5557	Indonesian Corpus	cited_context	data collected by [8]	https://doi.org/10.1109/ICAICTA.2016.7803103 (2016)	https://doi.org/10.1016/J.PROCS.2016.04.053 (2016)	The dataset collected by [8] is used to evaluate feature performance in Indonesian named-entity recognition (NER). It consists of 1500 sentences, 126,820 tokens, and 15 named entity (NE) classes. Researchers employ ensemble supervised learning methods to assess the effectiveness of different features in NER tasks, focusing on improving recognition accuracy and robustness.	https://www.sciencedirect.com/science/article/pii/S1877050916300679	not released	4	not released
5684	Indonesian Corpus	citing_context	Indonesian Translation of Holy Qur’an	https://doi.org/10.1109/ICAICTA59291.2023.10390123 (2023)	https://doi.org/10.1109/CITSM.2016.7577585 (2016)	The Indonesian Translation of Holy Qur’an dataset is used for developing rule-based question answering systems, with a focus on improving performance through semantic annotation. This approach enhances the system's ability to accurately respond to queries by leveraging the structured content of the translation. The dataset's detailed and contextually rich text supports the creation of more sophisticated and context-aware question answering models.	https://doi.org/10.1109/CITSM.2016.7577585	not released	4	no dataset URL provided
5576	Indonesian Corpus	citing_context	Dinakaramani corpus	https://doi.org/10.1109/ICAICTA59291.2023.10390353 (2023)	https://doi.org/10.1109/IC3INA.2018.8629519 (2018)	The Dinakaramani corpus is primarily used for Indonesian part-of-speech (POS) tagging in natural language processing research. It provides a manually tagged corpus and a standardized tagset, enabling the creation and evaluation of probabilistic and neural network-based models. This dataset enhances the accuracy and consistency of POS tagging models, contributing to advancements in Indonesian language processing.	https://ieeexplore.ieee.org/document/6973519	not released	4	not released
5577	Indonesian Corpus	cited_context	disaster data	https://doi.org/10.1109/ICCSAI53272.2021.9609781 (2021)	https://doi.org/10.1109/ICEEIE47180.2019.8981479 (2019)	The 'disaster data' dataset is used to conduct sentiment analysis on social media posts during disasters, focusing on public reactions and information dissemination. Researchers employ natural language processing techniques to analyze the sentiment of these posts, which helps in understanding how the public responds to and shares information during crisis events. This dataset enables researchers to identify patterns and trends in public communication, aiding in the development of more effective disaster response strategies.	https://ieeexplore.ieee.org/document/8981479	not released	4	not released
5664	Indonesian Corpus	citing_context	Indonesian language data in the hotel domain	https://doi.org/10.1109/ICEEI59426.2023.10346852 (2023)		The Indonesian language data in the hotel domain is used to train, validate, and test models on Indonesian language processing tasks. Comprising 5000 texts, the dataset is split into training, validation, and test sets, enabling researchers to develop and evaluate models specifically for the hotel domain. This dataset facilitates the improvement of natural language understanding and generation in Indonesian, focusing on applications such as customer service, review analysis, and automated response systems.	https://digilib.itb.ac.id/gdl/view_data/pembelajaran-transfer-dan-representasi-berbasis-span-untuk-ekstraksi-triplet-opini-untuk-analisis-sentimen-berbasis-aspek	not released	4	not released
5657	Indonesian Corpus	citing_context	Indonesia news online dataset	https://doi.org/10.1109/ICERA66156.2025.11087367 (2025)	https://doi.org/10.34123/SEMNASOFFSTAT.V2020I1.601 (2021)	The Indonesia news online dataset is used to extract location entities from Indonesian online news articles about forest fires. Researchers employ bidirectional LSTM-CNNs for entity recognition, leveraging the dataset's textual content to identify and locate geographical references related to forest fire incidents. This methodology aids in understanding the spatial distribution and impact of forest fires as reported in the media.	https://prosiding.stis.ac.id/index.php/semnasoffstat/article/view/601	not released	4	not released
5665	Indonesian Corpus	cited_context | citing_context	Indonesian language stop word list	https://doi.org/10.1109/ICIMCIS56303.2022.10017528 (2022)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003)	The Indonesian language stop word list is used for stop word removal in the preprocessing phase of text data, specifically to enhance information retrieval tasks. This dataset aids in refining text data by eliminating common words that do not contribute to the meaning, thereby improving the efficiency and accuracy of information retrieval systems.; The Indonesian language stop word list is used for stop word removal in text preprocessing, specifically to enhance the accuracy of information retrieval systems handling Indonesian text. This dataset facilitates the removal of common words that do not contribute to the meaning, improving the efficiency and effectiveness of text analysis in Indonesian language research.	https://eprints.illc.uva.nl/id/eprint/740/1/MoL-2003-02.text.pdf	not released	4	not released
5687	Indonesian Corpus	cited_context	Indonesian tweet on Twitter sentiment regarding the use of public land transport	https://doi.org/10.1109/ICODSE.2017.8285860 (2017)	https://www.semanticscholar.org/paper/1540afe9197e371b64d157bf3614fcef709ad292 (2015)	The dataset 'Indonesian tweet on Twitter sentiment regarding the use of public land transport' is used to analyze sentiment towards public land transport in Indonesia. Researchers employ Support Vector Machine (SVM) for classification, focusing on understanding public opinion and perceptions. This dataset enables the examination of specific issues and sentiments expressed by users, providing insights into the effectiveness and user satisfaction of public transport systems.	https://repository.telkomuniversity.ac.id/pustaka/100653/analisis-sentimen-pada-twitter-mengenai-penggunaan-transportasi-umum-darat-dalam-kota-dengan-metode-support-vector-machine.html	not released	4	not released
5688	Indonesian Corpus	cited_context	Indonesian tweets	https://doi.org/10.1109/ICODSE.2017.8285860 (2017)	https://www.semanticscholar.org/paper/33b9c2c8411276a7f565a1af8beff509fccd0a9a (2014)	The 'Indonesian tweets' dataset is used to analyze sentiment towards public figures in Indonesian social media. Researchers employ SVM and Naive Bayes classifiers with a bag-of-words approach to categorize and classify tweets. This methodology helps in understanding public opinion and emotional responses, enabling detailed sentiment analysis in the context of Indonesian social media.	https://www.neliti.com/publications/174778/analisis-sentimen-dan-klasifikasi-kategori-terhadap-tokoh-publik-pada-twitter	not released	4	not released
5736	Indonesian Corpus	cited_context	lexicon that has been developed by Joice (2002)	https://doi.org/10.1109/ICoDSE56892.2022.9971944 (2022)		The lexicon developed by Joice (2002) is used as a reference lexicon for linguistic analysis. While the specific research contexts and methodologies are not detailed, the dataset provides a foundational resource for researchers conducting linguistic studies. Its primary role is to serve as a standardized reference, enabling consistent and reliable linguistic analysis across various research questions and applications.		not released	4	not released
5584	Indonesian Corpus	cited_context | citing_context	FacQA	https://doi.org/10.1109/ICoICT55009.2022.9914847 (2022), https://doi.org/10.48550/arXiv.2212.09648 (2022), https://doi.org/10.48550/arXiv.2210.13778 (2022)	https://www.semanticscholar.org/paper/57dff08efcb9e2d6417a1e851a23109e03bf0c56 (2007)	The FacQA dataset is primarily used to develop and evaluate Indonesian question answering systems, focusing on extracting answers from news articles using machine learning techniques. It is utilized to assess the impact of dataset size on model performance, ranging from small (around 3K samples) to moderate (around 5K samples). The dataset includes real-world, non-synthetic data and contains multilingual content, particularly English terms and entity names, which is crucial for improving and evaluating the performance of both monolingual and multilingual models in Indonesian Natural Language Understanding tasks.; The FacQA dataset is used to develop and evaluate Indonesian question answering systems, particularly focusing on extracting answers from news articles. It highlights the challenges of working with a small dataset size of around 3K samples, which impacts the performance and training of machine learning models. This dataset enables researchers to address the limitations of small-scale QA datasets in the Indonesian language, contributing to the improvement of QA systems despite data scarcity.	https://www.semanticscholar.org/paper/A-machine-learning-approach-for-indonesian-question-Purwarianti-Tsuchiya/57dff08efcb9e2d6417a1e851a23109e03bf0c56	not released	4	there is a dataset but no URL
5568	Indonesian Corpus	cited_context	customer complaint data from Twitter	https://doi.org/10.1109/ICoICT55009.2022.9914902 (2022)	https://doi.org/10.1109/ICOICT.2017.8074652 (2017)	The customer complaint data from Twitter is used to optimize training data for multiclass text classification, enhancing classifier performance using Naïve Bayes and Support Vector Machine algorithms. This dataset enables researchers to improve the accuracy and efficiency of text classification models, specifically in handling and categorizing customer complaints.	https://ieeexplore.ieee.org/document/8074652	not released	4	not released
5574	Indonesian Corpus	cited_context	DBPedia Indonesia	https://doi.org/10.1109/ISCBI.2015.15 (2015), https://doi.org/10.1109/IALP.2014.6973520 (2014)	https://doi.org/10.1016/J.WEBSEM.2009.07.002 (2009)	The DBPedia Indonesia dataset is used to extract and organize structured information from Wikipedia Indonesia, supporting research on Indonesian language and knowledge representation. It facilitates the development of applications that require organized data, enhancing the extraction and utilization of information for various research purposes.		not released	4	not released
5570	Indonesian Corpus	cited_context	data corpus from Fachri	https://doi.org/10.1109/ISITIA.2016.7828656 (2016)		The 'data corpus from Fachri' is used for training and testing Named Entity Recognition (NER) systems. It contains 894 sentences labeled as DIRECT and 315 sentences labeled as INDIRECT, enabling researchers to evaluate NER performance on different sentence types. This structured dataset supports the development and assessment of NER models, particularly in handling varied linguistic structures.		not released	4	not released
5677	Indonesian Corpus	cited_context	Indonesian sentiment lexicon	https://doi.org/10.1109/ISITIA.2017.8124100 (2017)	https://doi.org/10.1109/CDAN.2016.7570949 (2016)	The Indonesian sentiment lexicon dataset is used to calculate the frequency of sentiment words in Indonesian text, primarily for sentiment analysis in opinion mining. Researchers employ this dataset to quantify positive, negative, and neutral sentiments, enabling them to analyze public opinions and attitudes in various contexts. This lexicon facilitates the identification and measurement of sentiment frequencies, enhancing the accuracy of sentiment analysis tasks.		not released	4	not released
5571	Indonesian Corpus	citing_context	Dataset for Indonesian Texts	https://doi.org/10.1109/ISITIA59021.2023.10221008 (2023)	https://doi.org/10.14236/EWIC/EASE2006.10 (2006)	The 'Dataset for Indonesian Texts' is used for Semantic Role Labeling in Indonesian text sentences, focusing on information extraction. Researchers employ this dataset to analyze problem domains, methods, challenges, and opportunities in the field. This dataset enables detailed examination of semantic roles within Indonesian texts, enhancing understanding and improving natural language processing techniques.	https://www.scienceopen.com/hosted-document?doi=10.14236/ewic/EASE2006.10	not released	4	not released
5672	Indonesian Corpus	citing_context	Indonesian News Sentences	https://doi.org/10.1109/ISITIA59021.2023.10221008 (2023)	https://doi.org/10.1109/ICOICT.2016.7571935 (2016)	The Indonesian News Sentences dataset is utilized for multi-document summarization and semantic role labeling. Researchers apply semantic graph techniques and focus on syntactic and semantic structures to generate summarization templates. This dataset enables the analysis and synthesis of Indonesian news articles, enhancing the accuracy and relevance of automated summarization systems.	https://ieeexplore.ieee.org/document/10221008	not released	4	not released
5710	Indonesian Corpus	citing_context	data in the form of text from Indonesian language	https://doi.org/10.1145/3330482.3330494 (2019)	https://doi.org/10.1166/ASL.2018.10675 (2018)	The dataset 'data in the form of text from Indonesian language' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. Therefore, based on the provided evidence, no detailed usage can be accurately described.	https://www.ingentaconnect.com/contentone/asp/asl/2018/00000024/00000002/art00049	not released	4	not released
5525	Indonesian Corpus	cited_context	4139 sentences from Wikipedia	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1016/J.PROCS.2018.08.193 (2018)	The '4139 sentences from Wikipedia' dataset is used to train and evaluate a hybrid BLSTM-CNN model for named-entity recognition in Indonesian. It focuses on identifying entities within a large corpus of Wikipedia sentences, enabling researchers to improve the accuracy of entity recognition in natural language processing tasks.		not released	4	this is a dataset, but the paper did not provide url
5526	Indonesian Corpus	cited_context	457 online news data from: Detiknews	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1016/J.PROCS.2016.04.053 (2016)	The '457 online news data from: Detiknews' dataset is used to build an Indonesian Named Entity Recognition (NER) system for newspaper articles. It focuses on 15 entity classes and employs ensemble supervised learning methods. This dataset enables researchers to develop and evaluate NER models specifically tailored for Indonesian news content, enhancing the accuracy of entity identification in this linguistic context.		not released	4	this is a dataset, but the paper did not provide url
5529	Indonesian Corpus	cited_context	591 sentence reviews and 8,574	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1109/KST51265.2021.9415829 (2021)	The dataset, comprising 591 sentence reviews and 8,574 CRF and HMM models, is used for aspect and opinion extraction on Indonesian lipstick product reviews. Researchers employ Conditional Random Fields (CRF) and Hidden Markov Models (HMM) to identify and classify aspects and opinions within the reviews, enabling detailed sentiment analysis and understanding of consumer feedback.		not released	4	this is a dataset, but the paper did not provide url
5531	Indonesian Corpus	cited_context	72,212 Twitter data	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1109/AGERS51788.2020.9452770 (2020)	The 72,212 Twitter data dataset is used to train and evaluate machine learning models, including Naive Bayes, random forest, SVM, logistic regression, and CRF, for information extraction in flood monitoring. This dataset enables researchers to develop and test algorithms that can effectively extract relevant information from social media posts, enhancing real-time flood monitoring and response efforts.		not released	4	this is a dataset, but the paper did not provide url
5532	Indonesian Corpus	cited_context	7,440 Twitter data	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1109/ICAICTA.2016.7803103 (2016)	The 7,440 Twitter data dataset is used for classifying complaint tweets in Indonesian, employing natural language processing (NLP) techniques and sentiment analysis. This dataset enables researchers to analyze and categorize tweets based on their content and emotional tone, specifically addressing the identification and classification of complaints. The dataset's focus on the Indonesian language and its large volume support robust NLP models and sentiment analysis algorithms.		not released	4	this is a dataset, but the paper did not provide url
5533	Indonesian Corpus	cited_context	800 Twitter data	https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://doi.org/10.1109/ICIMCIS48181.2019.8985194 (2019)	The '800 Twitter data' dataset is used to extract and analyze information about Indonesian tourist attractions from social media posts. It employs content analysis and information retrieval techniques to structure and categorize the data, enhancing semantic understanding. This dataset enables researchers to gain insights into tourist attraction data, improving its organization and interpretation.		not released	4	this is a dataset, but the paper did not provide url
5614	Indonesian Corpus	cited_context | citing_context	Hate speech detection in the Indonesian language dataset	https://doi.org/10.11591/IJEECS.V11.I1.PP294-299 (2018), https://doi.org/10.18653/v1/W19-3506 (2019), https://doi.org/10.1109/ICoICT49345.2020.9166251 (2020), https://doi.org/10.1109/ICACSIS.2018.8618182 (2018), https://doi.org/10.30534/IJATCSE/2019/6481.32019 (2019), https://doi.org/10.1088/1757-899X/830/3/032006 (2020)	https://doi.org/10.18653/v1/N16-2013 (2016), https://doi.org/10.1109/ICACSIS.2017.8355039 (2017)	The 'Hate speech detection in the Indonesian language dataset' is used to train and evaluate models for detecting hate speech in Indonesian social media. Research focuses on linguistic features and context-specific indicators of abusive language, providing a preliminary study and dataset for this purpose. The dataset enables researchers to develop and refine hate speech detection algorithms tailored to the Indonesian language.; The dataset is used to detect and classify hate speech in Indonesian social media, focusing on offensive, abusive, and politically charged content. It provides labeled instances for training and evaluating machine learning models, with studies employing techniques like unigram and SMOTE to enhance classification accuracy. Research questions address linguistic patterns, context-specific indicators, and the impact of political issues on hate speech. The dataset's labeled instances (260 hate, 430 non-hate) enable robust model development and evaluation.	https://ieeexplore.ieee.org/document/8355039	not released	4	not released
5791	Indonesian Corpus	citing_context	Pratiwi et al. dataset	https://doi.org/10.11591/ijeecs.v33.i1.pp450-462 (2024)	https://doi.org/10.1109/ICACSIS.2018.8618182 (2018)	The Pratiwi et al. dataset is used for detecting and evaluating hate speech in Indonesian Instagram comments. Researchers apply FastText for classification and analyze social media content, comparing the performance of their models against original ones. This dataset enables the assessment of hate speech detection methodologies, focusing on the effectiveness of machine learning models in this context.	https://ieeexplore.ieee.org/document/8618182	not released	4	not released
5693	Indonesian Corpus	citing_context	Indonesian voice-to-text data set	https://doi.org/10.14569/IJACSA.2021.0120327 (2021)	https://doi.org/10.1109/ICELTICs50595.2020.9315538 (2020)	The Indonesian voice-to-text dataset is used to train a Time Delay Neural Network Factorization (TDNNF) model for automatic speech recognition. This focuses on enhancing the accuracy of transcribing Indonesian audio, leveraging the dataset's specific characteristics to improve speech recognition performance in the Indonesian language.	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9315538	not released	4	not released
5652	Indonesian Corpus	citing_context	Indonesian Emotional Speech Corpus (IDESC)	https://doi.org/10.14569/IJACSA.2021.0120422 (2021)	https://doi.org/10.1109/SLT.2014.7078619 (2014)	The Indonesian Emotional Speech Corpus (IDESC) is used to enhance emotion recognition in Indonesian television talk shows. It forms the first Indonesian emotional speech corpus, contributing to affective computing by providing essential linguistic resources. The dataset enables researchers to develop and test algorithms for recognizing emotions in spoken language, specifically tailored to the Indonesian context.	https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051413	not released	4	not released
5683	Indonesian Corpus	cited_context	Indonesian Text Morphing Viseme	https://doi.org/10.14569/ijacsa.2022.0130950 (2022)	https://doi.org/10.3991/IJET.V13I08.8084 (2018)	The Indonesian Text Morphing Viseme dataset is used to enhance pronunciation learning in Indonesian visual speech synthesis. It combines morphing viseme and syllable concatenation techniques to improve the accuracy and naturalness of synthesized speech, supporting educational applications in language learning. This dataset enables researchers to develop more effective visual speech models by providing detailed viseme data aligned with syllabic structures.	https://www.proquest.com/docview/2666959428?pq-origsite=gscholar&fromopenview=true	not released	4	not released
5633	Indonesian Corpus	cited_context | citing_context	INARTE	https://doi.org/10.1016/j.dib.2023.109998 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021)	https://doi.org/10.1109/ICSTC.2018.8528283 (2018), https://doi.org/10.1007/978-3-031-23793-5_34 (2018)	The INARTE dataset is used to train and evaluate natural language inference models in Indonesian, focusing on textual entailment recognition. It contains 10,000 sentence pairs for various NLP tasks, enabling researchers to assess model accuracy in understanding and reasoning about textual relationships. Studies employ this dataset to improve and test the performance of models in recognizing entailment, contributing to advancements in Indonesian NLP.; The INARTE dataset is used to evaluate textual entailment models on Indonesian data, primarily from Wikipedia. It includes a larger set of approximately 1.5k example pairs and a smaller set of 400 pairs, both with binary labels. Researchers use this dataset to assess model performance and explore semi-supervised methods for enhancing entailment recognition.	https://ieeexplore.ieee.org/document/8528283	not released	4	not released
5661	Indonesian Corpus	citing_context	Indonesian-Javanese code-mixed data	https://doi.org/10.14569/ijacsa.2023.0141053 (2023)	https://doi.org/10.18653/v1/2020.semeval-1.170 (2020)	The Indonesian-Javanese code-mixed data dataset is utilized for sentiment analysis, particularly in low-resource language contexts. Researchers employ a Sentence BERT pre-trained model and compare lexicon-based and transformer-based approaches with an attention mechanism. This dataset enables the evaluation of these methods, highlighting its relevance in mixed linguistic contexts and facilitating advancements in sentiment analysis for under-resourced languages.	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9609781	not released	4	not released
5567	Indonesian Corpus	citing_context	cross-domain dataset for health misinformation detection in Bahasa Indonesia	https://doi.org/10.14569/ijacsa.2025.01601117 (2025)	https://doi.org/10.1109/CITSM52892.2021.9589004 (2021)	The cross-domain dataset for health misinformation detection in Bahasa Indonesia is used to develop a comprehensive resource that encompasses diverse health topics. This dataset enables researchers to build and test models for detecting health misinformation, facilitating broader generalization across various health-related contexts. The dataset's diversity in health topics enhances its utility in addressing the complex landscape of health misinformation in the Indonesian language.	https://thesai.org/Publications/ViewPaper?Volume=16&Issue=1&Code=ijacsa&SerialNo=117	not released	4	not released
5615	Indonesian Corpus	citing_context	hate speech in Indonesian	https://doi.org/10.18280/jesa.580102 (2025)	https://doi.org/10.18653/v1/W19-3506 (2019)	The 'hate speech in Indonesian' dataset is used to detect hate speech and abusive language in Indonesian Twitter. Research focuses on multi-label classification and classifying tweets into hate speech and non-hate speech categories. The dataset enables the analysis of linguistic features of hate speech, supporting the development of models to identify and categorize abusive content.		not released	4	not released
5678	Indonesian Corpus	citing_context	Indonesian short answer dataset	https://doi.org/10.18280/ria.370502 (2023)	https://doi.org/10.1109/ICECCE52056.2021.9514223 (2021)	The Indonesian short answer dataset is used to assess short answers and predict scores in Indonesian, primarily through the application of Word2Vec embeddings and a Siamese Manhattan LSTM model for automatic essay scoring. This dataset enables researchers to develop and evaluate algorithms that can automatically score short answers, enhancing the efficiency and consistency of educational assessments.	https://ieeexplore.ieee.org/document/9514223	not released	4	not released
5517	Indonesian Corpus	cited_context | citing_context	100-article dataset	https://doi.org/10.18653/v1/2020.aacl-main.60 (2020)	https://www.semanticscholar.org/paper/49917b77c8dbbaf55da7194f8b0b7b92edfa1690 (2015)	The 100-article dataset is used to train a naive Bayes model for extracting summary sentences in Indonesian text summarization. It focuses on improving the accuracy of summarizing article content, employing a machine learning approach to enhance the quality and relevance of generated summaries.; The 100-article dataset is used to train a naive Bayes model for extracting summary sentences in Indonesian text summarization. It focuses on enhancing the accuracy of summarizing article content, specifically by improving the model's ability to identify and extract relevant summary sentences. This dataset enables researchers to evaluate and refine summarization techniques tailored to the Indonesian language.	https://scispace.com/pdf/indonesian-text-summarization-based-on-naive-bayes-method-513hy15g09.pdf	not released	4	not released
5522	Indonesian Corpus	cited_context | citing_context	2,000 sentences from a news portal	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	https://doi.org/10.1007/11563983_7 (2005)	The dataset of 2,000 sentences from a news portal is used for named entity recognition in Indonesian, focusing on annotating person, location, and organization entities within news text. This involves labeling specific terms to identify and categorize named entities, enabling research in natural language processing and improving the accuracy of entity recognition models in Indonesian news articles.; The dataset of 2,000 sentences from a news portal is used for named entity recognition in Indonesian, focusing on annotating person, location, and organization entities within news text. This involves labeling specific terms to identify and classify named entities, enabling research in natural language processing and improving the accuracy of entity recognition systems in Indonesian news articles.	https://link.springer.com/chapter/10.1007/11563983_7	not released	4	not released
5694	Indonesian Corpus	cited_context | citing_context	Indonesian Wikipedia	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020), https://doi.org/10.18653/v1/2020.aacl-main.60 (2020)	https://www.semanticscholar.org/paper/2c08e64edebbeafd4e85d68c08ba272a2455f75e (2003), https://www.semanticscholar.org/paper/6aa8282509e14ad793a3a800aca00d079b9913fb (2017)	The Indonesian Wikipedia dataset is primarily used to train BERT-based models, such as I NDO BERT and BERT-Base, by providing large volumes of Indonesian text (ranging from 55M to 90M words). This text, sourced from web and news articles, enhances the models' understanding and representation of the Indonesian language, improving their performance in language-related tasks.; The Indonesian Wikipedia dataset is primarily used to train BERT-Base models, contributing between 55M to 90M words of Indonesian text from web and news sources. This dataset enhances language understanding and representation, specifically supporting the development of I NDO BERT through language modeling. The large volume of text enables robust training, improving the model's performance in various natural language processing tasks.		not released	4	not released
5621	Indonesian Corpus	citing_context	Ibrohim and Budi (2018) dataset	https://doi.org/10.18653/v1/2023.acl-long.711 (2023)	https://doi.org/10.1016/J.PROCS.2018.08.169 (2018)	The Ibrohim and Budi (2018) dataset is used to detect and classify abusive language in Indonesian social media. It provides additional annotations for implicit hate, enhancing its utility for hate speech analysis. Researchers use this dataset to train and evaluate models, focusing on identifying and classifying abusive content. The dataset's detailed annotations enable more nuanced and accurate detection of abusive language.		not released	4	not released
5519	Indonesian Corpus	citing_context	13,169 tweets	https://doi.org/10.18653/v1/W19-3506 (2019)	https://doi.org/10.1016/J.PROCS.2018.08.169 (2018)	The dataset of 13,169 tweets is used in Indonesian social media research to detect and analyze abusive and hate speech. It employs machine learning models and annotation methods to identify offensive content and improve online safety. The dataset includes 7,608 non-hate speech tweets and 5,561 hate speech tweets, collected over two annotation phases, enabling robust experimentation and model training for hate speech detection.		not released	4	not released
5528	Indonesian Corpus	cited_context	5,700 hate speech tweets	https://doi.org/10.18653/v1/W19-3506 (2019)	https://doi.org/10.1016/J.PROCS.2018.08.169 (2018)	The '5,700 hate speech tweets' dataset is used to study and detect abusive language in Indonesian social media. It involves a two-phase annotation process to identify and classify offensive content, including both hate speech and non-hate speech tweets. This dataset helps researchers analyze abusive language patterns and improve detection methods across various social media genres.		not released	4	not released
5666	Indonesian Corpus	citing_context	Indonesian language trial results documents	https://doi.org/10.21456/vol14iss4pp311-320 (2024)	https://doi.org/10.1109/ICACSIS51025.2020.9263157 (2020)	The Indonesian language trial results documents dataset is used to evaluate the effectiveness of deep learning models, specifically Bi-LSTM and CRF approaches, in recognizing entities within Indonesian court decision documents. This evaluation focuses on the accuracy and performance of these models, providing insights into their applicability for processing legal texts in the Indonesian language.	https://ieeexplore.ieee.org/document/9263157	not released	4	not released
5594	Indonesian Corpus	citing_context	data from reviews on Google Playstore	https://doi.org/10.25139/inform.v9i1.5465 (2023)	https://doi.org/10.31294/instk.v2i1.419 (2021)	The dataset from Google Playstore reviews is used to gauge customer satisfaction, particularly for Shopee account holders. Researchers analyze the content of these reviews to assess user experience and sentiment. The methodology involves sentiment analysis and content analysis to extract insights from the textual data. This dataset enables researchers to understand user perceptions and identify areas for improvement in the app's functionality and user interface.		not released	4	not released
5778	Indonesian Corpus	cited_context	data opini di Twitter	https://doi.org/10.30864/EKSPLORA.V10I1.390 (2020)		The 'data opini di Twitter' dataset is used for sentiment analysis of opinions on Indonesian marketplace services. Researchers employ the Naive Bayes algorithm to classify sentiments, achieving high accuracy rates (93% to 93.33%). This dataset enables the evaluation of customer perceptions and satisfaction levels in online marketplaces, providing insights into service quality and user experience.		not released	4	not released
5656	Indonesian Corpus	citing_context	Indonesian-English Code-Mixed Twitter Data	https://doi.org/10.30865/mib.v7i4.6816 (2023)	https://doi.org/10.18653/v1/D19-5554 (2019)	The Indonesian-English Code-Mixed Twitter Data dataset is used to analyze and normalize code-mixed tweets, focusing on the use of slang and non-standard language in social media. Researchers employ this dataset to understand linguistic phenomena in multilingual online environments, specifically addressing how users blend Indonesian and English in their tweets. This dataset enables detailed examination of language normalization techniques and the sociolinguistic aspects of code-switching in digital communication.	https://aclanthology.org/D19-5554.pdf	not released	4	not released
5732	Indonesian Corpus	citing_context	Korpus Bahasa Indonesia untuk Deteksi Emosi dari Teks	https://doi.org/10.31937/ti.v14i1.2540 (2022)	https://www.semanticscholar.org/paper/7562204c80b95b7d5dfb3d1138fda76d903e7e41 (2019)	The 'Korpus Bahasa Indonesia untuk Deteksi Emosi dari Teks' dataset is used for sentiment analysis and emotion recognition in Indonesian text. It focuses on detecting emotional content from reader responses, employing methodologies that analyze textual data to identify and categorize emotions. This dataset enables researchers to explore how emotional states are expressed in written form, enhancing understanding of emotional communication in digital contexts.		not released	4	not released
5530	Indonesian Corpus	cited_context	5W1H-style news extraction and corpus	https://doi.org/10.3390/ijgi9120712 (2020)	https://doi.org/10.1109/ICAICTA.2015.7335365 (2015)	The 5W1H-style news extraction and corpus dataset is used to extract key news elements (who, what, where, when, why, and how) from Indonesian articles. It employs a structured extraction method to identify these elements, though it does not include geoparsed toponyms or detailed event semantics. This dataset enables researchers to analyze and categorize news content systematically, facilitating studies on media reporting and information dissemination in Indonesian contexts.	https://dl.acm.org/doi/10.3115/1119394.1119402	not released	4	not released
5572	Indonesian Corpus	citing_context	Data Tweet Lazada	https://doi.org/10.36080/idealis.v5i2.2961 (2022)		The 'Data Tweet Lazada' dataset is used to analyze public sentiment towards Lazada by employing text mining and machine learning techniques. It focuses on customer opinions and feedback expressed in Indonesian tweets. This dataset enables researchers to understand consumer perceptions and sentiments, providing insights into customer satisfaction and areas for improvement.	https://repository.binadarma.ac.id/690/1/Dodi%20Setian%20%28bab%200%29.pdf	not released	4	not released
5775	Indonesian Corpus	citing_context	datasets on existing Independent Learning Independent Campus (MBKM) programs	https://doi.org/10.37385/jaets.v6i1.4746 (2024)	https://doi.org/10.31219/osf.io/ujmte (2020)	The dataset on existing Independent Learning Independent Campus (MBKM) programs is used to develop and evaluate an Indonesian automatic question-and-answer system. It focuses on the content and structure of MBKM programs, enabling researchers to enhance the system's performance in generating accurate and contextually relevant answers.		not released	4	not released
5603	Indonesian Corpus	citing_context	Google Maps User Code-Mixed Reviews	https://doi.org/10.37859/jf.v13i02.5170 (2023)	https://doi.org/10.1016/j.procs.2023.01.221 (2023)	The Google Maps User Code-Mixed Reviews dataset is used for sentiment analysis across various sectors, including tourist attractions, airline services, and healthcare. Researchers employ language representation models and web scraping tools to handle dynamic content. Specific applications include analyzing user opinions and experiences at sites like Borobudur and Prambanan Temples, and testing BERT's performance on Indonesian language reviews. This dataset facilitates multilingual context analysis and enhances understanding of customer feedback.	https://aclanthology.org/2020.paclic-1.41/	not released	4	not released
5595	Indonesian Corpus	citing_context	GCP corpus	https://doi.org/10.48550/arXiv.2205.15960 (2022)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The GCP corpus is extensively used in cross-lingual and machine translation research, providing multilingual parallel texts for training and evaluating translation models. It supports the development of large monolingual dictionaries and linguistic analysis, particularly for Asian languages, including Indonesian. The dataset's wide coverage of over 20 languages, including low-resource languages, enhances translation quality and coverage, making it valuable for both high-resource and under-resourced language pairs. Its annotated parallel texts facilitate syntactic and semantic analysis, crucial for improving machine translation systems.		not released	4	not released
5600	Indonesian Corpus	cited_context | citing_context	Global Communication Plan (GCP) corpus	https://doi.org/10.48550/arXiv.2205.15960 (2022)	https://www.semanticscholar.org/paper/38142195ecbeda00b25108e435183c87a7a76ac3 (2018)	The Global Communication Plan (GCP) corpus is used to support machine translation studies by providing multilingual parallel texts. It is specifically utilized for training and evaluating translation models across multiple languages, including Indonesian. This dataset enables researchers to develop and assess the performance of translation systems, enhancing their accuracy and effectiveness in handling diverse linguistic data.; The Global Communication Plan (GCP) corpus is used in research to support machine translation studies, particularly focusing on multilingual parallel texts. It is employed for training and evaluating translation models, enabling researchers to improve the accuracy and fluency of translations across multiple languages. The dataset's multilingual nature is a key characteristic that facilitates these research efforts.	https://aclanthology.org/anthology-files/pdf/L/L18/L18-1545.pdf	not released	4	not released
5744	Indonesian Corpus	citing_context	MediaWiki Translations	https://doi.org/10.48550/arXiv.2206.07238 (2022)	https://doi.org/10.31091/MUDRA.V33I1.196 (2018)	The MediaWiki Translations dataset is used to study parallel translations between Indonesian and local languages, enhancing machine translation accuracy and lexical variation analysis. It includes parallel texts from Indonesian and the top five local languages, enabling researchers to train and evaluate machine translation models and conduct corpus analysis to support translation and lexical variation research.		not released	4	not released
5627	Indonesian Corpus	cited_context | citing_context	ID Multilabel HS	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.18653/v1/W19-3506 (2019)	The 'ID Multilabel HS' dataset is used to detect multi-label hate speech and abusive language in Indonesian Twitter. Research focuses on classification and annotation methodologies, employing machine learning techniques to identify and categorize various forms of offensive content. This dataset enables researchers to develop and evaluate models that can accurately classify multiple types of hate speech, enhancing the understanding and management of abusive language in social media.	https://aclanthology.org/W19-3506/	not released	4	not released
5629	Indonesian Corpus	cited_context | citing_context	ID Stance	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IALP.2018.8629144 (2018)	The ID Stance dataset is used to classify stances towards political figures in blog writing, specifically by analyzing Kompasiana articles that mention predefined Indonesian politicians. The dataset supports research in sentiment and opinion analysis, employing methods to categorize stances expressed in the articles. This enables detailed examination of public opinions and sentiments towards specific political figures.; The ID Stance dataset is used to classify stances towards political figures in blog writing, specifically by analyzing Kompasiana articles. It supports research in sentiment and opinion analysis, focusing on detecting stances towards predefined Indonesian politicians. The dataset enables researchers to employ classification methodologies to understand public opinions and sentiments expressed in online articles.	https://ieeexplore.ieee.org/document/8629144	not released	4	not released
5698	Indonesian Corpus	cited_context | citing_context	Indonesian WSD	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/41713cdd3af12f4fe8e50fee0d71ffb1a904a49e (2018)	The Indonesian WSD dataset is used for word sense disambiguation (WSD) in Indonesian, focusing on supervised learning methods. It is automatically collected using the CrossLingual WSD (CLWSD) approach, leveraging WordNet and the parallel corpus GIZA++. This dataset enables researchers to develop and evaluate WSD models specifically tailored for the Indonesian language, enhancing the accuracy of natural language processing tasks in this linguistic context.; The Indonesian WSD dataset is used for word sense disambiguation in Indonesian, focusing on improving supervised learning methods. It is automatically collected using the CrossLingual WSD (CLWSD) approach, leveraging WordNet and the parallel corpus GIZA++. This dataset enables researchers to enhance the accuracy of supervised learning models for Indonesian WSD tasks.	https://ieeexplore.ieee.org/document/8850957	not released	4	not released
5723	Indonesian Corpus	cited_context	Karonese dataset	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.21437/SLTU.2018-14 (2018)	The Karonese dataset is primarily used for developing and enhancing speech technologies, including automatic speech recognition, text-to-speech synthesis, and speech-to-text translation, with a focus on Indonesian and regional languages like Sundanese, Javanese, Batak, and Balinese. It supports high-quality speech data for training and testing models, particularly in conversational and news broadcast contexts. Additionally, the dataset is utilized for machine translation, sentiment analysis, and hate speech detection, addressing cross-lingual communication, emotional content in text, and online content moderation. Its multilingual and code-mixed nature makes it valuable for improving natural language processing and preserving under-resourced languages.	https://section.iaesonline.com/index.php/IJEEI/article/view/3565	not released	4	not released
5565	Indonesian Corpus	citing_context	COVID-19 KG Bahasa Indonesia	https://doi.org/10.48550/arXiv.2409.00061 (2024)	https://doi.org/10.1109/IBITeC59006.2023.10390908 (2023)	The 'COVID-19 KG Bahasa Indonesia' dataset is used to represent and query domain-specific knowledge about COVID-19 in the Indonesian language. It focuses on constructing a knowledge graph to facilitate the semantic representation and retrieval of pandemic-related data, enabling researchers to effectively manage and access information pertinent to the COVID-19 context in Bahasa Indonesia.	https://ieeexplore.ieee.org/document/10390908	not released	4	not released
5748	Indonesian Corpus	citing_context	Monas VQA dataset	https://doi.org/10.1109/ICACSIS51025.2020.9263149 (2020)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The Monas VQA dataset is used to develop and evaluate methods for visual question answering (VQA) specifically in the Indonesian language context. Researchers employ this dataset to enhance and test algorithms that can accurately answer questions about images using natural language processing techniques tailored for Indonesian. This dataset enables the assessment of VQA models' performance and robustness in a linguistically diverse setting.	https://ieeexplore.ieee.org/document/9263149	not released	4	not released
5756	Indonesian Corpus	cited_context | citing_context	NERP	https://doi.org/10.1109/KSE56063.2022.9953794 (2022), https://doi.org/10.1145/3592854 (2023), https://doi.org/10.18653/v1/2020.aacl-main.85 (2020), https://www.semanticscholar.org/paper/468723fead0a7adba99d0333dcafef432b6289b2 (2022)	https://doi.org/10.1109/IALP.2018.8629158 (2018)	The NERP dataset is primarily used for training and evaluating named entity recognition (NER) models on Indonesian news texts, focusing on improving entity tagging accuracy. It is also utilized for part-of-speech (POS) tagging, enhancing syntactic and morphological analysis. The dataset supports the development and testing of NLP models, including fine-tuning pre-trained models like IndoBERT and XLM-R, and is notable for its size, containing 8,400 sentences. It aids in various NLP tasks such as span extraction, sentiment aspect identification, and information retrieval.; The NERP dataset is primarily used for training and evaluating named entity recognition (NER) models in Indonesian, focusing on identifying and classifying entities such as persons, organizations, locations, and food and beverages in text. It is also utilized for developing and testing span extraction models for information retrieval and part-of-speech (POS) tagging models to enhance syntactic analysis and linguistic accuracy in Indonesian news text. The dataset supports fine-grained opinion mining by training models to identify sentiment aspects in text. Its specific focus on Indonesian text makes it valuable for improving language understanding and information retrieval tasks.		not released	4	not released
5566	Indonesian Corpus	citing_context	dataset created by Barik, Mahendra & Adriani (2019)	https://doi.org/10.7717/peerj-cs.1312 (2023)	https://doi.org/10.18653/v1/D19-5554 (2019)	The dataset created by Barik, Mahendra & Adriani (2019) is primarily used for studying code-mixed Indonesian-English social media content. It is employed in language identification experiments using CRF, achieving an F1 score of 89.58%, and in text normalization tasks to address the challenges of processing such content. Additionally, it is used to investigate the impact of code-mixed normalization on emotion classification, specifically on Twitter data. The dataset's focus on code-mixed content makes it valuable for these specialized linguistic and computational tasks.	https://aclanthology.org/D19-5554.pdf	not released	4	not released
5686	Indonesian Corpus	cited_context	Indonesian Treebank in the ParGram Parallel Treebank (ParGramBank)	https://www.semanticscholar.org/paper/68c1f36518e1e4f99197f894adea2284f5b7aca1 (2019)	https://www.semanticscholar.org/paper/bd295c8fdf638141cfab227c9692b2fbdc5c2146 (2013)	The Indonesian Treebank in the ParGram Parallel Treebank (ParGramBank) is used to study Lexical Functional Grammar in Indonesian, focusing on syntactic structures within a limited set of 79 sentences and 433 words. This dataset enables detailed analysis of Indonesian syntax, providing a structured resource for linguistic research. The small, curated size of the dataset facilitates in-depth examination of specific grammatical features and syntactic patterns.	https://aclanthology.org/P13-1054/	not released	4	not released
5792	Indonesian Corpus	cited_context	product reviews from Bukalapak	https://doi.org/10.30864/EKSPLORA.V10I1.390 (2020)	https://doi.org/10.20473/JISEBI.4.1.57-64 (2018)	The 'product reviews from Bukalapak' dataset is used to analyze user sentiment in product reviews from the Indonesian marketplace Bukalapak. Researchers employ Support Vector Machine (SVM) for classification, achieving 93.42% accuracy. This dataset enables the study of customer opinions and feedback, providing insights into consumer behavior and product perception in the Indonesian e-commerce context.		not released	4	not released
5794	Indonesian Corpus	cited_context	dataset provided from [18]	https://doi.org/10.1109/IAICT50021.2020.9172020 (2020)	https://doi.org/10.1109/ICTS.2017.8265649 (2017)	The dataset provided from [18] is used to conduct experiments on hoax news detection in the Indonesian language. It consists of 600 news articles collected from various web sources. Researchers employ this dataset to analyze and develop methods for identifying hoax news, focusing on the linguistic and contextual features of the articles. This dataset enables the evaluation of different detection models and algorithms, contributing to the development of more effective hoax news identification systems.	https://ieeexplore.ieee.org/document/8265649	not released	4	not released
5795	Indonesian Corpus	cited_context	real learner data	https://doi.org/10.14716/IJTECH.V8I5.878 (2017)	https://www.semanticscholar.org/paper/01e35a95572f8d1b80ab96abce1e1be2675e530e (2006)	The 'real learner data' dataset is used to evaluate the effectiveness of annotation schemes in correcting preposition errors made by Indonesian second language learners. This involves applying the annotation scheme to real-world learner data to assess its accuracy and practical utility in error correction. The dataset's focus on authentic learner errors enables researchers to test and refine methods for improving language learning outcomes.	https://www.semanticscholar.org/paper/Towards-Construction-of-an-Error-corrected-Corpus-Irmawati-Komachi/01e35a95572f8d1b80ab96abce1e1be2675e530e	not released	4	not released
5796	Indonesian Corpus	cited_context	review data from a website of the restaurant	https://doi.org/10.20473/JISEBI.4.1.57-64 (2018)		The dataset of review data from a restaurant website is used to analyze customer reviews, focusing on sentiment and feedback patterns in online reviews. Researchers employ text analysis methods to identify positive and negative sentiments, as well as common themes and issues raised by customers. This helps in understanding customer satisfaction and areas for improvement in restaurant services.		not released	4	not released
5798	Indonesian Corpus	citing_context	review Google map	https://doi.org/10.37859/jf.v13i02.5170 (2023)	https://doi.org/10.1109/JCSSE.2019.8864150 (2019)	The 'review Google map' dataset is used to collect customer reviews for sentiment analysis, particularly focusing on tourist attractions and airline services. Researchers employ language representation models to analyze these reviews, aiming to understand customer sentiments and experiences. This dataset enables detailed sentiment analysis by providing a rich source of user-generated content, facilitating the development and testing of natural language processing techniques.		not released	4	not released
5809	Indonesian Corpus	cited_context | citing_context	SmSA	https://doi.org/10.56127/ijst.v3i3.1739 (2024), https://doi.org/10.48550/arXiv.2205.15960 (2022), https://doi.org/10.48550/arXiv.2311.12405 (2023)	https://doi.org/10.1109/ICAICTA.2019.8904199 (2019), https://doi.org/10.1109/IALP.2018.8629262 (2018)	The SmSA dataset is used for creating a culturally relevant translated Indonesian sentiment analysis dataset, ensuring linguistic and cultural accuracy through skilled bilingual speakers. It is also utilized for multi-label aspect categorization, leveraging its high agreement in manual re-annotation and diverse topics. This dataset enables precise and contextually appropriate research in sentiment analysis and aspect categorization, enhancing the reliability and applicability of the results.; The SmSA dataset is primarily used for sentiment and emotion analysis in Indonesian social media, particularly Twitter. It is employed to train and evaluate models for identifying sentiments (positive, negative, neutral) and emotions (joy, anger, sadness, etc.). The dataset provides statistical data on the distribution and characteristics of sentiment and emotion labels, ensuring topic diversity and validating manual re-annotations. It contains 4,403 samples, making it suitable for both experimental and comparative studies.	https://ieeexplore.ieee.org/abstract/document/8904199	not released	4	not released
5812	Indonesian Corpus	cited_context | citing_context	speech corpus for an Indonesian speech-to-speech translation system	https://doi.org/10.1109/ICELTICs50595.2020.9315538 (2020)	https://doi.org/10.1109/ICSDA.2017.8384448 (2017)	The speech corpus dataset is used to develop a speech-to-speech translation system for Indonesian, focusing on collecting and processing speech data to enhance translation accuracy. This involves methodologies centered on data collection and processing techniques tailored to improve the performance of translation systems. The dataset's specific focus on Indonesian speech data enables researchers to address the unique linguistic challenges of the language, thereby enhancing the accuracy and reliability of speech-to-speech translation applications.; The dataset is used to develop a speech-to-speech translation system for Indonesian, focusing on the collection and processing of speech data to enhance translation accuracy. It enables researchers to build and refine models that can effectively translate spoken Indonesian into other languages, addressing the specific challenges of Indonesian phonetics and dialects.	https://ieeexplore.ieee.org/abstract/document/8384448	not released	4	not released
5821	Indonesian Corpus	citing_context	IndoAbbr		https://doi.org/10.1109/IALP51396.2020.9310514 (2020)	The dataset, containing 42,207 abbreviations from Indonesian news media, is used to train and evaluate abbreviation identification algorithms. It enhances models' ability to recognize and expand abbreviations in Indonesian text, specifically addressing the challenge of abbreviation handling in news articles. This dataset supports research in natural language processing, focusing on improving the accuracy and reliability of abbreviation recognition systems in Indonesian language contexts.	https://doi.org/10.1109/IALP51396.2020.9310514	not released	4	not released
5822	Indonesian Corpus	cited_context	System Usability Scale (SUS)	https://doi.org/10.14569/ijacsa.2022.0130950 (2022)	https://doi.org/10.1109/ICACSIS.2016.7872776 (2016)	The System Usability Scale (SUS) dataset is used to translate and adapt usability scale statements into Indonesian, ensuring cultural and linguistic accuracy. This adaptation focuses on measuring user satisfaction effectively in Indonesian contexts. The dataset enables researchers to conduct usability evaluations that account for local nuances, enhancing the reliability and validity of user experience assessments.	https://ieeexplore.ieee.org/document/7872776	not released	4	not released
5827	Indonesian Corpus	citing_context	datasets that combine Javanese-Indonesian and Sundanese-Indonesian text	https://doi.org/10.1109/SIML65326.2025.11081166 (2025)	https://doi.org/10.1145/3582768.3582771 (2022)	The dataset combining Javanese-Indonesian and Sundanese-Indonesian text is used to detect hate speech in code-mixed Indonesian social media. Researchers employ an abusive words lexicon to analyze the integration of these languages, focusing on identifying and characterizing abusive content. This approach helps in understanding the nuances of multilingual hate speech, enhancing the accuracy of detection models.	https://dl.acm.org/doi/abs/10.1145/3582768.3582771	not released	4	not released
5839	Indonesian Corpus	citing_context	Twitter Dataset for Hate Speech and Cyberbullying Detection in Indonesian Language	https://doi.org/10.14569/ijacsa.2023.01406125 (2023)	https://doi.org/10.1109/ICIMTech.2019.8843722 (2019)	The Twitter Dataset for Hate Speech and Cyberbullying Detection in Indonesian Language is used to identify and analyze linguistic patterns and abusive content in Indonesian social media, specifically Twitter. Researchers employ this dataset to detect hate speech and cyberbullying by examining the textual data, enabling them to understand and address harmful online behaviors.	https://ieeexplore.ieee.org/document/8843722	not released	4	not released
5841	Indonesian Corpus	citing_context	Twitter social media data	https://doi.org/10.1109/ICICoS62600.2024.10636859 (2024)	https://doi.org/10.1051/e3sconf/202235905001 (2022)	The Twitter social media data dataset is used for aspect-based sentiment analysis, specifically applying the K-Nearest Neighbors algorithm to Indonesian language content. This dataset enables researchers to analyze sentiments expressed in tweets, focusing on specific aspects or features of products, services, or topics. The dataset's relevance lies in its ability to capture real-time public opinions and reactions, providing valuable insights into consumer behavior and public sentiment in Indonesia.	https://www.e3s-conferences.org/articles/e3sconf/abs/2022/26/e3sconf_icenis2022_05001/e3sconf_icenis2022_05001.html	not released	4	not released
5730	Indonesian Corpus	cited_context	Kompas online collection	https://www.semanticscholar.org/paper/da19f6f033257a7831f53f238e37b4e5768e9f43 (2006)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003)	The Kompas online collection is used as a training set for information retrieval in Bahasa Indonesia, specifically to study the effects of stemming on retrieval performance. This dataset enables researchers to evaluate and optimize stemming algorithms, enhancing the accuracy and efficiency of information retrieval systems in the Indonesian language.		not released	4	not released
5853	Indonesian Corpus	citing_context	user reviews from Tokopedia	https://doi.org/10.1109/ICIMCIS60089.2023.10348996 (2023)	https://doi.org/10.1088/1757-899X/909/1/012071 (2020)	The 'user reviews from Tokopedia' dataset is used to analyze the correlation between quality services and user reviews in the Indonesian marketplace. Researchers employ sentiment analysis and develop recommendation systems, leveraging the dataset's focus on customer feedback to enhance service quality and user experience.	https://www.researchgate.net/publication/347825651_Sentiment_analysis_of_tokopedia_application_review_to_service_product_recommender_system_using_neural_collaborative_filtering_for_marketplace_in_Indonesia	not released	4	not released
5861	Indonesian Corpus	citing_context	World Loanword Database (WOLD)	https://doi.org/10.1515/cog-2021-0092 (2022)	https://doi.org/10.1515/9783110218442 (2009)	The World Loanword Database (WOLD) is used to study the prevalence and etymology of loanwords in Indonesian, specifically identifying 34% of words as loanwords, with 8.9% from the greater Java area. It supports comparative linguistic analysis and verifies the borrowing status of terms like 'wangi' from Javanese, Sundanese, or Balinese, providing etymological insights.	https://www.taylorfrancis.com/chapters/edit/10.4324/9781315644936-48/malay-indonesian-1-uri-tadmor	not released	4	not released
6397	Javanese Corpus	citing_context	dataset consisting of 1770 words	https://doi.org/10.11591/ijai.v13.i3.pp3498-3509 (2024)	https://doi.org/10.29303/jcosine.v4i1.346 (2020)	The dataset consisting of 1770 words is used to train and evaluate a hidden Markov model for Javanese part-of-speech tagging. This focuses on achieving high accuracy in linguistic annotation, enabling researchers to improve the precision of computational tools for analyzing Javanese language structure.	https://journals.bilpubgroup.com/index.php/fls/article/view/6957	not released	4	not released
6406	Javanese Corpus	citing_context	Javanese language dataset	https://doi.org/10.37287/JLH.V1I1.102 (2020)	https://doi.org/10.30998/JH.V3I1.90 (2019)	The Javanese language dataset is used to analyze abusive swearing variations in the Temanggung Javanese dialect. Researchers focus on identifying the types of abusive language and exploring the social realities and contexts in which such language is used. This dataset enables detailed linguistic and sociolinguistic analysis, providing insights into the cultural and social dynamics of the Temanggung Javanese community.	https://journal.unindra.ac.id/index.php/hortatori/article/view/90	not released	4	not released
6418	Javanese Corpus	citing_context	Version 1 (V1)	https://doi.org/10.1109/SIML65326.2025.11081055 (2025)	https://doi.org/10.1145/3639233.3639247 (2023)	The Version 1 (V1) dataset is used to investigate the impact of lexicon-based knowledge transfer for hate speech detection in Indonesia's code-mixed languages, particularly addressing the imbalance in Indo-Javanese and other language data. Researchers employ this dataset to develop and evaluate methods that enhance hate speech detection models, leveraging the dataset's focus on lexicon-based approaches to improve performance in underrepresented language contexts.	https://dl.acm.org/doi/10.1145/3639233.3639247	not released	4	not released
6404	Javanese Corpus	citing_context	Japanese Honorific Corpus	https://doi.org/10.48550/arXiv.2502.20864 (2025)		The Japanese Honorific Corpus is used to measure and compare lexical diversity in honorific systems, particularly through Yule’s K value. It serves as a comparative baseline for the Javanese Honorific Corpus, facilitating the integration of honorific distinctions into linguistic corpora and highlighting differences in lexical diversity between Japanese and Javanese honorifics. This dataset enables researchers to draw cross-linguistic insights and enhance the understanding of honorific usage in different cultural contexts.	https://aclanthology.org/2022.dclrl-1.3/	not released	4	not released
6405	Javanese Corpus	cited_context	Javanese and Sundanese corpora	https://www.semanticscholar.org/paper/d3f1c77ea3a4e381cfef8213fa6861928396172a (2020)	https://www.semanticscholar.org/paper/967db70850f4d79081e750e65aaa4e272cb472ba (2018)	The Javanese and Sundanese corpora are used to build multilingual text-to-speech systems, focusing on the integration of Javanese and Sundanese languages, often combined with Indonesian data. These corpora enable researchers to train and evaluate systems that handle linguistic features specific to these languages, facilitating the analysis of Malayo-Polynesian linguistic patterns and enhancing the performance of multilingual speech synthesis.	http://www.lrec-conf.org/proceedings/lrec2018/pdf/8888.pdf	not released	4	not released
6394	Javanese Corpus	cited_context | citing_context	10ID-10JV-10SU	https://doi.org/10.1109/ACCESS.2020.3027619 (2020)	https://www.semanticscholar.org/paper/21b786b3f870fc7fa247c143aa41de88b1fc6141 (2018)	The dataset '10ID-10JV-10SU' is used to fine-tune the WaveGlow model for multi-speaker, multilingual text-to-speech applications. It focuses on enhancing the model's capability to handle multiple languages and speakers, specifically improving the quality and versatility of synthesized speech across different linguistic contexts. This dataset enables researchers to address the challenge of creating more natural and adaptable speech synthesis systems.; The dataset '10ID-10JV-10SU' is used to fine-tune WaveGlow for multi-speaker, multilingual Text-to-Speech (TTS) systems, specifically focusing on the Javanese language. It employs a multi-speaker, multilingual dataset to enhance the TTS model's performance, addressing the nuances and specific characteristics of the Javanese language. This enables more natural and accurate speech synthesis for Javanese speakers.	https://ieeexplore.ieee.org/document/9208651	not released	4	not released
6557	Khmer Corpus	citing_context	TriECCC	https://doi.org/10.1109/ICASSP49357.2023.10095644 (2023)	https://doi.org/10.1109/O-COCOSDA202152914.2021.9660421 (2021)	The TriECCC dataset is used for domain adaptation in speech translation, focusing on Khmer, English, and French. It provides a trilingual corpus that facilitates training and evaluation of speech translation models, enabling researchers to improve cross-lingual performance and adapt models to specific domains.	https://www.worldscientific.com/doi/10.1142/S2717554522500072?srsltid=AfmBOooNFaXxp4Lbg-2ADsUiKuFT0IA0fAjGExbQAm0wdPjnLdH-EV6z	not released	4	not released
6541	Khmer Corpus	cited_context	ECCC corpus	https://doi.org/10.1109/O-COCOSDA202152914.2021.9660421 (2021)	https://doi.org/10.18653/v1/D19-5201 (2019)	The ECCC corpus is used for machine translation, specifically focusing on a Khmer-English bilingual text dataset to enhance translation quality. Researchers employ this corpus to develop and refine translation models, leveraging its bilingual text data to address challenges in cross-language information transfer and improve the accuracy and fluency of translations.	https://ieeexplore.ieee.org/document/9660421	not released	4	not released
6555	Khmer Corpus	citing_context	synthetic scene text	https://doi.org/10.48550/arXiv.2410.18277 (2024)	https://doi.org/10.1109/ACCESS.2023.3332361 (2023)	The 'synthetic scene text' dataset is mentioned in various citation contexts but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided in the given descriptions. Therefore, there is insufficient evidence to detail how this dataset is actually used in research.	https://ieeexplore.ieee.org/document/10316307	not released	4	not released
6556	Khmer Corpus	citing_context	translated MMLU	https://doi.org/10.48550/arXiv.2407.19672 (2024)	https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678 (2020)	The translated MMLU dataset is used to evaluate cross-lingual alignment, particularly focusing on a model's ability to handle Khmer language data. Despite the dataset's primary focus on Western knowledge, it is employed to assess how well models can transfer and apply this knowledge to a less-resourced language like Khmer. This evaluation helps researchers understand the limitations and capabilities of cross-lingual models in handling diverse linguistic contexts.	https://arxiv.org/pdf/2407.19672	not released	4	not released
6538	Khmer Corpus	cited_context | citing_context	BTEC	https://doi.org/10.1109/APSIPAASC47483.2019.9023137 (2019), https://www.semanticscholar.org/paper/18427ea04252bc943c47ade69ad85290057cf909 (2015)	https://doi.org/10.21437/Eurospeech.2003-150 (2003)	The BTEC dataset is mentioned in research citations but lacks detailed descriptions of its usage, methodology, or specific research applications. There is no explicit information on how it is employed in studies or its relevance to particular research questions or characteristics.; The BTEC dataset is used to collect travel-related expressions in twenty-one languages, focusing on multilingual travel communication and expression variability. Researchers employ this dataset to analyze and compare linguistic patterns and communication strategies across different languages, enhancing understanding of multilingual travel interactions. This dataset enables detailed examination of how travel-related concepts are expressed and understood in diverse linguistic contexts.	https://ieeexplore.ieee.org/document/9023137	not released	4	not released
6559	Kinyarwanda Corpus	cited_context	Affect in Tweets	https://doi.org/10.48550/arXiv.2304.06845 (2023)	https://doi.org/10.18653/v1/S18-1001 (2018)	The 'Affect in Tweets' dataset is used for sentiment analysis in code-mixed Kinyarwanda tweets, focusing on affective language and linguistic features in social media. Researchers employ this dataset to analyze emotional content and linguistic nuances, enabling studies on how emotions are expressed in multilingual online contexts.	https://aclanthology.org/S18-1001/	not released	4	not released
6568	Kinyarwanda Corpus	citing_context	Kinyarwanda monolingual dataset	https://doi.org/10.48550/arXiv.2402.08638 (2024)	https://doi.org/10.18653/v1/S16-1081 (2016)	The Kinyarwanda monolingual dataset is used to create a new monolingual STR (Semantic Textual Similarity) dataset for Kinyarwanda, specifically focusing on semantic textual similarity in multilingual research contexts. This involves employing methodologies that assess and compare the semantic equivalence of texts within the Kinyarwanda language. The dataset enables researchers to develop and evaluate models that can accurately measure textual similarity, contributing to advancements in multilingual natural language processing.	https://aclanthology.org/S16-1081/	not released	4	not released
6558	Kinyarwanda Corpus	cited_context	110K sentiment-labeled tweets	https://doi.org/10.48550/arXiv.2304.06845 (2023)	https://doi.org/10.3115/v1/S14-2009 (2014)	The '110K sentiment-labeled tweets' dataset is mentioned in research citations but lacks detailed usage descriptions. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular study. The dataset's actual use in research remains unspecified based on the provided information.	https://aclanthology.org/S14-2009/	not released	4	not released
6579	Kinyarwanda Corpus	cited_context | citing_context	small dataset of frequent words annotated by linguists	https://doi.org/10.18653/v1/2020.coling-main.409 (2020)	https://www.semanticscholar.org/paper/a2d87894b31c8621ec3c3d72328bdfb142ea4b5a (2013)	The 'small dataset of frequent words annotated by linguists' is mentioned in the literature but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided, and there is no evidence of its use in Kinyarwanda language research or any other specific linguistic studies. The dataset's role and impact in research remain unclear based on the available information.; The 'small dataset of frequent words annotated by linguists' is mentioned in research contexts but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided. The dataset's role and impact in linguistic studies are not explicitly outlined in the available information.	https://aclanthology.org/P13-1057.pdf	not released	4	not released
6802	Kyrgyz Corpus	citing_context	Kyrgyz-Russian Parallel Corpus	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.1109/BigComp57234.2023.00037 (2023)	The Kyrgyz-Russian Parallel Corpus is used to support machine translation and automatic speech recognition tasks, particularly for developing models for less-resourced Turkic languages. This dataset enables researchers to train and evaluate models that enhance the processing and understanding of these languages, focusing on improving translation accuracy and speech recognition performance.	https://www.hse.ru/en/ma/ling/students/diplomas/930858853	not released	4	not released
6804	Kyrgyz Corpus	citing_context	Kyrgyz-UzTurk	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.48550/arXiv.2205.06072 (2022)	The Kyrgyz-UzTurk dataset is used as a comprehensive linguistic resource for Central Asian languages, including Kyrgyz. It includes a variety of POS tags and syntactic features, enabling detailed linguistic analysis. Researchers use this dataset to explore grammatical structures and syntactic patterns, enhancing understanding of these languages' linguistic properties.	https://aclanthology.org/2022.sigul-1.26/	not released	4	not released
6832	Lingala Corpus	citing_context	Lingala Speech Translation (LiSTra)	https://www.semanticscholar.org/paper/397ccc27dfb952d0311461e4dfd47aead6bc0e0c (2022)	https://doi.org/10.1609/AAAI.V34I05.6360 (2019)	The Lingala Speech Translation (LiSTra) dataset is used to develop and evaluate speech translation systems in the Lingala language. Researchers compare traditional cascade and end-to-end architectures, focusing on interactive attention mechanisms. This dataset enables the assessment of different methodologies in speech translation, specifically addressing the performance and effectiveness of these systems in handling the Lingala language.	https://aclanthology.org/2022.dclrl-1.8/	not released	4	not released
6837	Maithili Corpus	citing_context	Maithili NER corpus	https://doi.org/10.1145/3533428 (2020)	https://www.semanticscholar.org/paper/35c6bcc3a86023f95b6be40aa6eddf8df52c027d (2020)	The Maithili NER corpus is used to develop and evaluate named entity recognition systems for the Maithili language, focusing on identifying and classifying named entities in text. This dataset enables researchers to improve the accuracy of NER models specific to Maithili, addressing the need for robust linguistic tools in under-resourced languages.	https://dl.acm.org/doi/abs/10.3233/JIFS-210051	not released	4	not released
7168	Marathi Corpus	cited_context | citing_context	2,622 Marathi Wikipedia articles	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1145/2389776.2389779 (2012)	The dataset of 2,622 Marathi Wikipedia articles is used to enhance multilingual entity filling through a language-independent approach, specifically focusing on named entity recognition within Marathi Wikipedia content. This involves identifying and classifying named entities in the articles, which aids in improving the accuracy and coverage of entity recognition systems for the Marathi language. The dataset's large volume of text provides a rich resource for training and evaluating these systems, enabling more effective multilingual natural language processing tasks.; The dataset of 2,622 Marathi Wikipedia articles is used to enhance multilingual entity filling, specifically focusing on named entity recognition within Marathi content. Researchers employ a language-independent approach to improve the accuracy of identifying and filling named entities, leveraging the rich textual data and structured information available in these articles. This methodology addresses the challenge of recognizing entities in low-resource languages like Marathi, thereby advancing natural language processing techniques.	https://doi.org/10.1145/2389776.2389779	not released	4	not released
7170	Marathi Corpus	cited_context	CFILT-annotated NER	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1145/3238797 (2018)	The CFILT-annotated NER dataset is utilized for named entity recognition (NER) in Indian languages, particularly Marathi. It contains 73,523 tokens, 6,138 sentences, and 6,036 named entities, and is used to train and evaluate NER models. The dataset focuses on identifying person, location, and organization tags, enhancing NER tagging performance through multilingual learning and creating a larger gold standard for low-resource settings.	https://doi.org/10.1145/3238797	not released	4	not released
7196	Marathi Corpus	cited_context | citing_context	InDEE-2019	https://doi.org/10.1145/3548457 (2022)	https://www.semanticscholar.org/paper/e6916adb30b22fc5a42174c8307444f05052cd17 (2019)	The InDEE-2019 dataset is used to address resource scarcity in natural disaster event extraction and to detect word-level plagiarism in Marathi text. It employs n-gram methods and rule-based discourse analysis, focusing on linguistic patterns and content from news websites. This dataset enables researchers to analyze natural disaster discourse and identify plagiarized content, enhancing understanding and detection capabilities in these areas.; The InDEE-2019 dataset is used to address resource scarcity in the natural disaster domain for Marathi and other languages by providing labeled data from news websites for event extraction and discourse analysis. It is also utilized to detect word-level plagiarism in Marathi text using the n-gram method, particularly in poems and user-collected summaries. This dataset enables researchers to enhance natural language processing techniques and improve the accuracy of event detection and plagiarism detection in low-resource languages.	https://www.arxiv.org/abs/1908.07018v1	not released	4	not released
7197	Marathi Corpus	citing_context	Indian Language Corpora	https://doi.org/10.18653/v1/2023.iwslt-1.43 (2023)	https://www.semanticscholar.org/paper/6b48ae58acc2d1a7d73c9c271c0df3307dd2e1c9 (2020)	The Indian Language Corpora dataset is used to collect speech data from low-income workers, particularly in low-resource languages such as Marathi, through crowd-sourcing methods. This approach facilitates the creation of speech corpora, enabling research focused on improving speech recognition and processing technologies for underrepresented linguistic communities.	https://aclanthology.org/2020.lrec-1.343/	not released	4	not released
7210	Marathi Corpus	citing_context	Kaun Banega Crorepati (KBC) Marathi questions	https://doi.org/10.48550/arXiv.2308.09862 (2023)	https://doi.org/10.1007/978-981-10-8569-7_4 (2018)	The Kaun Banega Crorepati (KBC) Marathi questions dataset is used to develop an English-Marathi QA system. It focuses on translating and classifying Marathi questions from a popular TV show, enabling researchers to enhance cross-lingual question answering capabilities. The dataset's relevance lies in its authentic, context-rich questions, which support the development and evaluation of translation and classification algorithms.	https://link.springer.com/chapter/10.1007/978-981-10-8569-7_4	not released	4	not released
7224	Marathi Corpus	citing_context	Marathhi question database	https://doi.org/10.1109/ICNTE56631.2023.10146625 (2023)	https://doi.org/10.2139/SSRN.3852112 (2021)	The Marathhi question database is used to develop a Marathi QA system, specifically focusing on 901 questions categorized into 50 different types. The dataset employs a keyword-based approach for classification. This enables researchers to address the challenge of creating effective question-answering systems for the Marathi language, enhancing natural language processing capabilities in this domain.	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3852112	not released	4	not released
7226	Marathi Corpus	citing_context	Marathi audio corpus	https://doi.org/10.1080/09720529.2021.2014134 (2022)	https://doi.org/10.1109/ICSDA.2013.6709893 (2013)	The Marathi audio corpus is used to collect audio samples from 303 males and 197 females, primarily for automatic speech recognition research. The dataset focuses on the creation and characteristics of the corpus, enabling the development and evaluation of speech recognition systems tailored to the Marathi language.	https://ieeexplore.ieee.org/document/6709893	not released	4	not released
7227	Marathi Corpus	cited_context | citing_context	Marathi collections	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1007/978-981-13-9187-3_2 (2018)	The 'Marathi collections' dataset is used to detect word-level plagiarism in Marathi text, employing the n-gram method to analyze poems and user-collected summaries. This dataset enables researchers to address specific issues of textual integrity and authenticity in Marathi literature, leveraging its rich content and structured format.	https://link.springer.com/chapter/10.1007/978-981-13-9187-3_2	not released	4	not released
7230	Marathi Corpus	citing_context	Marathi lexical resource for sentiment analysis	https://doi.org/10.1007/s13278-022-00877-w (2022)	https://doi.org/10.1007/978-981-16-0507-9_37 (2020)	The Marathi lexical resource for sentiment analysis is used to develop a sentiment analysis tool for the Marathi language, focusing on adjectives and adverbs. The dataset is created using corpus-based, SentiWordNet-based, and Hindi SentiWordNet-based approaches. It enables researchers to build and refine sentiment analysis models specifically tailored for Marathi, enhancing the accuracy of sentiment detection in Marathi texts.	https://link.springer.com/chapter/10.1007/978-981-16-0507-9_37	not released	4	not released
7234	Marathi Corpus	citing_context	Marathi question classification dataset		https://doi.org/10.1109/ICNTE56631.2023.10146625 (2023)	The Marathi question classification dataset is used to evaluate and compare model performance in classifying questions, specifically by translating the English TREC dataset into Marathi. This enables researchers to assess the effectiveness of models in handling Marathi language questions, focusing on classification accuracy and cross-lingual transfer capabilities.	https://ieeexplore.ieee.org/document/10146625	not released	4	not released
7235	Marathi Corpus	citing_context	Marathi Question Classifier dataset	https://doi.org/10.48550/arXiv.2309.15779 (2023)	https://doi.org/10.1007/978-981-10-8569-7_4 (2018)	The Marathi Question Classifier dataset is used to train machine learning classifiers to identify answer types for Marathi questions. It consists of 1000 manually translated questions, enabling researchers to develop and evaluate models that can accurately classify the type of answers required for these questions. This dataset supports research in natural language processing, specifically in the context of question classification for the Marathi language.	https://link.springer.com/chapter/10.1007/978-981-10-8569-7_4	not released	4	not released
7236	Marathi Corpus	citing_context	Marathi Quiz TV Show	https://doi.org/10.1109/ICAIS56108.2023.10073689 (2023)	https://doi.org/10.1007/978-981-10-8569-7_4 (2018)	The Marathi Quiz TV Show dataset is used to classify Marathi questions and identify answer types, specifically for training and evaluating machine learning models. This dataset enables researchers to develop and test algorithms that can accurately categorize and respond to Marathi-language queries, enhancing natural language processing capabilities in the Marathi language.	https://link.springer.com/chapter/10.1007/978-981-10-8569-7_4	not released	4	not released
7242	Marathi Corpus	cited_context	Marathi wiki	https://www.semanticscholar.org/paper/f77ec4c85a8505b9c9502ccbd94f79082a4689d5 (2018)	https://doi.org/10.18653/v1/K17-3009 (2017)	The Marathi wiki dataset is used to train external word embeddings, which enhance UDPipe's performance in processing Marathi text. Specifically, it improves tokenization, part-of-speech tagging, lemmatization, and parsing. This dataset enables researchers to develop more accurate natural language processing tools for the Marathi language, addressing the need for robust linguistic resources.		not released	4	not released
7243	Marathi Corpus	cited_context	Marathi Wikipedia articles	https://doi.org/10.1145/2389776.2389779 (2012)	https://doi.org/10.3115/1072399.1072415 (1995)	The Marathi Wikipedia articles dataset is used to evaluate the performance of named entity recognition systems, specifically focusing on manually tagged named entities within Marathi language articles. This dataset enables researchers to assess the accuracy and effectiveness of NER models tailored for the Marathi language, providing a benchmark for linguistic and computational analysis.		not released	4	not released
7254	Marathi Corpus	cited_context | citing_context	newspapers collected manually from the sports domain of Marathi text	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.35940/ijitee.b7023.129219 (2019)	The dataset of manually collected Marathi sports news articles is used to apply machine learning techniques for text classification. Research focuses on evaluating the effectiveness of various algorithms in categorizing these articles, leveraging the dataset's specific focus on the sports domain to enhance algorithm performance and accuracy.; The dataset of newspapers collected manually from the sports domain of Marathi text is used to apply machine learning techniques for text classification, specifically focusing on evaluating algorithm performance and accuracy in classifying Marathi sports news. This dataset enables researchers to test and refine models tailored to the Marathi language, enhancing the precision of text classification in this domain.	https://www.academia.edu/download/109680222/B7023129219.pdf	not released	4	not released
7260	Marathi Corpus	citing_context	speech Database	https://doi.org/10.35940/ijeat.b3832.029320 (2020)	https://www.semanticscholar.org/paper/eb98c1ab0b81187f49905dfa89a85bc7451c5554 (2016)	The 'speech Database' is used to build an Ahirani Automatic Speech Recognition (ASR) system using Hidden Markov Models (HMMs). It focuses on recognizing 20 isolated words in the Marathi dialect of Ahirani. This dataset enables researchers to develop and test ASR models tailored to this specific dialect, enhancing speech recognition accuracy in localized contexts.	https://www.ijcstjournal.org/volume-2/issue-3/IJCST-V2I3P27.pdf	not released	4	not released
7263	Marathi Corpus	cited_context | citing_context	Test corpus of 1,763 distinct words	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1109/ICCICT.2018.8325898 (2018)	The dataset of 1,763 distinct words is used to evaluate a hybrid part-of-speech (POS) tagger for Marathi sentences. Researchers focus on assessing the tagging accuracy and performance of the POS tagger, utilizing the dataset to measure its effectiveness in correctly identifying and tagging parts of speech in Marathi text. This enables the refinement and improvement of natural language processing tools for the Marathi language.	https://ieeexplore.ieee.org/abstract/document/8325898	not released	4	not released
7264	Marathi Corpus	cited_context | citing_context	Treebank for Indian languages	https://doi.org/10.1145/3548457 (2022)	https://www.semanticscholar.org/paper/67728616a406409dd804f598aa68afcd88f52870 (2017)	The Treebank for Indian languages dataset is used to develop a unified parsing strategy for major Indian languages, including Marathi. Researchers focus on syntactic structures and annotation standards, employing the dataset to enhance parsing accuracy and consistency across these languages. This enables more reliable linguistic analysis and computational processing of Indian languages.; The Treebank for Indian languages dataset is used to develop a unified parsing strategy for major Indian languages, including Marathi. It focuses on syntactic structures and linguistic annotations, enabling researchers to analyze and model the grammatical features of these languages. This dataset supports the creation of more accurate and linguistically informed parsers, enhancing natural language processing capabilities for Indian languages.	https://aclanthology.org/W17-6529.pdf	not released	4	not released
7268	Marathi Corpus	cited_context	Wiki+CommonCrawl (FT-WC)	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The Wiki+CommonCrawl (FT-WC) dataset is used to train word vectors for multiple languages, including Marathi, with a focus on cross-lingual representation learning. This involves employing methodologies that enable the creation of robust multilingual embeddings, which are crucial for enhancing the performance of natural language processing tasks across different languages. The dataset's extensive multilingual content supports the development of models that can effectively transfer knowledge between languages, facilitating more accurate and contextually rich language understanding and processing.	https://arxiv.org/pdf/1802.06893	not released	4	not released
7273	Min Nan Chinese Corpus	citing_context	Min-Mandarin bilingual speech recordings	https://www.semanticscholar.org/paper/ab3d2ba190cb3539e1e363d6c382aebab76ed8e2 (2024)	https://doi.org/10.4000/DISCOURS.5852 (2009)	The Min-Mandarin bilingual speech recordings dataset is used to study discourse segmentation in Min Nan Chinese, particularly focusing on spontaneous speech from sociolinguistic interviews conducted across Taiwan between 2004 and 2010. This dataset enables researchers to analyze natural language use, providing insights into the structural and contextual elements of Min Nan Chinese discourse.	https://aclanthology.org/2024.codi-1.5.pdf	not released	4	not released
7328	Nepali Corpus	cited_context | citing_context	enhanced NER corpus	https://doi.org/10.1007/s10462-021-10093-1 (2021)	https://www.semanticscholar.org/paper/8450c495467c30b0d860adec099d18b86821b482 (2014)	The enhanced NER corpus is used to train and evaluate deep learning-based named entity recognition (NER) models specifically for the Nepali language. This dataset enhances model accuracy by providing a larger and more diverse set of annotated data, which is crucial for improving the performance of NER tasks in low-resource languages like Nepali.; The enhanced NER corpus is used to train and evaluate deep learning-based named entity recognition (NER) models specifically for the Nepali language. This dataset focuses on improving the accuracy of NER tasks by providing a rich set of annotated data, enabling researchers to develop more effective models for identifying and classifying named entities in Nepali text.	https://www.semanticscholar.org/paper/Named-Entity-Recognition-for-Nepali-language%3A-A-Dey-Paul/8450c495467c30b0d860adec099d18b86821b482	not released	4	not released
7323	Nepali Corpus	citing_context	dataset by Bam	https://doi.org/10.1109/CCCS.2018.8586841 (2018)	https://doi.org/10.4236/IIM.2014.62004 (2014)	The 'dataset by Bam' is used to train and evaluate named entity recognition (NER) models for Nepali text, particularly focusing on news from political, educational, biographical, sports, and stock exchange domains. It employs support vector machine (SVM) models to identify entities in Nepali language data, enabling researchers to improve NER accuracy in these specific contexts.	https://www.scirp.org/journal/paperinformation?paperid=43828	not released	4	not released
7335	Nepali Corpus	citing_context	Nepali Alzheimer’s disease dataset	https://doi.org/10.1109/ACCESS.2023.3342154 (2023)	https://doi.org/10.1001/ARCHNEUR.1994.00540180063015 (1994)	The Nepali Alzheimer’s disease dataset is used to study Alzheimer’s disease in Nepali speakers by analyzing manually translated transcripts from 168 AD patients and 98 cognitively normal participants. It serves as the source data for creating the dataset, providing original transcripts that facilitate research into the linguistic and cognitive impacts of Alzheimer’s in this population.	https://www.sciencedirect.com/science/article/abs/pii/S1071581921001798	not released	4	not released
7326	Nepali Corpus	cited_context | citing_context	corpus containing 42 tags and 1,50,839 words in the Nepali language	https://doi.org/10.1007/s10462-021-10093-1 (2021)	https://doi.org/10.1109/ISACC.2015.7377332 (2015)	The dataset, comprising 42 tags and 1,50,839 words in the Nepali language, is used to train a statistical tagger for part-of-speech tagging. This corpus enhances the model's ability to handle linguistic variations, making it effective for accurately tagging parts of speech in Nepali text. The large size and diverse tag set improve the robustness and accuracy of the tagging model.; The dataset, containing 42 tags and 1,50,839 words in the Nepali language, is used to train a statistical tagger for part-of-speech tagging. The large corpus size supports robust model training, enabling accurate identification and classification of word types in Nepali text. This enhances natural language processing capabilities for the Nepali language.	https://ieeexplore.ieee.org/document/7377332	not released	4	not released
7359	Nepali Corpus	citing_context	Resource Management	https://doi.org/10.21437/SLTU.2018-11 (2018)	https://doi.org/10.1109/ICASSP.1988.196669 (1988)	The 'Resource Management' dataset is used to train acoustic models for Nepali language continuous speech recognition. Researchers employ Kaldi recipes to enhance the model's performance. This dataset specifically supports the development and improvement of speech recognition systems for the Nepali language, leveraging its acoustic data to refine model accuracy.	https://ieeexplore.ieee.org/document/196669	not released	4	not released
7338	Nepali Corpus	citing_context	Nepali MCQ dataset	https://doi.org/10.3126/kjse.v9i1.78368 (2025)	https://doi.org/10.18653/v1/2024.acl-long.44 (2023)	The Nepali MCQ dataset is used to create an extractive QA dataset in Nepali, leveraging its parallel reading comprehension structure. This enhances question generation and answer extraction, focusing on improving natural language processing tasks specific to the Nepali language. The dataset's structured format supports the development of more accurate and contextually relevant QA systems.	https://aclanthology.org/2024.acl-long.44/	not released	4	not released
7362	Nepali Corpus	cited_context	speech corpora in Nepali	https://doi.org/10.21437/SLTU.2018-11 (2018)	https://www.semanticscholar.org/paper/967db70850f4d79081e750e65aaa4e272cb472ba (2018)	The speech corpora in Nepali is used to develop and evaluate multilingual text-to-speech systems, specifically focusing on the Nepali language. Researchers utilize the corpus to investigate phonetic and prosodic characteristics of Nepali speech, enhancing the accuracy and naturalness of synthesized speech. This dataset supports the creation of more linguistically diverse and culturally relevant text-to-speech technologies.	http://www.lrec-conf.org/proceedings/lrec2018/pdf/8888.pdf	not released	4	not released
7330	Nepali Corpus	citing_context	corpus for Nepali document		https://doi.org/10.1109/CCCS.2018.8586841 (2018)	The 'corpus for Nepali document' dataset is used to study information retrieval in the Nepali language, focusing on news articles from various categories such as political, educational, biography, sports, and stock exchange. Researchers employ this dataset to analyze and improve retrieval methods, leveraging its diverse content to address specific challenges in Nepali language processing.	https://doi.org/10.1109/CCCS.2018.8586841	not released	4	not released
7349	Nepali Corpus	cited_context	Nepali WORDNET	https://doi.org/10.4236/IIM.2013.56018 (2013)	https://www.semanticscholar.org/paper/4b69af4ab75f8f7ee2de9deef383b6cc38f546bf (2009)	The Nepali WORDNET dataset is used to build a rich lexical resource for the Nepali language, specifically enhancing machine translation capabilities. It provides semantic relationships between words, which improves the accuracy and context of translations. This dataset enables researchers to develop more sophisticated natural language processing tools for Nepali, addressing the need for better linguistic resources in under-resourced languages.	https://www.semanticscholar.org/paper/Experiences-in-building-the-Nepali-WordNet-insights-Chakrabarty/4b69af4ab75f8f7ee2de9deef383b6cc38f546bf	not released	4	not released
7360	Nepali Corpus	cited_context | citing_context	self-created anaphora-annotated data-set	https://doi.org/10.1007/s10462-021-10093-1 (2021)	https://doi.org/10.1109/compe49325.2020 (2020)	The self-created anaphora-annotated dataset is used for anaphora resolution in Nepali, integrating both language-dependent features such as prefix and suffix inflections in nouns and language-independent features. This dataset enables researchers to develop and test algorithms that accurately resolve anaphoric references, enhancing natural language processing capabilities in Nepali.; The self-created anaphora-annotated dataset is used for anaphora resolution in Nepali, integrating both language-dependent features such as prefix and suffix inflections in nouns and language-independent features. This dataset enables researchers to develop and test algorithms that resolve anaphoric references, enhancing natural language processing capabilities in Nepali.	https://ieeexplore.ieee.org/document/9200135	not released	4	not released
7363	Nepali Corpus	citing_context	test dataset	https://doi.org/10.3115/v1/W14-3916 (2014)	https://doi.org/10.3115/v1/W14-3907 (2014)	The 'test dataset' is used for training and evaluating a language identification system on social media content, specifically tweets and Facebook posts with code-switching between Spanish/English and Nepali/English. It focuses on n-gram prefix patterns to assess the system's robustness and performance on different platforms. This dataset enables researchers to test and improve language identification models in multilingual and code-switching contexts.	https://aclanthology.org/W14-3916.pdf	not released	4	not released
7337	Nepali Corpus	cited_context | citing_context	Nepali lexicon	https://doi.org/10.1007/s10462-021-10093-1 (2021)	https://doi.org/10.4236/IIM.2014.62004 (2014)	The Nepali lexicon dataset, introduced in 2004, is utilized for foundational Natural Language Processing (NLP) tasks in the Nepali language. It includes root words, head words, pronunciations, parts of speech, synonyms, and idioms. Researchers use this comprehensive resource to develop and enhance NLP systems, focusing on linguistic analysis and language understanding. The dataset's rich lexical content supports the creation of more accurate and contextually aware NLP models for the Nepali language.; The Nepali lexicon dataset, introduced in 2004, serves as a foundational resource for Nepali Natural Language Processing (NLP). It includes root words, head words, pronunciations, parts of speech, synonyms, and idioms. Researchers use this dataset to develop and enhance NLP tools and systems, focusing on linguistic analysis and language technology applications. The comprehensive nature of the lexicon supports the creation of more accurate and contextually rich NLP models for the Nepali language.	https://www.scirp.org/journal/paperinformation?paperid=43828	not released	4	not released
7393	Nigerian Pidgin Corpus	cited_context | citing_context	corpus published in (Deuber, 2005)	https://www.semanticscholar.org/paper/fa01cc42f9d67fd7d50c6e15ba4d9d87cfec3ce3 (2017)	https://www.semanticscholar.org/paper/df3d90db3450b60e86cb95146629b658eedbc6bb (2005)	The corpus published in (Deuber, 2005) is used to create a lexicon of Nigerian Pidgin words, focusing on language contact, variation, and change in an African urban setting. Researchers employ the corpus's transcribed files for detailed linguistic analysis, enabling the study of lexical development and sociolinguistic patterns.; The corpus published in (Deuber, 2005) is used to create a lexicon of Nigerian Pidgin words, focusing on language contact, variation, and change in an African urban setting. Researchers employ the corpus's transcribed files for linguistic analysis, enabling detailed examination of lexical items and their usage in context. This dataset facilitates the study of linguistic dynamics and evolution within urban environments.	https://www.semanticscholar.org/paper/Nigerian-Pidgin-in-Lagos%3A-Language-contact%2C-and-in-Deuber/df3d90db3450b60e86cb95146629b658eedbc6bb	not released	4	not released
7372	Nigerian Pidgin Corpus	citing_context	DiscoNaija	https://doi.org/10.48550/arXiv.2406.18776 (2024)	https://doi.org/10.1007/s10579-025-09850-3 (2025)	The DiscoNaija dataset is used to create a discourse-annotated parallel corpus of Nigerian Pidgin and English. Researchers focus on analyzing syntactic and lexical similarities to infer discourse relations in Nigerian Pidgin. This methodology enables a deeper understanding of the linguistic structure and discourse patterns in Nigerian Pidgin, facilitating comparative linguistic studies and enhancing the annotation of discourse markers in under-resourced languages.	https://link.springer.com/article/10.1007/s10579-025-09850-3	not released	4	not released
7386	Nigerian Pidgin Corpus	citing_context	Nigerian Pidgin English corpus	https://doi.org/10.18653/v1/2021.conll-1.5 (2021)	https://doi.org/10.18653/v1/W19-7803 (2019)	The Nigerian Pidgin English corpus is used to train and evaluate models for various natural language processing tasks, including dependency parsing, sentiment analysis, and named entity recognition. It facilitates machine translation from Nigerian Pidgin to English and supports the development of speech recognition systems, enhancing accuracy in transcribing spoken language. The dataset also aids in analyzing syntactic structures and collecting parallel text, particularly from religious sources, supporting linguistic and computational studies in low-resource languages.	https://aclanthology.org/W19-7803.pdf	not released	4	not released
7397	Nigerian Pidgin Corpus	citing_context	WAPE (BBC)	https://doi.org/10.18653/v1/2025.findings-naacl.85 (2024)	https://doi.org/10.3115/1073083.1073135 (2002)	The WAPE (BBC) dataset is used to evaluate text generation metrics in Nigerian Pidgin, specifically comparing similarity to English. Researchers focus on metrics such as BLEU, ChrF++, and BERTScore to assess the performance and quality of generated Pidgin text. This dataset enables the analysis of these metrics, providing insights into the effectiveness of text generation models in a less-resourced language.	https://aclanthology.org/2025.findings-naacl.85.pdf	not released	4	not released
7402	Northeastern Thai Corpus	citing_context	iAPP	https://doi.org/10.48550/arXiv.2504.05898 (2025)	https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The iAPP dataset is used to train and evaluate abstractive summarization and question-answering models for Thai local dialects, particularly focusing on Northeastern Thai. It supports multilingual summarization performance and human evaluation methods, providing QA and summarization samples to assess model effectiveness in these specific linguistic contexts.	https://aclanthology.org/2021.findings-acl.413/	not released	4	not released
7404	Northern Kurdish Corpus	citing_context	Annotated Gumar Corpus	https://doi.org/10.18653/v1/2021.sigmorphon-1.25 (2021)	https://www.semanticscholar.org/paper/2156f2f4591ebb8d831c95b153b578ab58f1fed3 (2018)	The Annotated Gumar Corpus is used for extracting morphological annotations of nouns and adjectives, focusing on linguistic analysis and evaluating annotation quality. This dataset supports research in morphological structures and annotation methodologies, enhancing the accuracy and reliability of linguistic studies.	https://aclanthology.org/L18-1607.pdf	not released	4	not released
7411	Northern Pashto Corpus	cited_context	Northern Pashto language dataset	https://doi.org/10.22581/MUET1982.2101.14 (2021)	https://www.semanticscholar.org/paper/4eb31f206dd4cb2663b520dfdef86d9ae3e532a6 (2015)	The Northern Pashto language dataset is inferred to be relevant to the research topic of Northern Pashto language, though specific usage details are not provided. It is likely used to support linguistic studies, potentially involving analysis of language structure, dialectal variations, or sociolinguistic aspects. However, without explicit methodological or application details, the exact research questions and approaches remain unspecified.	https://www.semanticscholar.org/paper/A-Hybrid-Approach-for-NER-System-for-Scarce-%3A-with-Naz-Umar/4eb31f206dd4cb2663b520dfdef86d9ae3e532a6	not released	4	not released
7405	Northern Pashto Corpus	cited_context	1000 Pashto unique ligatures	https://doi.org/10.3390/electronics10202508 (2021)	https://doi.org/10.1007/s10586-017-0916-2 (2017)	The '1000 Pashto unique ligatures' dataset is used to study Pashto ligature recognition. Researchers focus on creating and analyzing this dataset to understand the formation and visual characteristics of these ligatures. This involves methodologies centered on pattern recognition and linguistic analysis, enabling the development of more accurate computational models for Pashto script processing.	https://link.springer.com/article/10.1007/s10586-017-0916-2	not released	4	not released
7412	Northern Pashto Corpus	citing_context	dataset of 8K images covering 1K unique Pashto texts	https://doi.org/10.48550/arXiv.2505.10055 (2025)	https://doi.org/10.1371/journal.pone.0133648 (2015)	The dataset of 8K images covering 1K unique Pashto texts is used to train and evaluate a robust OCR method for Pashto script. The research focuses on achieving scale, rotation, and location invariances in cursive writing, leveraging the dataset's extensive coverage of unique texts to enhance the OCR system's performance and reliability.	https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133648	not released	4	not released
7406	Northern Pashto Corpus	citing_context	Benchmark Pashto Handwritten Character Dataset	https://doi.org/10.3390/s23136060 (2023)	https://doi.org/10.1155/2021/6669672 (2021)	The Benchmark Pashto Handwritten Character Dataset is used to develop and evaluate Optical Character Recognition (OCR) systems for handwritten Pashto characters. Researchers focus on deep neural network models, particularly those employing rule activation functions, to improve recognition accuracy. This dataset enables the testing and refinement of these models, addressing the specific challenges of recognizing handwritten Pashto script.	https://dl.acm.org/doi/abs/10.1155/2021/6669672	not released	4	not released
7430	Northern Sotho Corpus	citing_context	SAfriSenti corpus	https://doi.org/10.1371/journal.pone.0325102 (2025)	https://www.semanticscholar.org/paper/004849a0e4f8a5de90157c72571763fcd25674ab (2022)	The SAfriSenti corpus is used to explore sentiment analysis in under-resourced South African languages, particularly in multilingual contexts. It supports AI for Social Good applications by enabling researchers to develop and evaluate sentiment analysis models in these languages. The dataset's focus on multilingual data makes it valuable for enhancing natural language processing capabilities in diverse linguistic environments.	https://aclanthology.org/2022.sigul-1.9/	not released	4	not released
7426	Northern Sotho Corpus	cited_context	Northern Sotho language dataset	https://doi.org/10.3115/1564508.1564516 (2009)	https://www.semanticscholar.org/paper/414f849fcc7303348a2a041601fce724cfa1d021 (2004)	The Northern Sotho language dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how this dataset is used in research based on the provided evidence.	https://dl.acm.org/doi/10.5555/1564508.1564516	not released	4	not released
7517	Odia Corpus	citing_context	handwritten Odia numeral dataset	https://doi.org/10.1109/ICINPRO43533.2018.9096757 (2018)	https://doi.org/10.1007/s11704-014-3354-9 (2014)	The handwritten Odia numeral dataset is used to train and evaluate models for recognizing Odia numerals, achieving high accuracy rates (96.3%) with a dataset of 2500 samples. This dataset enables researchers to develop and test algorithms specifically tailored for Odia numeral recognition, contributing to advancements in Optical Character Recognition (OCR) technology for underrepresented scripts.	https://ieeexplore.ieee.org/document/7490025	not released	4	not released
7515	Odia Corpus	cited_context	dataset containing 150 sample per class	https://doi.org/10.1007/s11042-020-09457-6 (2020)	https://doi.org/10.1007/978-981-10-6872-0_18 (2018)	The dataset containing 150 samples per class is used to train and evaluate models for Odia character recognition. Specifically, it focuses on assessing the performance of models using Discrete Wavelet Transform (DWT) and Principal Component Analysis (PCA). This dataset enables researchers to compare the effectiveness of these techniques in recognizing Odia characters, providing insights into optimal feature extraction methods for this specific language.	https://link.springer.com/chapter/10.1007/978-981-10-6872-0_18	not released	4	not released
7526	Odia Corpus	cited_context	NIT, RKL handwritten Odia Character	https://doi.org/10.1109/CINE.2017.27 (2017)	https://link.springer.com/article/10.1007/s11704-014-3354-9 (2014)	The NIT, RKL dataset is used for offline recognition of handwritten Odia characters and numerals, focusing on improving recognition accuracy. Studies employ standard benchmarks to evaluate performance. This dataset enables researchers to develop and test algorithms specifically tailored for recognizing handwritten elements in the Odia language, enhancing the accuracy and reliability of character and numeral recognition systems.	https://link.springer.com/article/10.1007/s11704-014-3354-9	not released	4	not released
7534	Odia Corpus	citing_context	Odia Poetry with Sentiment Polarity Information	https://www.semanticscholar.org/paper/b9ff097425eb3f169e0479086beb031b84b41e0a (2020)	https://www.semanticscholar.org/paper/86692421c4a0198cf10935a6466013f7b82a1fa4	The 'Odia Poetry with Sentiment Polarity Information' dataset is used to enhance feature extraction and improve classification accuracy in sentiment analysis of Odia poetry. It supports the creation of an annotated corpus, enabling researchers to focus on sentiment polarity. This dataset facilitates more accurate sentiment classification and contributes to the development of sentiment analysis models tailored for the Odia language.	http://lrec-conf.org/workshops/lrec2018/W11/pdf/15_W11.pdf	not released	4	not released
7535	Odia Corpus	cited_context | citing_context	dataset of 2500 handwritten samples	https://www.semanticscholar.org/paper/0c10ef5c9c74ea81ae7bf012bf212c7b3b2379de (2016)	https://doi.org/10.1007/s11704-014-3354-9 (2014)	The dataset of 2500 handwritten samples is mentioned in the citation context but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.; The dataset of 2500 handwritten samples is used for Odia numeral recognition and classification. It is specifically employed to train and evaluate machine learning models for Odia language processing. The dataset's large number of handwritten samples enables robust model training and evaluation, enhancing the accuracy of numeral recognition in the Odia language.	https://link.springer.com/article/10.1007/s11704-014-3354-9	not released	4	not released
7522	Odia Corpus	citing_context	ISI Kolkata dataset	https://doi.org/10.1080/13682199.2022.2163348 (2022)	https://doi.org/10.1109/ICDAR.2005.183 (2005)	The ISI Kolkata dataset is mentioned in the citation context of various research papers, but no detailed descriptions of its usage, methodology, research questions, or specific characteristics are provided. Therefore, based on the available information, there is insufficient evidence to describe how this dataset is actually used in research.	https://inria.hal.science/inria-00104486v1	not released	4	not released
8571	Sadri Corpus	citing_context	A novel comprehensive database for offline Persian handwriting recognition	https://doi.org/10.1109/AISP61396.2024.10475269 (2024)	https://doi.org/10.1109/IRANIANMVIP.2011.6121550 (2011)	The Sadri dataset is used to evaluate methods for Farsi word-level recognition, particularly in clustering word images in offline handwritten systems. This involves analyzing the effectiveness of the proposed methodologies in recognizing and categorizing handwritten Farsi words, leveraging the dataset's specific characteristics to enhance the accuracy and reliability of recognition algorithms.	https://www.sciencedirect.com/science/article/pii/S0031320316300097	not released	4	not released
8651	Setswana Corpus	citing_context	104 Setswana books	https://doi.org/10.48550/arXiv.2408.02239 (2024)	https://doi.org/10.55492/dhasa.v3i03.3819 (2021)	The Rare Tswana Books collection is used to source texts for linguistic analysis, specifically focusing on rare African books written in Setswana from the 1800s. Researchers employ this dataset to study historical linguistic patterns and changes, leveraging the unique historical context and content of these rare books. This enables in-depth analysis of the evolution of the Setswana language over time.	https://upjournals.up.ac.za/index.php/dhasa/article/view/3819/3524	not released	4	not released
8638	Setswana Corpus	citing_context	JW300 monolingual dataset for Setswana	https://doi.org/10.48550/arXiv.2310.09141 (2023)	https://www.semanticscholar.org/paper/7a0640db17b461b75df21f737154ad0f6d43e9de (2020)	The JW300 monolingual dataset for Setswana is used as an input corpus for Setswana language processing, specifically to support language modeling and other NLP tasks. It provides monolingual text that facilitates the development and evaluation of algorithms and models tailored for the Setswana language. This dataset enables researchers to enhance the performance and accuracy of NLP applications in Setswana.	https://huggingface.co/dsfsi/PuoBERTaJW300	not released	4	not released
8654	Setswana Corpus	citing_context	SAfriSenti	https://doi.org/10.1371/journal.pone.0325102 (2025)	https://www.semanticscholar.org/paper/004849a0e4f8a5de90157c72571763fcd25674ab (2022)	The SAfriSenti dataset is used to develop sentiment analysis models for South African under-resourced languages, focusing on applications in AI for Social Good. Researchers employ this dataset to explore and enhance AI capabilities in social contexts, leveraging its annotated sentiment data to build more inclusive and effective natural language processing systems.	https://aclanthology.org/2022.sigul-1.9/	not released	4	not released
8666	Sindhi Corpus	cited_context	Corpus (Rahman, 2010)	https://doi.org/10.48550/arXiv.2408.15720 (2024)	https://doi.org/10.18653/v1/N16-2008 (2016)	The Corpus (Rahman, 2010) is utilized for enhancing Sindhi language processing capabilities through downstream NLP tasks. It provides 100K tokens to improve language technology tools and contains 4.1M tokens for developing specific tasks such as POS tagging and named-entity recognition. This dataset enables researchers to build and refine models for Sindhi, addressing the need for robust language processing resources in this domain.	https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3820418	not released	4	not released
8667	Sindhi Corpus	citing_context	database for Sindhi language	https://www.semanticscholar.org/paper/15f224b4725a363f11f40474a851dc250cda9638 (2019)	https://doi.org/10.1109/ICETAS.2018.8629110 (2018)	The Sindhi language dataset is used to store and process Sindh image text, supporting OCR systems for recognizing both handwritten and printed Sindhi letters. It is essential for developing and evaluating OCR systems, particularly addressing the lack of multi-font and multi-size printed Sindhi character recognition systems. This dataset enables researchers to improve the accuracy and robustness of Sindhi script OCR technologies.	https://ieeexplore.ieee.org/document/8629110	not released	4	not released
8672	Sindhi Corpus	citing_context	POS annotated corpus of 37162 words	https://www.semanticscholar.org/paper/a6fbd88a5991c8a767c9d14a0089772f9f4c48f7 (2016)	https://www.semanticscholar.org/paper/887d8d4bae488941d9a3a26ec39122abc84fad79 (2010)	The 'POS annotated corpus of 37162 words' is mentioned in research citations but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided. Its role in enabling research is unclear from the given information. The dataset's characteristics and relevance to any particular research area are not specified.	https://aclanthology.org/2021.icon-main.75.pdf	not released	4	not released
8673	Sindhi Corpus	cited_context	POS tagged corpus of Sindhi-Devanagari	https://doi.org/10.26615/issn.2603-2821.2021_004 (2021)	https://doi.org/10.13140/RG.2.2.35194.03524 (2019)	The POS tagged corpus of Sindhi-Devanagari is used to compare the size and utility of Sindhi-Devanagari and Sindhi-Roman corpora, focusing on part-of-speech tagging, token count, and sentence numbers. It is also employed to train and evaluate part-of-speech tagging models for Sindhi in both Devanagari and Roman scripts, emphasizing linguistic accuracy and coverage. This dataset facilitates research by providing a standardized resource for linguistic analysis and model evaluation.	http://paper.ijcsns.org/07_book/201911/20191113.pdf	not released	4	not released
8674	Sindhi Corpus	cited_context | citing_context	SDSEG	https://doi.org/10.1109/ACCESS.2024.3507382 (2020)	https://www.semanticscholar.org/paper/867fd4ef5ed039761ac7ece131bc42daac5280a5 (2017)	The SDSEG dataset is used to evaluate model performance in cross-lingual word segmentation and morpheme segmentation tasks, focusing on precision, recall, and F1-score metrics. It serves as a benchmark for sequence labelling approaches, enabling researchers to assess and compare the effectiveness of different models in these linguistic tasks.; The SDSEG dataset is used to evaluate precision, recall, and F1-scores for cross-lingual word and morpheme segmentation tasks. It enables researchers to assess the performance of segmentation models across different languages, providing a standardized benchmark for comparing experimental results. The dataset's focus on morphological analysis supports the development and refinement of natural language processing techniques.	https://ieeexplore.ieee.org/document/10769409	not released	4	not released
8676	Sindhi Corpus	citing_context	Sindhi News Headlines Dataset (SNHD)	https://doi.org/10.1109/ACCESS.2025.3576853 (2025)	https://doi.org/10.17485/ijst/2019/v12i33/146130 (2019)	The Sindhi news headline classification dataset is used to evaluate advanced models in Sindhi language processing, particularly for part-of-speech tagging. It consists of 2,800 headlines categorized into five topics: Entertainment, Sports, Science and Technology, International, and National. This dataset enables researchers to assess model performance in classifying Sindhi headlines, contributing to the development of more accurate natural language processing techniques for the Sindhi language.	https://ieeexplore.ieee.org/document/11025811	not released	4	not released
8677	Sindhi Corpus	citing_context	Sindhi printed dataset		https://doi.org/10.5815/ijitcs.2019.06.02 (2019)	The Sindhi printed dataset is used to train and evaluate character recognition models, focusing on the effectiveness of different thinning algorithms. This dataset enables researchers to analyze and compare various thinning techniques in character recognition tasks, contributing to the development of more accurate and efficient recognition systems.	https://www.mecs-press.org/ijitcs/ijitcs-v11-n6/v11n6-2.html	not released	4	not released
8679	Sindhi Corpus	cited_context	Sindhi text sentiment analysis dataset	https://doi.org/10.1016/j.dib.2018.05.062 (2018)	https://doi.org/10.14569/IJACSA.2017.081038 (2017)	The Sindhi text sentiment analysis dataset is used for evaluating sentiment classification models in the Sindhi language. It consists of 19 attributes and 6841 records, enabling researchers to assess the performance of various sentiment analysis techniques specifically tailored for Sindhi text. This dataset supports the development and refinement of models that can accurately classify sentiments in Sindhi language data.	https://doi.org/10.14569/IJACSA.2017.081038	not released	4	not released
8680	Sindhi Corpus	citing_context	Sindhi Tweets Dataset	https://doi.org/10.1109/ACCESS.2025.3576853 (2025)	https://doi.org/10.1109/INMIC48123.2019.9022770 (2019)	The Sindhi Tweets Dataset is utilized for syntactic parsing and part-of-speech tagging, providing annotated data for training and evaluating NLP models. It serves as a benchmark for comparing POS tagging models in Sindhi. Additionally, the dataset is employed for sentiment analysis of Sindhi tweets using supervised machine learning techniques, enabling the classification of sentiments in social media content and the analysis of syntactic and morphological structures in annotated text.	https://doi.org/10.1109/INMIC48123.2019.9022770	not released	4	not released
8684	Sindhi Corpus	cited_context | citing_context	unlabeled Sindhi text corpus	https://doi.org/10.1109/ACCESS.2024.3507382 (2020)		The 'unlabeled Sindhi text corpus' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications involving this dataset. Its role and impact in research are not clearly defined based on the provided evidence.; The unlabeled Sindhi text corpus is used in research to develop and evaluate unsupervised models for Sindhi language processing. This dataset enables researchers to focus on the performance of unsupervised methods, providing a resource to test and refine algorithms without labeled data. The corpus supports the exploration of natural language processing techniques specific to the Sindhi language, enhancing the understanding and capabilities of computational models in this linguistic context.	https://arxiv.org/pdf/1911.12579	not released	4	not released
8685	Sindhi Corpus	citing_context	Urdu-Sindhi SER	https://doi.org/10.1109/CISCE52179.2021.9445883 (2021)	https://doi.org/10.14569/ijacsa.2020.01104104 (2020)	The Urdu-Sindhi SER dataset is used to train and evaluate models for speech emotion recognition in both Sindhi and Urdu, particularly focusing on low-resource languages. It contains 701 audio files for Sindhi and 734 for Urdu, enabling researchers to develop and test algorithms that can accurately recognize emotions in spoken language. This dataset supports the advancement of speech processing techniques in underrepresented linguistic contexts.	https://thesai.org/Publications/ViewPaper?Volume=11&Issue=4&Code=IJACSA&SerialNo=104	not released	4	not released
9629	Sudanese Arabic Corpus	citing_context	Sudanese Arabic Dataset	https://doi.org/10.1109/ICCCEEE.2018.8515862 (2018)	https://doi.org/10.1109/FiCloud.2014.100 (2014)	The Sudanese Arabic Dataset is used to train machine learning classifiers such as Naïve Bayes, KNN, and SVM for sentiment analysis. It focuses on analyzing social media comments and messages, enabling researchers to understand public sentiment in online platforms. The dataset's relevance lies in its application to Arabic dialects, specifically Sudanese Arabic, enhancing the accuracy of sentiment classification in this linguistic context.	https://ieeexplore.ieee.org/document/6984256	not released	4	not released
9660	Swahili Corpus	cited_context | citing_context	Kiswahili phoneme dictionary	https://doi.org/10.21248/jlcl.36.2023.243 (2022)		The Kiswahili phoneme dictionary is used for linguistic studies and natural language processing tasks, particularly in Swahili. It contains 31,759 word-phoneme pairs, enabling phoneme analysis and Part of Speech (POS) tagging. The dataset also supports the creation of resources for analyzing Swahili story texts, focusing on both linguistic and cultural aspects of the language. This rich resource facilitates detailed linguistic analysis and enhances understanding of Swahili language processing.; The Kiswahili phoneme dictionary is used for Swahili language processing, particularly in Part of Speech (POS) tagging, enhancing linguistic analysis and natural language processing tasks. It provides a resource of 31,759 word-phoneme pairs, facilitating research into phonetic and orthographic correspondences. Additionally, it supports the creation of Swahili story text collections, aiding research in Swahili language and literature.	https://arxiv.org/abs/2210.16537	not released	4	not released
9637	Swahili Corpus	cited_context	annotated Swahili dataset	https://doi.org/10.1145/3445975 (2021)		The annotated Swahili dataset is used for morphological annotation of Swahili text, employing a two-level morphological dictionary to tag linguistic features. This methodology supports research focused on understanding and processing the morphological structure of Swahili, enabling detailed analysis of linguistic elements within the language.	https://doi.org/10.17632/rvt89578g5.3	not released	4	not released
9662	Swahili Corpus	citing_context	log files from an online Swahili language dataset	https://doi.org/10.17576/gema-2019-1904-01 (2019)	https://www.semanticscholar.org/paper/b0685923ada11bc3d7b02d39712dfa07daa88b36 (2004)	The dataset 'log files from an online Swahili language dataset' is mentioned in research citations but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided, and there is no explicit information on how these log files enable research. The dataset's relevance and specific characteristics remain unspecified in the available literature.	https://www.semanticscholar.org/paper/On-how-electronic-dictionaries-are-really-used-Schryver-Joffe/b0685923ada11bc3d7b02d39712dfa07daa88b36	not released	4	not released
9680	Swahili Corpus	cited_context	Swahili-English Language Data	https://www.semanticscholar.org/paper/3a2cc7117987d0350627cfde5b41a586c2c68a6d (2021)	https://doi.org/10.18653/v1/W16-5803 (2016)	The Swahili-English Language Data dataset is used to estimate the proportion of English tokens within the dataset, focusing on the impact of these tokens on model performance. It is also utilized to analyze code-switching phenomena in Swahili speakers, examining how code-switching affects model performance. This dataset enables researchers to understand the linguistic dynamics and technical challenges associated with multilingual data in natural language processing models.	https://aclanthology.org/W16-5803/	not released	4	not released
9830	Sylheti Corpus	citing_context	a new speech database in Sylheti	https://doi.org/10.1109/CSCC49995.2020.00014 (2020)	https://ieeexplore.ieee.org/document/9402663 (2020)	The Sylheti speech database is used to develop and evaluate speech recognition systems for isolated words in the Sylheti language. Researchers focus on enhancing Automatic Speech Recognition (ASR) performance for under-resourced languages, leveraging the dataset's recordings to improve system accuracy and robustness.	https://ieeexplore.ieee.org/document/9402663	not released	4	not released
10901	Uyghur Corpus	cited_context	1898 sentence speech utterances		https://doi.org/10.23919/APSIPA.2018.8659652 (2018)	The '1898 sentence speech utterances' dataset is used to train a multilingual Text-to-Speech (TTS) system, specifically focusing on Mandarin and Uyghur languages. It contains 948 Mandarin and 950 Uyghur speech utterances, enabling research in cross-lingual speech synthesis. This dataset supports the development of TTS models that can effectively handle multiple languages, enhancing the accuracy and naturalness of synthesized speech across different linguistic contexts.	https://ieeexplore.ieee.org/document/8659652	not released	4	not released
11184	Wolof Corpus	citing_context	SENCORPUS: A French-Wolof Parallel Corpus	https://www.semanticscholar.org/paper/088f840081f0739443c69dca43094a42771f62de (2022)	https://aclanthology.org/2020.lrec-1.341/ (2020)	The French-Wolof dataset is used to train neural machine translation models, specifically focusing on the distribution of domain data. Preprocessing steps, such as removing diacritics from the source training data, are employed to enhance model performance. This dataset enables researchers to address challenges in translating between French and Wolof, contributing to advancements in low-resource language translation.	https://aclanthology.org/2020.lrec-1.341/	not released	4	not released
11185	Wolof Corpus	cited_context	corpus of 83,000 Wolof French parallel sentences	https://doi.org/10.48550/arXiv.2305.00606 (2023)	https://www.semanticscholar.org/paper/088f840081f0739443c69dca43094a42771f62de (2022)	The dataset of 83,000 Wolof French parallel sentences is primarily used to train neural machine translation systems for both French-Wolof and Wolof-French translations. Researchers employ the Transformer architecture and develop Word Embedding and LSTM-based models, particularly focusing on low-resource settings. This dataset enables the advancement of machine translation technologies in under-resourced languages, enhancing the accuracy and efficiency of Wolof-French translation systems.	https://aclanthology.org/2022.lrec-1.717/	not released	4	not released
11188	Wolof Corpus	cited_context | citing_context	Wolof monolingual corpus	https://www.semanticscholar.org/paper/2a2369e051d2ddd2396d8af40e743b9ff1ea83ac (2020), https://www.semanticscholar.org/paper/4cf0b3cf6b922dedcb4af292aafbc7e90721bf26 (2020)	https://doi.org/10.3115/v1/D14-1162 (2014), https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f (2013)	The Wolof monolingual corpus is used to train word embedding models such as CBOW, Skip-gram, and GloVe. Researchers assess the quality of the corpus by evaluating the effectiveness of these embedding techniques, focusing on their performance and the insights they provide into the Wolof language. This dataset enables the development and comparison of different embedding methods, contributing to the improvement of natural language processing tasks in Wolof.	https://aclanthology.org/2020.lrec-1.341/	not released	4	not released
11191	Yoruba Corpus	citing_context	2020 Yorùbá Wikipedia dump	https://www.semanticscholar.org/paper/4ca059f9aa3c16dffb2e7239205082aa47072dcf (2020)		The 2020 Yorùbá Wikipedia dump is used to study diacritic usage in Yorùbá language articles, providing a cleaned and normalized version for linguistic analysis. It serves as the largest diacritized dataset for research in Yorùbá language and computational linguistics, focusing on diacritical marks and phonological features. This dataset enables detailed linguistic studies by offering extensive, high-quality textual data.	https://aclanthology.org/2020.lrec-1.335/	not released	4	not released
11207	Yoruba Corpus	citing_context	LagosNWU Yorùbá Speech Corpus	https://doi.org/10.1109/SLT54892.2023.10023089 (2022)		The LagosNWU Yorùbá Speech Corpus is used to study Yorùbá speech, focusing on gender representation, audio quality, and high-resolution audio. It consists of 2.75 to 4 hours of recordings from 33 to 36 speakers, recorded at 16 kHz and 48 kHz. Research emphasizes diverse speaker demographics and phonetic/phonological aspects, enabling detailed analysis of Yorùbá speech patterns.	https://hdl.handle.net/20.500.12185/431	not released	4	not released
8842	Southern Sotho Corpus	citing_context	Southern Sotho vowels dataset	https://doi.org/10.2989/16073614.2014.925221 (2014)		The Southern Sotho vowels dataset is used to evaluate eight normalization procedures on vowel formants, aiming to reduce speaker-specific variability while retaining age and ethnicity information. Comprising 4,434 vowel tokens from twelve speakers balanced by gender, age, and locality, the dataset enables researchers to systematically assess the effectiveness of these procedures in phonetic analysis.	https://doi.org/10.2989/16073614.2014.925221	not released	4	the paper did not have a link to the speech dataset
8818	Somali Corpus	cited_context	IARPA MATERIAL	https://doi.org/10.1145/3626772.3657884 (2024)	https://www.semanticscholar.org/paper/de02a7e9c5e0eeb2cab4d7dd810e5e573327a2f5 (2020)	The IARPA MATERIAL dataset is used to collect documents from blogs, news, and topical texts in six less-resourced languages, facilitating cross-language information retrieval research. This dataset supports methodologies focused on retrieving and analyzing information across different languages, enhancing the ability to access and understand content in less-resourced linguistic contexts.		not released	4	there is a dataset, but the url is not provided by the paper.
8829	Southern Kurdish Corpus	citing_context	Shaqlawa	https://doi.org/10.1515/flin-2024-2049 (2024)		The Shaqlawa dataset is used to study Southern Kurdish folklore and linguistic patterns, specifically focusing on traditional stories and dialect variations. Researchers employ qualitative analysis to explore the cultural and linguistic nuances within these texts, enabling a deeper understanding of Southern Kurdish oral traditions and dialectal differences.		not released	4	there is not such a dataset
5537	Indonesian Corpus	cited_context | citing_context	900K Indonesian tweets	https://doi.org/10.18653/v1/D19-5554 (2019)	https://etd.repository.ugm.ac.id/penelitian/detail/128633 (2017)	The Adikara (2015) dataset is used to collect 900K Indonesian tweets for sentiment analysis, specifically focusing on applying sentiment classification techniques in the Indonesian language. This dataset enables researchers to develop and evaluate algorithms for classifying sentiments in social media content, enhancing the accuracy of sentiment analysis in the Indonesian context.; The Adikara (2015) dataset is used to collect 900K Indonesian tweets for sentiment analysis, specifically focusing on the Indonesian language. This dataset enables researchers to analyze public opinion and emotional trends by employing natural language processing techniques. The large volume of tweets provides a robust sample for understanding nuanced linguistic patterns and sentiment in social media contexts.		not released	4	this is a master thesis, cannot access the full paper
5659	Indonesian Corpus	citing_context	Indonesian Hoax News Detection Dataset	https://doi.org/10.1109/SIML61815.2024.10578272 (2024)	https://doi.org/10.1109/ICIMCIS56303.2022.10017528 (2022)	The Indonesian Hoax News Detection Dataset is used to train and evaluate machine learning models for detecting fake news in Indonesian-language online media. Research focuses on improving classification accuracy and model performance, utilizing the dataset's specific characteristics to enhance the reliability of fake news detection systems.	https://data.mendeley.com/	not released	4	not specfied
10205	Tigrigna Corpus	citing_context	Tigrinya Language Speech Corpus	https://doi.org/10.18653/V1/2020.WINLP-1.5 (2020)	https://www.semanticscholar.org/paper/cc60f276ac89eb91dd83fb1de8be698a8f8cee8b (2018)	The Tigrinya Language Speech Corpus is used to develop speech recognition systems for Tigrigna, specifically focusing on creating a medium-sized read speech corpus. This dataset supports linguistic and technological advancements by providing a resource for training and testing speech recognition models, enabling researchers to improve the accuracy and robustness of these systems for the Tigrigna language.	https://aclanthology.org/W18-3811.pdf	not released	4	the authors said they have the intention of making the corpus available for researchers and developers by means of a third party, but cannot found.
5631	Indonesian Corpus	citing_context	ID-WOZ	https://doi.org/10.1155/2022/8407713 (2022), https://doi.org/10.1145/3575803 (2022)		The ID-WOZ dataset is primarily used for task-oriented dialogue modeling in the Indonesian language, focusing on domains such as restaurants, hotels, taxis, attractions, police, hospitals, planes, movies, and wear. It is employed to conduct extensive experiments on dialogue system performance, intention classification, and slot-filling tasks. The dataset is often compared with MultiWOZ and other local datasets to evaluate model effectiveness in both low-resource and culturally specific contexts. It supports the adaptation and evaluation of dialogue models in the Indonesian language, enhancing their performance in multi-domain scenarios.		not released	4	The data used to support the ﬁndings of this study areavailable from the corresponding author upon request
5635	Indonesian Corpus	citing_context	IndoLem	https://doi.org/10.23887/janapati.v12i2.62582 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	The Indo4B Indonesian corpus is used for pre-training the indoBERT model, specifically to enhance performance in Indonesian NLP tasks. This involves leveraging large-scale language data to improve the model's capabilities in handling Indonesian-specific linguistic challenges. The dataset's extensive size and focus on Indonesian content enable researchers to develop more effective and contextually accurate NLP models for Indonesian applications.	https://indolem.github.io/		1	but this dataset's name is IndoLem
8828	Southern Kurdish Corpus	citing_context	Pewan corpus	https://doi.org/10.48550/arXiv.2304.01319 (2023)	https://www.semanticscholar.org/paper/0657ee32d949a1f8bb98362f4e539c8db03d52c5 (2020)	The Pewan corpus is used to analyze linguistic patterns and statistical properties, specifically focusing on the rank-size distribution for Northern and Central Kurdish. Researchers employ statistical methods to examine the frequency and distribution of linguistic elements, enabling insights into the structural characteristics of these dialects. This dataset facilitates a quantitative approach to understanding Kurdish language dynamics.	https://dl.dropbox.com/u/10883132/Pewan.zip		1	https://www.semanticscholar.org/paper/Sorani-Kurdish-versus-Kurmanji-Kurdish%3A-An-Esmaili-Salavati/2fb97d01e987a798958472d384ecbb533a321996
5648	Indonesian Corpus	cited_context | citing_context	Indonesian Common Crawl (CC-ID)	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://www.semanticscholar.org/paper/5feb32a73dd1bd9e13f84a7b3344497a5545106b (2016)	The Indonesian Common Crawl (CC-ID) dataset is primarily used to train fastText models for word embeddings, specifically focusing on the Indonesian language. These models are employed to enhance performance in Indonesian text classification tasks and to conduct large-scale self-supervised learning. The dataset facilitates comparisons between different training approaches, enabling researchers to evaluate and improve model effectiveness in handling Indonesian text data.; The Indonesian Common Crawl (CC-ID) dataset is used to train fastText models for word embeddings, specifically focusing on the Indonesian language. It serves as a benchmark to evaluate and compare model performance in Indonesian text classification tasks. The dataset provides a large-scale resource that enhances the accuracy and effectiveness of these models, facilitating advancements in Indonesian language processing.	https://data.statmt.org/cc-100/		1	part of the dataset
7	Akan Corpus	citing_context	Akan UGSpeechData	https://www.semanticscholar.org/paper/d8e6c45b1504089cdde501164d0bb557ac544dfb (2025)	https://www.sciencedirect.com/science/article/pii/S2352340925006043	The Akan UGSpeechData dataset is used to train an Akan base model for standard speech recognition, utilizing 100 hours of transcribed Akan speech comprising 18,787 samples. This dataset enables researchers to develop and refine speech recognition systems specifically tailored for the Akan language, enhancing accuracy and performance in Akan speech processing applications.	https://www.scidb.cn/en/detail?dataSetId=bbd6baee3acf43bbbc4fe25e21077c8a		1	
8	Akan Corpus	citing_context	EmoryNLP	https://doi.org/10.48550/arXiv.2502.10973 (2025)	https://www.semanticscholar.org/paper/ba4c923b43360325cba984549aa3c3224863d1f6 (2018)	The EmoryNLP dataset is used for Akan language research, specifically in dialogue modeling and emotion detection. It focuses on daily conversation scenarios to understand natural language interactions and analyzes emotional expressions in multi-party and text-based conversations. This dataset enables researchers to develop models that can accurately capture and interpret the nuances of Akan language dialogues and emotions.	https://www.emorynlp.org/		1	
12	Akan Corpus	cited_context	wordsim-353	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	https://doi.org/10.1109/ICDARW.2019.40090 (2019)	The wordsim-353 dataset is used to evaluate word similarity in African languages, specifically Yoruba and Twi, by translating and adapting the original English dataset. Researchers employ this adapted dataset to assess the performance of computational models in capturing semantic similarities between words in these languages. This enables the development and refinement of natural language processing tools tailored for African linguistic contexts.	http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/		1	
135	Assamese Corpus	cited_context	ALT	https://doi.org/10.1162/tacl_a_00452 (2021)		The ALT dataset is primarily used to collate parallel data for machine translation, focusing on Indian languages, particularly Assamese. It serves as a training set for developing and evaluating translation models, enhancing their performance for under-resourced and low-resource languages. This dataset supports the creation of more accurate and robust machine translation systems, specifically addressing the needs of Assamese and other Indian languages.	https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/		1	
145	Assamese Corpus	cited_context	Assamese WordNet	https://doi.org/10.5815/ijem.2016.03.04 (2016)	https://www.semanticscholar.org/paper/01d99bfa62e7cc66ee70115ea5d6e4fc471d5c96 (2014)	The Assamese WordNet dataset is used to enhance the quality of bilingual machine translation systems and document classification in Assamese Natural Language Processing (NLP). It provides essential lexical and semantic information, which improves the accuracy and context understanding in these NLP tasks. This dataset enables researchers to develop more effective and culturally relevant NLP applications for the Assamese language.	https://www.tezu.ernet.in/~nlp/paper/nwdaa_11.pdf		1	
170	Assamese Corpus	cited_context	Xlit-IITB-Par	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)	https://www.semanticscholar.org/paper/a4a7467a02f9eef2e26dfec7efa01da936e8aa54 (2014)	The Xlit-IITB-Par dataset is primarily used for transliteration research, focusing on improving the accuracy and consistency of transliteration models for Indian languages. It is applied to evaluate machine translation systems, compare parallel corpora, and enhance transliteration models by providing a diverse set of examples. The dataset is also used to study the readability and accuracy of transliterated content, particularly in children's stories.	https://www.cfilt.iitb.ac.in/iitb_parallel/		1	
158	Assamese Corpus	cited_context	Leipzig Corpora Collection (WikiDump)	https://doi.org/10.1109/I3CS58314.2023.10127567 (2023)	https://doi.org/10.1007/978-3-642-20128-8_8 (2013)	The Leipzig Corpora Collection (WikiDump) is used to provide a multi-lingual dataset consisting of Wikipedia content, enabling researchers to compare and analyze Assamese language data. This dataset facilitates linguistic analysis by offering a rich, structured resource that supports cross-linguistic studies and comparisons, enhancing understanding of the Assamese language within a broader multilingual context.	https://wortschatz.uni-leipzig.de/en/download/Assamese		1	
160	Assamese Corpus	cited_context	Mann Ki Baat test set	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/d0986c1792bb6565804985e3ecfd2cae4509a5c3 (2020)	The Mann Ki Baat test set is used to evaluate the translation quality and consistency of models translating the Indian Prime Minister’s speeches into 8 Indic languages, including Assamese. This dataset facilitates the assessment of translation accuracy and coherence, enabling researchers to compare model performance across different languages.	https://quiz.mygov.in/quiz/mann-ki-baat-100-quiz/		1	
163	Assamese Corpus	cited_context	OPUS3	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/f4af3fe736b616452424d50cbd47d52f0a210582 (2020)	The OPUS3 dataset is used to collect sentence-aligned corpora for linguistic research. It has been utilized to gather data specifically for the Assamese language, enhancing linguistic studies by providing aligned sentences. The dataset, collected on 21 March 2021, supports research into language structure and usage, facilitating detailed analysis and comparison.	https://opus.nlpl.eu/Samanantar/corpus/version/Samanantar		1	
154	Assamese Corpus	cited_context	IndicCorp	https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)		The IndicCorp dataset is primarily used for training and evaluating models on Assamese language tasks, focusing on transliteration accuracy, consistency, and readability. It provides a large corpus of Indic language data, including parallel and monolingual texts, which supports the development of transliteration systems and enhances performance in language processing tasks. The dataset is also utilized for mining word pairs and creating lexical resources, contributing to multilingual text processing and machine translation, especially for low-resource languages like Assamese.	https://opendatalab.com/OpenDataLab/IndicCorp		1	
134	Assamese Corpus	citing_context	Aksharantar	https://doi.org/10.1145/3639565 (2024)	https://doi.org/10.48550/arXiv.2205.03018 (2022)	The Aksharantar dataset is used to train and evaluate a multilingual transliteration model for 21 Indian languages, including Assamese. It focuses on achieving state-of-the-art results using a transformer-based model. The dataset's multilingual nature and comprehensive coverage of Indian languages enable researchers to develop and test advanced transliteration systems.	https://huggingface.co/datasets/ai4bharat/Aksharantar		1	
155	Assamese Corpus	citing_context	IndicTTS database	https://doi.org/10.48550/arXiv.2407.14056 (2024)	https://doi.org/10.18653/v1/2024.acl-long.843 (2024)	The IndicTTS database is used to select neutral-like utterances across a diverse range of domains, contributing to the creation of a comprehensive dataset for Indian languages. This methodology ensures a broad representation, enabling research in speech synthesis and natural language processing for multiple Indian languages.	https://huggingface.co/collections/SPRINGLab/indictts-datasets-6757d20ec68d32dbcd3e3951		1	
152	Assamese Corpus	citing_context	HASOC 2023 test set	https://doi.org/10.48550/arXiv.2310.02249 (2023)	https://doi.org/10.48550/arXiv.2211.11418 (2022)	The HASOC 2023 test set is used to evaluate specialized models like BengaliSBERT, GujaratiSBERT, and assamese-bert, focusing on state-of-the-art performance in hate speech detection and offensive content classification. This dataset enables researchers to assess the effectiveness of these models in identifying and classifying harmful content, contributing to the development of more robust and culturally sensitive natural language processing systems.	https://hasocfire.github.io/hasoc/2023/task1.html		1	
148	Assamese Corpus	cited_context	Dakshina dataset	https://doi.org/10.48550/arXiv.2205.03018 (2022), https://doi.org/10.18653/v1/2023.findings-emnlp.4 (2022)	https://doi.org/10.3115/v1/N15-3017 (2015)	The Dakshina dataset is primarily used for training and evaluating machine translation, transliteration, and script conversion systems for Assamese and other Indic languages. It supports research on linguistic features, script characteristics, and information retrieval tasks. The dataset facilitates the development and evaluation of language processing tools, focusing on low-resource languages and cross-script conversion accuracy. It is also utilized for analyzing morphological and phonological features, enhancing model performance, and compiling datasets for processing South Asian languages written in the Latin script.	https://github.com/google-research-datasets/dakshina		1	
159	Assamese Corpus	citing_context	linked Indian Wordnets	https://www.semanticscholar.org/paper/f126edb612d87fe5b68b5170d242a2f2f31a2e0f (2020)	https://doi.org/10.1145/3297001.3297045 (2019)	The linked Indian Wordnets dataset is used to generate true cognate data, which is essential for creating a cognate dataset. This dataset specifically aids in improving the accuracy of phylogenetic trees for Indian languages, including Assamese. The methodology involves leveraging the structured lexical relationships within the Wordnets to identify and validate cognates, enhancing the precision of linguistic evolutionary studies.	https://github.com/cfiltnlp/IWN-En		1	
171	Assamese Corpus	citing_context	XORQA	https://doi.org/10.1109/RAIT65068.2025.11089369 (2025)	https://doi.org/10.48550/arXiv.2407.13522 (2024)	The XORQA dataset is used to develop and evaluate context-grounded question-answering (QA) systems for 11 Indic languages, including Assamese. It focuses on assessing the performance and capabilities of QA models in understanding and generating answers within specific linguistic contexts, thereby enabling researchers to improve multilingual QA systems for Indian languages.	https://github.com/AkariAsai/XORQA		1	
147	Assamese Corpus	citing_context	CommonVoice corpora	https://doi.org/10.1109/APSIPAASC63619.2025.10848811 (2024)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The CommonVoice corpora are used to train and evaluate speech recognition models, particularly for Assamese and other Indian languages. Researchers focus on improving the performance and accuracy of these models in recognizing speech in less-resourced languages. The dataset's diverse and multilingual nature enables robust testing and validation, enhancing the generalizability of the models.	https://commonvoice.mozilla.org/en/datasets		1	
161	Assamese Corpus	citing_context	Mozilla Common Voice	https://doi.org/10.1109/ACCESS.2023.3316142 (2023)	https://doi.org/10.1109/cvpr.2016.90 (2015)	The Mozilla Common Voice dataset is used to evaluate the performance of pre-trained models combined with machine learning classifiers, specifically focusing on voice recognition accuracy. This dataset enables researchers to assess and improve the effectiveness of voice recognition systems by providing a diverse set of audio samples.	https://commonvoice.mozilla.org/en/datasets		1	
162	Assamese Corpus	citing_context	online handwritten Assamese characters dataset	https://doi.org/10.3745/JIPS.02.0008 (2015)		The online handwritten Assamese characters dataset is used to collect and analyze samples of 121 conjunct consonants, 10 numeric characters, and 52 basic alphabetic characters. This dataset focuses on character recognition and analysis, employing methodologies that involve collecting online handwritten samples. It enables research into the accurate identification and classification of Assamese characters, supporting advancements in handwriting recognition technology.	https://archive.ics.uci.edu/dataset/208/online+handwritten+assamese+characters+dataset		1	
181	Bavarian Corpus	cited_context	DCK	https://www.semanticscholar.org/paper/56051929213235139795ed282f1ad0b30c549ac8 (2016)	https://doi.org/10.1515/ZGL-2013-0009 (2013)	The dataset 'DCK' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	https://www.uni-due.de/germanistik/beisswenger/aktivitaeten.php		1	
183	Bavarian Corpus	citing_context	German NoSta-D	https://doi.org/10.48550/arXiv.2403.12749 (2024)		The German NoSta-D dataset is used for Named Entity Recognition (NER) annotations in non-standard German genres. It includes historical, chat, spoken, learner, and literary prose data. Researchers employ this dataset to enhance NER models, addressing the challenges of varied and non-standard language forms. This enables more accurate entity recognition in diverse textual contexts, improving the robustness of natural language processing systems.	https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/forschung/nosta-d		1	
184	Bavarian Corpus	cited_context | citing_context	GermEval 2014	https://doi.org/10.48550/arXiv.2403.12749 (2024)	https://www.semanticscholar.org/paper/0a1cf8dbb859c13cbdb40788d7e69060155f9d77 (2014), https://www.semanticscholar.org/paper/9b644bf5262e0d02d7ac25dab509d07d240b263a (2014)	The GermEval 2014 dataset is used to guide Named Entity Recognition (NER) annotations in Bavarian, aligning with German annotation guidelines and dataset structure. This facilitates consistent and standardized NER tasks, enabling researchers to develop and evaluate NER models specifically for the Bavarian language. The dataset's structured format supports the creation of annotated corpora, enhancing the accuracy and reliability of NER systems in Bavarian.; The GermEval 2014 dataset is used to study the Bavarian language by applying fine-grained annotations to derived and partially-contained named entities. Researchers adapt this dataset to enhance the precision of named entity recognition, focusing on linguistic nuances specific to Bavarian. This approach aids in developing more accurate natural language processing models for under-resourced languages.	https://sites.google.com/site/germeval2014ner/		1	
186	Bavarian Corpus	citing_context	Kontatti corpus	https://doi.org/10.48550/arXiv.2403.10293 (2024)	https://doi.org/10.22210/suvlin.2019.088.07 (2020)	The Kontatti corpus is used to analyze Bavarian speech data, focusing on multidialectal variations within the Bavarian language. It contributes to linguistic studies by eliciting comparable spoken data in minor languages, enabling researchers to make detailed observations about linguistic features and variations.	https://kontatti.projects.unibz.it/category/corpus/		1	
193	Bavarian Corpus	cited_context | citing_context	XSID	https://doi.org/10.48550/arXiv.2501.03863 (2025), https://doi.org/10.48550/arXiv.2403.10293 (2024), https://doi.org/10.18653/V1/2021.NAACL-MAIN.197 (2021), https://www.semanticscholar.org/paper/733f61bfa5bffa1cebeceb0d22dfc654b3a7b911 (2024)	https://www.semanticscholar.org/paper/d2dac0f91d12e2aa0c2471052fb8887cd81aa2ad (2024), https://doi.org/10.1109/ICASSP.2013.6639292 (2013)	The xSID dataset is primarily used to test and enhance prediction accuracy in Bavarian dialects, particularly for slot and intent detection in digital assistant queries. It serves as a benchmark for comparing performance across different Bavarian dialects, such as South Tyrolean and Upper Bavarian. The dataset has been extended with additional dialectal data from various regions, including Upper Bavaria, Bernese Swiss German, Neapolitan, and Norwegian dialects, to support multilingual spoken language understanding tasks. This extension aids in evaluating the generalizability of models and studying the variability and standardization of Upper German dialects.; The XSID dataset is used to train and evaluate cross-linguistic models, particularly for Bavarian language tasks, including intent classification and slot filling. It focuses on the Austro-Bavarian German dialect, with 800 sentences per language, totaling 10,000 sentences. This dataset supports research in natural language processing, enabling the analysis and improvement of model performance on low-resource dialects like South Tyrolean (de-st).	https://huggingface.co/datasets/SEACrowd/xsid		1	
172	Bavarian Corpus	cited_context	Bavarian	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://doi.org/10.18653/v1/P17-1178 (2017)	The Bavarian dataset is used to create automatically aligned German–Alemannic/Bavarian bitexts, focusing on cross-lingual annotation and alignment for low-resource languages. This methodology involves aligning sentences between German and Bavarian/Alemannic dialects, enabling research in linguistic resource development and improving machine translation capabilities for under-resourced languages.	https://huggingface.co/datasets/SEACrowd/wikiann		1	
175	Bavarian Corpus	cited_context	Bavarian Treebank	https://doi.org/10.48550/arXiv.2304.09805 (2023)	https://www.semanticscholar.org/paper/70026982e00bc01bec76148bfb7f14c982c4ab85 (2021)	The Bavarian Treebank is used to create a syntactically annotated corpus of Bavarian, which supports linguistic analysis and natural language processing tasks. It is employed to study the Bavarian language, focusing on linguistic features and dialectal variations. The dataset's syntactic annotations enable detailed examination of grammatical structures, enhancing understanding of the language's unique characteristics.	https://github.com/UniversalDependencies/UD_Bavarian-MaiBaam		1	
180	Bavarian Corpus	citing_context	DBÖ	https://www.semanticscholar.org/paper/a6653831335ca1101160c8959e1c622302c6a81f (2020)		The DBÖ dataset is used as a knowledge base for a comprehensive, joint approach combining dictionary and atlas data, specifically for the Bavarian language. It is utilized to curate, enrich, interlink, and publish Bavarian linguistic data in the LLOD platform, enhancing data management and interoperability. This enables detailed linguistic research and resource integration.	https://euralex.org/publications/database-of-bavarian-dialects-dbo-electronically-mapped-dboema-a-system-for-archiving-maintaining-and-field-mapping-of-heterogeneous-dialect-data-for-the-compilation-of-dialect-lexicons/#:~:text=Dialect%20Lexicons%20%E2%80%93%20Euralex-,Database%20of%20Bavarian%20Dialects%20(DB%C3%96)%20Electronically%20Mapped%20(dbo@,the%20Compilation%20of%20Dialect%20Lexicons&text=dbo@ema%20is%20a%20system,to%20the%20project%20dbo@ema.		1	Database of Bavarian Dialects (DBÖ) Electronically Mapped (dbo@ema)
718	Bhojpuri Corpus	citing_context	IndicVoices	https://www.semanticscholar.org/paper/3a67a93afc0b0c3caf580ebb73d66d9a1b575c21 (2025)	https://doi.org/10.1109/ICASSP49357.2023.10096933 (2022)	The IndicVoices dataset is used to improve Automatic Speech Recognition (ASR) systems for Bhojpuri, a low-resource Indian language. It provides a large-scale, curated collection of audio and text pairs, which are leveraged to enhance ASR performance through web-mined data. This dataset supports ASR development by offering a diverse and rich set of audio and text pairs, thereby improving ASR accuracy and system robustness.	https://github.com/AI4Bharat/IndicVoices		1	
a	Bhojpuri Corpus	citing_context	Vaani	https://www.semanticscholar.org/paper/3a67a93afc0b0c3caf580ebb73d66d9a1b575c21 (2025)			https://vaani.iisc.ac.in/		1	
715	Bhojpuri Corpus	citing_context	Bhojpuri	https://www.semanticscholar.org/paper/71a88ba9a07a7a290755430c2c6282008b85db12 (2019)	https://doi.org/10.1162/tacl_a_00065 (2016)	The Bhojpuri dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, it cannot be accurately described how this dataset is used in research based on the provided evidence.	https://huggingface.co/datasets/ankur02/bhojpuri		1	
716	Bhojpuri Corpus	citing_context	Bhojpuri UD	https://doi.org/10.18653/v1/2024.findings-acl.857 (2024)	https://www.semanticscholar.org/paper/cc58ebb219feb0f6da5c0f52bed1721dcc4068a1 (2020)	The Bhojpuri UD dataset is used to evaluate the performance of part-of-speech (POS) tagging in Bhojpuri, specifically focusing on the accuracy of universal dependency annotations. Researchers employ this dataset to assess and improve the precision of linguistic tagging tools, contributing to the development of more reliable natural language processing systems for the Bhojpuri language.	https://github.com/UniversalDependencies/UD_Bhojpuri-BHTB		1	The Bhojpuri Universal Dependency Treebanks (BHTB)
722	Bhojpuri Corpus	citing_context	ULCA NewsOnAir	https://www.semanticscholar.org/paper/3a67a93afc0b0c3caf580ebb73d66d9a1b575c21 (2025)		The ULCA NewsOnAir dataset is used to train monolingual Bhojpuri models, focusing on enhancing speech recognition and language modeling. It provides 133.4 hours of news broadcast data, which improves the model's understanding of spoken language and robustness in diverse speech contexts, particularly in conversational speech. This dataset contributes to advancing the performance of Bhojpuri speech recognition systems by offering rich, varied speech data.	https://github.com/Open-Speech-EkStep/ULCA-asr-dataset-corpus		1	ULCA ASR Dataset Corpus
a	Bhojpuri Corpus	citing_context	SpeeS-IA	https://www.semanticscholar.org/paper/3a67a93afc0b0c3caf580ebb73d66d9a1b575c21 (2025)	https://doi.org/10.48550/arXiv.2206.12931 (2022)		https://github.com/kmi-linguistics/SpeeD-IA		1	
761	Burmese Corpus	citing_context	Myanmar Language Commission (MLC)	https://www.semanticscholar.org/paper/fdb63df962f8411726d277480699564321a2648d (2020)		The Myanmar Language Commission (MLC) dataset is used to obtain standard graphemes for Burmese language processing. Researchers focus on linguistic data for script analysis and normalization, employing the dataset to ensure consistency and accuracy in representing Burmese script. This enables precise linguistic analysis and supports the development of computational tools for Burmese language processing.	https://en.wikipedia.org/wiki/Myanmar%E2%80%93English_Dictionary		1	
759	Burmese Corpus	citing_context	high-quality in-house Burmese speech dataset	https://www.semanticscholar.org/paper/fdb63df962f8411726d277480699564321a2648d (2020)	https://doi.org/10.21437/Interspeech.2015-132 (2015)	The high-quality in-house Burmese speech dataset is used to develop the first statistical parametric Burmese text-to-speech (TTS) system based on Hidden Markov Models (HMMs). This dataset focuses on enhancing the quality of speech synthesis, enabling researchers to improve the naturalness and intelligibility of synthesized Burmese speech. The dataset's high quality and in-house nature provide essential acoustic data for training and refining HMM-based TTS models.	https://www.openslr.org/80/		1	
766	Burmese Corpus	citing_context	Wikipedia Burmese data	https://doi.org/10.1109/ICCI64209.2025.10987237 (2025)	https://doi.org/10.1162/tacl_a_00051 (2016)	The Wikipedia Burmese data dataset is used to train fastText embeddings, specifically focusing on subword information to enhance word vectors in the Burmese language. This methodology leverages the dataset's rich textual content to improve the representation of Burmese words, enabling more nuanced and contextually accurate natural language processing tasks.	https://www.kaggle.com/datasets/myominhtet/burmese-wikipedia-articles144k		1	
763	Burmese Corpus	cited_context	myPOS	https://doi.org/10.1016/j.heliyon.2022.e10375 (2022)		The 'myPOS' dataset is used to train and evaluate a Myanmar POS tagger, specifically extending the existing corpus with a standardized set of 15 POS tags. This dataset enables researchers to develop and refine POS tagging models for the Burmese language, enhancing the accuracy and robustness of natural language processing tools in this linguistic domain.	https://github.com/ye-kyaw-thu/myPOS		1	
762	Burmese Corpus	citing_context	myNER corpus	https://doi.org/10.1109/ICCI64209.2025.10987237 (2025)	https://doi.org/10.1016/j.heliyon.2022.e10375 (2022)	The myNER corpus is used for named entity recognition (NER) experiments, focusing on evaluating the performance of NER models on Burmese language data. This dataset enables researchers to test and refine NER algorithms, addressing specific challenges in identifying and classifying named entities within the Burmese language.	https://github.com/ye-kyaw-thu/myNER		1	
770	Cameroon Pidgin Corpus	citing_context	Spoken Corpus of Cameroon Pidgin English	https://doi.org/10.1163/19552629-01201003 (2019)		The 'Spoken Corpus of Cameroon Pidgin English' is used to analyze the grammatical structure and linguistic features of spoken Cameroon Pidgin English. Researchers focus on syntactic patterns, morphological features, and usage patterns in natural speech. This dataset enables detailed linguistic analysis by providing authentic spoken data, facilitating a deeper understanding of the language's structure and usage.	https://sussex.figshare.com/articles/online_resource/A_spoken_corpus_of_Cameroon_Pidgin_English_pilot_study/23436653?file=41148716		1	
818	Cebuano Corpus	citing_context	Bible in Cebuano	https://doi.org/10.1145/3342827.3342833 (2019)	https://doi.org/10.1007/s10579-014-9287-y (2014)	The 'Bible in Cebuano' dataset is used to study linguistic aspects of the Cebuano language, particularly through the translation and textual analysis of biblical passages. Researchers employ this dataset to examine the nuances and complexities of Cebuano translations, focusing on how religious texts are rendered in the language. This analysis helps in understanding the linguistic and cultural adaptations in Cebuano biblical translations.	https://github.com/Jaden-J/OfflineBible-Data		1	
829	Central Kurdish Corpus	citing_context	dataset containing 14,881 comments from various Facebook pages	https://doi.org/10.48550/arXiv.2304.04703 (2023)	https://doi.org/10.37652/juaps.2022.176501 (2022)	The dataset containing 14,881 comments from various Facebook pages is used for sentiment analysis of Kurdish comments. Researchers focus on creating and evaluating this dataset to understand public opinion and emotional responses. The methodology involves analyzing the textual content to gauge sentiments, providing insights into how the public reacts to various topics on social media.	https://www.philstat.org/index.php/MSEA/article/view/890		1	
842	Central Kurdish Corpus	cited_context | citing_context	Pewan text corpus	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021), https://doi.org/10.1007/s10579-022-09594-4 (2022)	https://www.semanticscholar.org/paper/2fb97d01e987a798958472d384ecbb533a321996 (2013)	The Pewan text corpus is used to analyze and compare Central Kurdish and Northern Kurdish texts, focusing on linguistic differences and similarities. Collected from online news agencies, the dataset enables researchers to introduce and examine these dialects using textual analysis methods, providing insights into their structural and lexical variations.; The Pewan text corpus is used to introduce and analyze Central Kurdish and Northern Kurdish languages, focusing on linguistic features and linguistic comparisons. Researchers employ the dataset to explore specific linguistic characteristics, enabling detailed analyses and comparisons between these Kurdish dialects. This corpus facilitates a deeper understanding of the structural and functional aspects of these languages.	https://sinaahmadi.github.io/resources/pewan.html		1	
839	Central Kurdish Corpus	citing_context	Kurdish Textbook Corpus (KTC)	https://doi.org/10.48550/arXiv.2305.06747 (2023)	https://www.semanticscholar.org/paper/6ad3ffae7d0f087f04b20434fbd4d91e5ea5b7b7 (2019)	The Kurdish Textbook Corpus (KTC) is used to develop parallel corpora for the Central Kurdish language, specifically focusing on K-12 school textbooks from the Kurdistan Region of Iraq. This dataset enables researchers to create linguistic resources for less-resourced languages, enhancing the availability of educational materials and supporting language documentation and preservation efforts.	https://sinaahmadi.github.io/resources/ktc.html		1	
844	Central Kurdish Corpus	cited_context | citing_context	small corpus of size 590K tokens	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	https://www.semanticscholar.org/paper/b103e1ec19f19780c2b172be85b2f0863fa20c8b (2010)	The small corpus of 590K tokens is used to develop a morphological lexicon for Central Kurdish, focusing on linguistic features. It is employed in training a semi-supervised method, enabling researchers to enhance the accuracy and coverage of morphological analysis in Central Kurdish. This dataset's size and focus on linguistic features make it suitable for building robust morphological models.; The dataset 'small corpus of size 590K tokens' is mentioned in the citation context but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.	https://shs.hal.science/halshs-00751634v1/document		1	
838	Central Kurdish Corpus	citing_context	Kurdish (Sorani) Named Entities	https://doi.org/10.1016/j.dib.2025.111839 (2025)	https://doi.org/10.48550/arXiv.2301.04962 (2023)	The Kurdish (Sorani) Named Entities dataset is used to enhance the Kurdish-BLARK Named Entities dataset, specifically focusing on improving named entity recognition in the Sorani Kurdish dialect. This involves refining the identification and classification of named entities, which aids in developing more accurate natural language processing tools for the Sorani Kurdish language.	https://kurdishblark.github.io/		1	
827	Central Kurdish Corpus	cited_context	Central Kurdish-English parallel corpus	https://www.semanticscholar.org/paper/9ca17bf6931ddae814ad440557377107bc334195 (2021)	https://www.semanticscholar.org/paper/602db1616942b9d4d408b4934c766da8e67854f4 (2021)	The Central Kurdish-English parallel corpus is used to study translation quality and linguistic features in Central Kurdish, particularly in domains like TED Talks, digital books, and academic content. It is also utilized to build a speech recognition system, focusing on pronunciation lexicon and speech data. Additionally, the dataset is employed to correct common text errors using before-and-after pairs and to analyze and process textual data for linguistic research.	https://kailab.org/datasets/dataset-3/		1	
826	Central Kurdish Corpus	citing_context	BD-4SK-ASR	https://doi.org/10.21928/uhdjst.v6n2y2022.pp117-125 (2022)	https://www.semanticscholar.org/paper/72dd600355084dc51d1ebc6d0b5cac26ec9e09ec (2019)	The BD-4SK-ASR dataset is used to develop and evaluate speech recognition systems for Sorani Kurdish, focusing on the linguistic and phonetic features of the Central Kurdish language. It includes primary school texts from Iraq’s Kurdistan Region, containing 200 sentences, which are utilized to enhance the accuracy and robustness of speech recognition models.	https://github.com/KurdishBLARK/BD-4SK-ASR		1	
823	Central Kurdish Corpus	cited_context | citing_context	AsoSoft corpus	https://doi.org/10.1007/s42803-022-00062-7 (2021)	https://doi.org/10.1093/LLC/FQY074 (2019)	The AsoSoft corpus is used for Central Kurdish language research, particularly in evaluating linguistic rules and processing. It contains 188M words from news, books, and magazines, enabling the assessment of rule consistency, word selection, and context extraction. The dataset supports experiments in language processing, morphological analysis, and the identification of standard and nonstandard forms through tagging and frequency analysis.; The AsoSoft corpus is used in research to analyze and process the Central Kurdish language, focusing on identifying standard and nonstandard forms, and evaluating language models. It contains 188M tokens from various sources, enabling training and evaluation of NLP tasks like tokenization, part-of-speech tagging, and parsing. The corpus supports linguistic experiments, morphological analysis, and the development of text corpora, addressing specific challenges in Central Kurdish language processing.	https://asosoft.github.io/resources/TextCorpus.html		1	
851	Central Pashto Corpus	cited_context	POLD		https://doi.org/10.7717/peerj-cs.1617 (2023)	The POLD dataset is used to fine-tune XLM-RoBERTa for transfer learning, specifically focusing on multilingual representation learning. It evaluates the effectiveness of pre-trained models in this context, enhancing the model's performance across languages. This dataset enables researchers to assess and improve the cross-lingual capabilities of deep learning models.	https://peerj.com/articles/cs-1617/#supp-2		1	
850	Central Pashto Corpus	citing_context	Pashto characters database	https://doi.org/10.1155/2021/3543816 (2021)	https://doi.org/10.22581/MUET1982.2101.14 (2021)	The Pashto characters database is used for simulation and experimental work focused on handwritten Pashto character recognition. Researchers apply Zernike Moments and Linear Discriminant Analysis to this dataset to develop and test algorithms for recognizing handwritten Pashto characters. This enables advancements in optical character recognition technology specific to the Pashto script.	https://www.techrxiv.org/doi/full/10.36227/techrxiv.13713337.v1		1	
847	Central Pashto Corpus	citing_context	Central Pashto Corpus	https://doi.org/10.1155/2021/3543816 (2021)	https://doi.org/10.1117/12.2003731 (2013)	The Central Pashto Corpus is used to develop and evaluate natural language processing (NLP) models for the Central Pashto language. Specifically, it focuses on text normalization and tokenization tasks. This dataset enables researchers to improve the accuracy and efficiency of NLP models tailored for Central Pashto, addressing challenges unique to the language's structure and usage.	https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8658/1/A-segmentation-free-approach-to-Arabic-and-Urdu-OCR/10.1117/12.2003731.short		1	
848	Central Pashto Corpus	citing_context	Gold-Standard Pashto Dataset	https://doi.org/10.1109/ACCESS.2022.3216881 (2022)	https://doi.org/10.6017/ITAL.V40I1.12553 (2021)	The Gold-Standard Pashto Dataset is used to develop and evaluate segmentation algorithms for Central Pashto text. It consists of 300 text-line images from three Pashto books, enabling researchers to test and refine algorithms specifically tailored for the segmentation of Pashto text. This dataset facilitates the advancement of text processing techniques for the Central Pashto language.	https://ital.corejournals.org/index.php/ital/article/view/12553		1	
853	Chichewa Corpus	citing_context	CMU Wilderness dataset	https://doi.org/10.48550/arXiv.2207.03546 (2022)	https://doi.org/10.1109/ICASSP.2019.8683536 (2019)	The CMU Wilderness dataset provides high-quality, single-speaker speech data for several African languages, including Chichewa. Despite containing noisy alignments and not being publicly available, it is utilized in research to enhance speech recognition and processing systems for under-resourced languages. The dataset's specific characteristics, such as its single-speaker nature, support the development and evaluation of speech models tailored to African languages.	http://festvox.org/cmu_wilderness/		1	
1975	Dholuo Corpus	citing_context	Autshumato	https://doi.org/10.21248/jlcl.36.2023.243 (2022)	https://www.semanticscholar.org/paper/feed9002765ede951407801628ab5e4cdc96b60b (2009)	The Autshumato dataset is used for machine translation research, specifically to improve the translational accuracy between official South African languages. It focuses on evaluating language pairs, which may include Dholuo, to enhance the quality and reliability of translations. This dataset enables researchers to test and refine machine translation models, addressing the specific challenges of translating between less commonly studied languages.	https://repo.sadilar.org/items/c6b36522-9531-4ec0-9565-17c7524404a0		1	
2135	Eastern Oromo Corpus	citing_context	K-MHaS	https://doi.org/10.1109/CCET56606.2022.10080837 (2022)	https://doi.org/10.48550/arXiv.2208.10684 (2022)	The K-MHaS dataset is used for multi-label hate speech recognition in the Eastern Oromo language, specifically focusing on intersectionality and subjectivity in news comments. Researchers employ this dataset to analyze and classify hate speech, considering the nuanced contexts and overlapping identities present in the data. This enables a deeper understanding of how hate speech manifests in online discourse within the Eastern Oromo community.	https://github.com/adlnlp/K-MHaS		1	
2138	Eastern Punjabi Corpus	citing_context	benchmark dataset of Gurumukhi offline handwritten Tehsil and Sub-tehsil names from Punjab	https://doi.org/10.1145/3593024 (2023)		The 'benchmark dataset of Gurumukhi offline handwritten Tehsil and Sub-tehsil names from Punjab' is mentioned in research citations but lacks detailed descriptions of its usage. No specific methodologies, research questions, or applications are provided, and the dataset's characteristics and research-enabling capabilities are not explicitly outlined in the available information.	https://sites.google.com/view/gurmukhi-benchmark/		1	
2139	Eastern Punjabi Corpus	citing_context	FakeNewsIndia	https://doi.org/10.48550/arXiv.2410.10407 (2024)	https://doi.org/10.1016/j.comcom.2022.01.003 (2022)	The FakeNewsIndia dataset is used to study fake news incidents in India, with a focus on collection methodology and impact assessment in social media. It is particularly relevant for understanding misinformation in the Eastern Punjabi language. Researchers use this dataset to analyze the spread and effects of fake news, employing methods that assess its impact on social media platforms.	https://github.com/rishabhkaushal/fakenewsincidentsIndia		1	
4409	Gujarati Corpus	citing_context	EMILLE corpus	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)	https://www.semanticscholar.org/paper/6569c9b81b7721fff588ee58c7607367045e9af0 (2003)	The EMILLE corpus is used to construct a corpus for South Asian language processing, focusing on minority languages including Gujarati. It facilitates collaborative research between Lancaster University and CIIL, Mysore, enabling the development of linguistic resources and tools for under-represented languages. This dataset supports the creation of annotated corpora, enhancing computational linguistics and natural language processing capabilities for these languages.	https://www.lancaster.ac.uk/fass/projects/corpus/emille/		1	
4411	Gujarati Corpus	citing_context	Free Spoken Gujarati Digit Dataset (FSGDD(12))	https://doi.org/10.1145/3597494 (2023)	https://doi.org/10.1007/978-981-15-4828-4_18 (2019)	The Free Spoken Gujarati Digit Dataset (FSGDD(12)) is primarily used to train and evaluate models for spoken digit recognition in Gujarati. Research focuses on classifying Gujarati digits using various machine learning models, including bi-directional LSTM, Naïve Bayes, and CNN. The dataset's regional diversity and speaker variability are key characteristics that enhance its utility for developing and testing spoken Gujarati language processing systems.	https://github.com/Jakobovski/free-spoken-digit-dataset		1	
4412	Gujarati Corpus	citing_context	Gujarati Corpus		https://www.semanticscholar.org/paper/6ab992162e4ba22c99c4e21b5d9854353462aef5 (2018)	The Gujarati Corpus is used to analyze open-ended conversations in Gujarati, focusing on lexical and discourse patterns in online chat dialog. It serves as a resource for understanding conversational dynamics and linguistic features in digital communication, enabling researchers to explore how language is used in informal, interactive settings.	https://github.com/sumanbanerjee1/Code-Mixed-Dialog		1	
4413	Gujarati Corpus	citing_context	Gujarati (Google, 2019a)	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)		The Gujarati (Google, 2019a) dataset is used to provide a structured resource for Gujarati language research, facilitating the development of models for various Gujarati language processing tasks. This dataset enables researchers to build and test models that can handle Gujarati-specific linguistic challenges, enhancing the accuracy and effectiveness of language processing technologies for this language.	http://www.openslr.org/78		1	
4418	Gujarati Corpus	citing_context	IIIT-INDIC-HW	https://doi.org/10.1109/SPIN57001.2023.10117106 (2023)	https://doi.org/10.1007/978-3-030-86337-1_30 (2021)	The IIIT-INDIC-HW dataset is used for recognizing handwritten text in Gujarati and other Indic scripts. It provides a large annotated dataset for training and evaluating CNN and transformer-based models, specifically focusing on handwritten text recognition for regional languages. This dataset enables researchers to develop and test algorithms that improve the accuracy of recognizing diverse Indic scripts.	https://www.kaggle.com/datasets/himanshukumarrajak/indic-data		1	
4423	Gujarati Corpus	citing_context	Linguistic Data Consortium for Indian Languages (LDC-IL)	https://doi.org/10.1007/s11277-022-09549-6 (2022)	https://doi.org/10.1007/s10579-020-09523-3 (2021)	The Linguistic Data Consortium for Indian Languages (LDC-IL) serves as a repository of linguistic resources for various Indian languages, including Gujarati. It supports research and development in language technology, particularly for under-resourced languages. The dataset provides linguistic data essential for natural language processing tasks such as machine translation and speech recognition, enabling researchers to develop and improve algorithms and models for these languages.	https://www.ldcil.org/publications.aspx		1	
4433	Gujarati Corpus	cited_context	Samanantar		https://doi.org/10.1162/tacl_a_00452 (2021)	The Tanzil dataset is primarily used to train neural machine translation models for Gujarati, focusing on a variety of multilingual text types including TED talk transcripts, Wikipedia articles, legal and administrative texts, Quranic and religious texts, blog posts, articles, movie and TV subtitles, and sentence-level parallel data. This dataset enables researchers to develop and refine translation models across diverse domains, enhancing the accuracy and contextual relevance of Gujarati translations.	https://huggingface.co/datasets/ai4bharat/samanantar		1	
4436	Gujarati Corpus	citing_context	VoxForge Indian	https://doi.org/10.1051/itmconf/20235401016 (2023)	https://doi.org/10.1145/3483446 (2021)	The VoxForge Indian dataset is used to evaluate the performance of a unidirectional RNN-CTC model for Gujarati speech recognition. Researchers focus on assessing the accuracy and robustness of the model, utilizing the dataset's speech samples to train and test the system. This enables the development and refinement of speech recognition technologies specifically tailored for the Gujarati language.	https://www.voxforge.org/home/audacity/indian-voice		1	
4464	Hausa Corpus	cited_context | citing_context	AfriSenti corpora	https://doi.org/10.1109/ICMI60790.2024.10585867 (2024), https://doi.org/10.48550/arXiv.2304.06845 (2023), https://doi.org/10.48550/arXiv.2302.08956 (2023)	https://doi.org/10.48550/arXiv.2302.08956 (2023), https://doi.org/10.1145/3580305.3599921 (2022)	The AfriSenti dataset is used for sentiment analysis in African languages, particularly focusing on Hausa. It serves as a benchmark for monolingual sentiment categorization, enabling researchers to analyze linguistic features, script variations, and textual data to understand nuances and biases in low-resource African languages. This dataset facilitates the development and evaluation of sentiment analysis models tailored to the unique characteristics of Hausa.; The AfriSenti corpora is used to train and evaluate multilingual models for sentiment analysis, specifically focusing on African languages including Hausa. It leverages translated corpora to English and addresses the linguistic nuances and cultural context of these languages. The dataset enables researchers to build and study sentiment analysis systems, enhancing understanding of contemporary language use in Hausa and other African languages.	https://afrisenti-semeval.github.io/		1	
4468	Hausa Corpus	citing_context	Common Voice v.19	https://doi.org/10.48550/arXiv.2505.20564 (2025)	https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5 (2019)	The Common Voice v.19 dataset is used to compare acoustic feature representations between Hausa and Yoruba recordings, focusing on acoustic diversity. Researchers sample 1,000 recordings from each language to analyze and compare their acoustic properties, enabling a detailed examination of linguistic and phonetic differences. This approach helps in understanding the acoustic characteristics and diversity within these languages.	https://huggingface.co/datasets/fsicoli/common_voice_19_0		1	
4469	Hausa Corpus	citing_context	HausaVisualGenome	https://doi.org/10.1155/2022/5483535 (2022)	https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8 (2014)	The English-Hausa dataset is used to evaluate machine translation performance, focusing on measuring improvements in BLEU scores for English-to-Hausa translations. This dataset enables researchers to assess the effectiveness of translation models and methodologies, providing quantitative metrics to compare different approaches in translating between these languages.	https://github.com/hausanlp/HausaVisualGenome		1	
4474	Hausa Corpus	citing_context	SemRel2024		https://doi.org/10.48550/arXiv.2402.08638 (2024)	The Hausa and Kinyarwanda datasets are used to collect pairs of sentences from news articles for linguistic analysis. Researchers focus on the sentence structure and content in these languages, employing methodologies that involve comparing and analyzing sentence pairs to understand linguistic patterns and features. This dataset enables detailed linguistic studies by providing authentic, context-rich textual data.	https://huggingface.co/datasets/SemRel/SemRel2024		1	
4475	Hausa Corpus	cited_context	Hausa NER	https://www.semanticscholar.org/paper/2ac19d63e1adba20473a6d1122c598f81efc3c58 (2022)	https://doi.org/10.18653/V1/2020.COLING-MAIN.480 (2020)	The Hausa and Yoruba datasets are used for news topic classification, focusing on multilingual text classification and analysis. Researchers employ these datasets to develop and evaluate algorithms that can accurately classify news articles into specific topics. This approach helps in understanding the performance of multilingual models and enhances the ability to process and categorize textual content in Hausa and Yoruba languages.	https://github.com/uds-lsv/transfer-distant-transformer-african		1	
4478	Hausa Corpus	cited_context	Hausa language lexicon		https://doi.org/10.1016/j.dib.2024.110124 (2024)	The Hausa language lexicon dataset is used to develop sentiment analysis resources for the Hausa language. It focuses on a lexicon containing 1000 negative and 1014 positive terms, with no neutral terms. Researchers employ this dataset to build and evaluate sentiment analysis models, addressing the need for linguistic resources in under-resourced languages like Hausa. The dataset's binary sentiment classification enables the creation of more nuanced and culturally relevant sentiment analysis tools.	https://huggingface.co/datasets/mangaphd/hausa_aug_lex		1	
4480	Hausa Corpus	cited_context | citing_context	Hausa summarization evaluation dataset		https://doi.org/10.1371/journal.pone.0285376 (2023)	The Hausa summarization evaluation dataset is used to assess the effectiveness of summarization methods, specifically on 113 Hausa news articles. Researchers employ the ROUGE evaluation toolkit to measure the quality of generated summaries, focusing on how well the summarization techniques capture essential information from the original texts. This dataset enables the evaluation and comparison of different summarization approaches in the Hausa language.; The Hausa summarization evaluation dataset is used to assess the effectiveness of summarization methods, specifically on 113 Hausa news articles. Researchers employ the ROUGE evaluation toolkit to measure the performance of their proposed summarization techniques, focusing on the accuracy and quality of generated summaries. This dataset enables the systematic evaluation and comparison of different summarization approaches in the Hausa language.	https://figshare.com/articles/dataset/S1_Data_-/22789581		1	
4482	Hausa Corpus	citing_context	HaVQA	https://doi.org/10.48550/arXiv.2505.14311 (2025)	https://doi.org/10.1109/ACCESS.2021.3127140 (2021)	The HaVQA dataset is used for visual question answering and multimodal research in the Hausa language. It integrates text, images, and other modalities to enhance understanding of Hausa content. Researchers use it to develop and evaluate visual question answering systems, focusing on improving AI's comprehension of Hausa linguistic and visual contexts.	https://github.com/shantipriyap/HausaVQA/		1	
4485	Hausa Corpus	cited_context | citing_context	large scale collection of diverse Hausa language datasets	https://doi.org/10.1109/ITED56637.2022.10051610 (2022)	https://www.semanticscholar.org/paper/6995aff0c181ef6c8236b7e9cc27af8ddcf935a1 (2021)	The large scale collection of diverse Hausa language datasets is used to provide a comprehensive resource for Hausa language research. It focuses on diverse linguistic aspects and enables various Natural Language Processing (NLP) tasks. This dataset supports research by offering a rich, varied corpus that facilitates the development and testing of NLP models and algorithms tailored to the Hausa language.; The large scale collection of diverse Hausa language datasets is used to provide a comprehensive resource for Hausa language research, supporting various linguistic studies and applications. This dataset enables researchers to conduct in-depth analyses and develop linguistic models, enhancing understanding and practical applications of the Hausa language.	https://github.com/ijdutse/hausa-corpus		1	
4489	Hausa Corpus	citing_context	MAFAND	https://doi.org/10.48550/arXiv.2409.00626 (2024)	https://doi.org/10.48550/arXiv.2205.02022 (2022)	The MAFAND dataset is used for bilingual evaluation in the news domain, focusing on assessing translation quality in African languages, including Hausa. Researchers employ this dataset to evaluate the accuracy and fluency of translations, leveraging its news-specific content to ensure contextually relevant assessments. This enables the development and refinement of translation models tailored for African languages.	https://github.com/masakhane-io/lafand-mt		1	
4494	Hausa Corpus	citing_context	MasakhaPOS	https://doi.org/10.48550/arXiv.2505.14311 (2025)		The MasakhaPOS dataset is used to develop and evaluate Part-of-Speech tagging models for 20 African languages, including Hausa, with a focus on linguistic diversity and accuracy. It is also utilized for sentiment analysis, visual question answering, and multimodal research in Hausa, enhancing understanding through integrated text and images. Additionally, the dataset supports the evaluation of machine translation systems, particularly for Hausa-English translations, both in general and news contexts, and assesses performance in low-resource and multilingual settings.	https://github.com/masakhane-io/masakhane-pos		1	
4496	Hausa Corpus	cited_context | citing_context	NaijaSenti corpus	https://www.semanticscholar.org/paper/ecd9168526d1a82ac2348c8de52bff6323322da9 (2022), https://doi.org/10.1109/UPCON62832.2024.10982868 (2024), https://doi.org/10.1109/ICMI60790.2024.10585876 (2024), https://doi.org/10.48550/arXiv.2304.06845 (2023)	https://doi.org/10.18653/v1/2021.mrl-1.11 (2021), https://doi.org/10.18653/v1/2023.semeval-1.68 (2023)	The NaijaSenti dataset is used to train and evaluate sentiment analysis models for Nigerian languages, particularly Hausa. It supports the development of AfriBERTa and other multilingual models, focusing on low-resource settings. The dataset is utilized to address gaps in existing research by combining labeled data with efficient techniques like LoRA. It enables the categorization of sentiment in Hausa language texts, especially in social media posts, enhancing the robustness and accuracy of sentiment classification in these contexts.; The NaijaSenti corpus is used to train and evaluate multilingual models for sentiment analysis, particularly focusing on African languages like Hausa. It leverages translated corpora to English and fine-tunes models such as AfriBERT to address linguistic nuances and cultural contexts specific to Naija (Pidgin English). This dataset enables researchers to develop more accurate sentiment analysis tools by incorporating diverse linguistic data and cultural insights.	https://github.com/hausanlp/NaijaSenti		1	
4497	Hausa Corpus	citing_context	NTREX-128	https://doi.org/10.48550/arXiv.2501.06374 (2025)	https://doi.org/10.18653/v1/2020.nlpcovid19-2.5 (2020)	The NTREX-128 dataset is used to train and evaluate models for multilingual tasks, particularly focusing on sentence-level translation across 200 languages. It is employed to benchmark low-resource and multilingual machine translation, assessing system performance across 101 and 128 language pairs. The dataset emphasizes low-resource scenarios, enhancing news dissemination and accessibility, especially for African languages. It enables researchers to evaluate and improve machine translation quality in diverse linguistic contexts.	https://huggingface.co/datasets/davidstap/NTREX		1	
4500	Hausa Corpus	cited_context | citing_context	Tanzil	https://doi.org/10.48550/arXiv.2205.01133 (2022)	https://doi.org/10.18653/v1/P19-1310 (2019)	The 'Quran translation in Hausa' dataset is primarily used for machine translation tasks, serving as a multilingual resource that includes Hausa. It provides a wide-coverage parallel corpus, which aids in the development and enhancement of translation models for low-resource languages. This dataset enables researchers to improve translation quality and coverage, specifically addressing the challenges of translating into and from Hausa.; The 'Quran translation in Hausa' dataset is used as part of a wide-coverage parallel corpus for machine translation tasks, particularly in low-resource language settings. It contributes to the development and evaluation of translation models for diverse language pairs, enhancing the performance and accuracy of translations in under-resourced languages like Hausa.	https://opus.nlpl.eu/Tanzil.php		1	
4463	Hausa Corpus	citing_context	African Low Resource TweetData	https://doi.org/10.1109/UPCON62832.2024.10982868 (2024)	https://doi.org/10.48550/arXiv.2304.13634 (2023)	The African Low Resource TweetData dataset is used for sentiment analysis in the Hausa language, a low-resource African language. Researchers leverage tweets to classify sentiment, employing methods tailored for low-resource settings. This dataset enables the development and evaluation of sentiment analysis models specifically for Hausa, addressing the challenge of limited annotated data in this language.	https://afrisenti-semeval.github.io/		2	
4481	Hausa Corpus	citing_context	Hausa Visual Genome	https://doi.org/10.1155/2022/5483535 (2022)	https://doi.org/10.48550/arXiv.2205.01133 (2022)	The Hausa Visual Genome dataset is used to investigate the impact of visual information on English to Hausa machine translation. Researchers focus on integrating multi-modal data to enhance translation accuracy, leveraging the dataset's visual and textual components to improve the performance of machine translation systems.	https://github.com/hausanlp/HausaVisualGenome		2	
4487	Hausa Corpus	citing_context	Lexicon dataset for the Hausa language	https://doi.org/10.1109/ICAISC64594.2025.10959458 (2025)	https://doi.org/10.1016/j.dib.2024.110124 (2024)	The Lexicon dataset for the Hausa language is used to develop a comprehensive lexicon focusing on vocabulary and linguistic features. This dataset supports natural language processing tasks by providing essential linguistic data, enabling researchers to enhance the accuracy and effectiveness of NLP models for the Hausa language.	https://doi.org/10.57967/hf/1541		2	
4490	Hausa Corpus	cited_context | citing_context	MAFAND-MT	https://doi.org/10.48550/arXiv.2505.14311 (2025), https://doi.org/10.48550/arXiv.2210.10692 (2022)	https://doi.org/10.1162/tacl_a_00474 (2021), https://doi.org/10.1162/tacl_a_00288 (2018)	The MAFAND-MT dataset is used for evaluating machine translation systems, particularly in low-resource and multilingual settings. It focuses on assessing the performance and quality of Hausa-English translations, especially in news contexts. This dataset enables researchers to compare and improve translation models across various language pairs, including Hausa.; The MAFAND-MT dataset is used in research to enhance and evaluate machine translation models for the Hausa language. It provides high-quality sentences as positive examples and creates negative examples by extracting sentences with low LASER alignment scores. This approach ensures a balanced dataset, improving model training and evaluation. Research specifically reports BLEU and CHRF scores on the test set to assess translation quality.	https://github.com/masakhane-io/lafand-mt		2	
5587	Indonesian Corpus	citing_context	Financial Phrasebank	https://doi.org/10.1109/ICEEI59426.2023.10346625 (2023)	https://doi.org/10.1002/asi.23062 (2013)	The Financial Phrasebank dataset is translated and used alongside IndoFinSent for sentiment analysis, serving as a comparative benchmark for financial sentiment detection. It is specifically applied to analyze the sentiment of Indonesian financial news article titles, focusing on detecting sentiment towards particular financial institutions. This usage highlights its role in enhancing the accuracy and reliability of financial sentiment analysis in the Indonesian context.	https://huggingface.co/datasets/takala/financial_phrasebank		1	The data set is made available for research purposes under appropriate license
5697	Indonesian Corpus	cited_context	Indonesian WordNet	https://doi.org/10.1007/978-3-642-23138-4_8 (2011)	https://www.semanticscholar.org/paper/bb2ffb3da76c4e7d74288209e49b7a97c16c4f6c (2008)	The Indonesian WordNet is an ongoing project aimed at developing a lexical database for the Indonesian language. While it is mentioned in various research contexts, no specific usage or application within current studies is described. This indicates that the dataset is primarily in development rather than actively employed in research methodologies or addressing specific research questions.	https://github.com/open-language/wordnets		1	
5558	Indonesian Corpus	citing_context	collection of Bible verses	https://doi.org/10.1109/ACCESS.2023.3308818 (2023)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.41 (2020)	The collection of Bible verses is used as mid-resource training data to fine-tune the mT5 model for translating Indonesian languages. This dataset introduces additional languages not present in the pre-training data, enhancing the model's multilingual capabilities. It specifically enables research focused on improving translation accuracy and expanding language coverage in machine translation systems.	https://www.kaggle.com/datasets/williammulianto/indonesia-bible-tb		1	
5653	Indonesian Corpus	citing_context	Indonesian emotion classification dataset	https://doi.org/10.1109/ACCESS.2024.3402809 (2024)	https://doi.org/10.1109/IALP.2018.8629262 (2018)	The Indonesian emotion classification dataset is used to classify emotions in Indonesian colloquial language tweets, focusing on five distinct emotional categories. Collected from Twitter over two weeks in June 2018, the dataset enables researchers to analyze and categorize emotional content in social media posts, employing methods tailored to the nuances of informal online communication. This supports research into sentiment analysis and emotional expression in digital contexts.	https://github.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset		1	
5711	Indonesian Corpus	citing_context	iSarcasmEval	https://doi.org/10.1109/ACCESS.2024.3416955 (2024)	https://doi.org/10.18653/v1/2022.semeval-1.111 (2022)	The iSarcasmEval dataset is used to address class imbalance in sarcasm detection by balancing the ratio of sarcastic to non-sarcastic comments. It is also utilized for translating sarcastic texts from English to Indonesian, enhancing sarcasm detection in social media posts. Additionally, it serves as a reference for developing an Indonesian sarcasm detection dataset, emphasizing expert human annotation and intended sarcasm detection.	https://www.google.com/search?q=iSarcasmEval&rlz=1C5CHFA_enDE822DE822&oq=iSarcasmEval&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIJCAEQABgKGIAEMgkIAhAAGAoYgAQyCQgDEAAYChiABDIJCAQQABgKGIAEMgkIBRAAGAoYgAQyCQgGEAAYChiABDIJCAcQABgKGIAEMgkICBAAGAoYgAQyCQgJEAAYChiABNIBBzI1M2owajGoAgCwAgA&sourceid=chrome&ie=UTF-8		1	
5739	Indonesian Corpus	citing_context	M-AILab multilingual corpora (M-AILABS)	https://doi.org/10.1109/ASRU57964.2023.10389730 (2023)	https://doi.org/10.48550/arXiv.2204.02470 (2022)	The M-AILab multilingual corpora (M-AILABS) dataset is used for multilingual speech recognition and translation, offering diverse language data for training and evaluation. It supports research in developing robust speech recognition systems and translation models by providing a wide range of linguistic content, enhancing model performance across multiple languages.	https://github.com/imdatceleste/m-ailabs-dataset		1	
5728	Indonesian Corpus	cited_context | citing_context	KOMPAS	https://doi.org/10.1109/ASRU57964.2023.10389730 (2023), https://www.semanticscholar.org/paper/9e06d3e4109871999611cb886ebdaca62e505030 (2008), https://doi.org/10.11591/eei.v12i2.4529 (2023)	https://www.semanticscholar.org/paper/8ed9c7d54fd3f0b1ce3815b2eca82147b771ca8f (2003), https://doi.org/10.1109/ISITIA.2016.7828656 (2016)	The KOMPAS dataset is used to enhance information retrieval systems for Bahasa Indonesia by augmenting article datasets and improving stemming effects analysis. It is also utilized to gather text materials for creating multilingual Indonesian language datasets, specifically focusing on magazine and newspaper content for the ML-Superb Challenge. This dataset significantly increases the corpus size, enabling more robust research in information retrieval and multilingual language processing.; The 'kompas' dataset is used to identify quotations in Indonesian online news texts. It involves analyzing 2506 sentences to develop and evaluate a rule-based method for quotation detection. This dataset enables researchers to enhance the accuracy of quotation identification in news articles, contributing to natural language processing and text analysis methodologies.	https://www.kompas.com		1	
5535	Indonesian Corpus	citing_context	AC-IQuAD	https://doi.org/10.1109/ICTIIA61827.2024.10761325 (2024)	https://doi.org/10.1109/SIBIRCON56155.2022.10017011 (2022)	The AC-IQuAD dataset is used to construct an Indonesian question answering (QA) dataset by leveraging Wikidata. It focuses on the automatic construction and evaluation of QA systems, enabling researchers to develop and test algorithms for generating and assessing QA pairs in the Indonesian language. This dataset facilitates the creation of robust QA models tailored to Indonesian linguistic nuances.	https://huggingface.co/datasets/SEACrowd/ac_iquad		1	
5741	Indonesian Corpus	cited_context	manually tagged Twitter messages	https://doi.org/10.1109/IALP.2016.7876005 (2016)		The manually tagged Twitter messages dataset is used to train the Stanford Log-linear Part-Of-Speech Tagger for Indonesian language, specifically focusing on accurate part-of-speech tagging of social media text. This dataset enables researchers to improve the performance of natural language processing tools in handling the unique linguistic features of Indonesian social media content.	https://github.com/kmkurn/id-pos-tagging		1	
5540	Indonesian Corpus	cited_context | citing_context	AM2iCo	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.18653/v1/2021.emnlp-main.571 (2021)	The AM2iCo dataset is used to evaluate word meaning in context and visually grounded reasoning across multiple languages, including Indonesian. It incorporates adversarial examples to assess model robustness and employs an ImageNet-style hierarchy to represent diverse linguistic and cultural contexts. This dataset enables researchers to evaluate cross-lingual performance and multicultural reasoning, specifically focusing on 14 language pairs and diverse cultural settings.; The AM2iCo dataset is used to evaluate word meaning in context and visually grounded reasoning across multiple languages, including Indonesian. It incorporates adversarial examples to assess model robustness and cross-lingual performance. The dataset employs an ImageNet-style hierarchy to represent diverse linguistic and cultural contexts, enabling research on multicultural reasoning over vision and language. Specifically, it evaluates the ability of representation models to understand word meaning in cross-lingual contexts for 14 language pairs, including Indonesian.	https://github.com/cambridgeltl/AM2iCo		1	
5554	Indonesian Corpus	cited_context | citing_context	Cendana	https://doi.org/10.1109/IALP51396.2020.9310459 (2020), https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/68c1f36518e1e4f99197f894adea2284f5b7aca1 (2019)	The Cendana dataset is used to create a dictionary of informal Indonesian words and to build a treebank, focusing on word-level standardization without sentence-level context. It is also utilized for training, validating, and testing models, including grammatical analyses such as parts-of-speech, phrases, entity relations, and meaning representations. This dataset enables detailed linguistic analysis and model development for informal Indonesian language processing.; The Cendana dataset is used for linguistic analysis of informal Indonesian, employing methodologies that focus on parts-of-speech tagging, phrase identification, entity relations, and meaning representations. This dataset enables researchers to explore the structural and semantic aspects of informal language use, providing insights into how linguistic elements interact in natural, everyday communication.	https://github.com/davidmoeljadi/INDRA/tree/master/tsdb/gold/Cendana		1	
5776	Indonesian Corpus	citing_context	online Bible	https://doi.org/10.1109/IC3INA64086.2024.10732202 (2024)	https://doi.org/10.47738/jads.v4i3.113 (2023)	The 'online Bible' dataset is used to source 30,000 sentence pairs for developing a Madurese-Indonesian translation model. It focuses on linguistic alignment and translation accuracy, employing methodologies that enhance the precision of translations between these languages. This dataset enables researchers to address specific challenges in cross-linguistic translation, particularly in aligning grammatical structures and maintaining semantic fidelity.	https://www.kaggle.com/datasets/williammulianto/indonesia-bible-tb		1	
5719	Indonesian Corpus	cited_context	Kamus Besar Bahasa Indonesia	https://doi.org/10.1109/ICEEI.2015.7352469 (2015)		The Kamus Besar Bahasa Indonesia dataset is used to verify the accuracy of generated words, ensuring they align with standard Indonesian vocabulary and usage. This involves comparing generated content against the dataset to check for correctness and adherence to linguistic norms. This methodology supports research focused on natural language generation and processing, enhancing the reliability and authenticity of generated text in Indonesian.	https://kbbi.web.id/		1	
5703	Indonesian Corpus	cited_context | citing_context	IndoSum	https://doi.org/10.1109/ICETSIS61505.2024.10459565 (2024), https://doi.org/10.1145/3341620.3341626 (2019), https://doi.org/10.33096/ilkom.v15i1.1532.124-131 (2023), https://doi.org/10.46799/jsa.v5i9.1483 (2024), https://doi.org/10.33795/jip.v10i4.5242 (2024), https://doi.org/10.12720/jait.14.4.656-667 (2023), https://doi.org/10.1109/ICAICTA59291.2023.10389952 (2023), https://doi.org/10.18653/v1/2020.aacl-main.60 (2020), https://doi.org/10.32890/jict2022.21.1.4 (2021), https://doi.org/10.1080/09540091.2021.1937942 (2021), https://doi.org/10.15676/ijeei.2021.13.4.10 (2021), https://www.semanticscholar.org/paper/330d74432b0f5171bdd900ea9093c6864a023b2a (2019)	https://doi.org/10.1109/IALP.2018.8629109 (2018)	The IndoSum dataset is primarily used for benchmarking and evaluating text summarization models in Indonesian, particularly focusing on news articles. It provides a large corpus of articles and manually-written summaries, enabling researchers to assess the performance, quality, and coherence of both abstractive and extractive summarization techniques. As the largest Indonesian summarization dataset, it supports the development and comparison of summarization methods, emphasizing the importance of summarization accuracy and content quality.; The IndoSum dataset is primarily used for Indonesian text summarization research, focusing on both abstractive and extractive summarization methods. It is employed to train, test, and evaluate various models, including BertSum, Pointer Generator Network, and IndoBERT, using metrics like BERTScore and recall-oriented measures. The dataset consists of 19k document-summary pairs, enabling 5-fold cross-validation to assess model performance and robustness. It serves as a benchmark for comparing summarization models, particularly in generating concise and coherent summaries of Indonesian news articles.	https://github.com/kata-ai/indosum		1	
5699	Indonesian Corpus	citing_context	Indonesia’s PPKM Tweets	https://doi.org/10.1109/ICITISEE57756.2022.10057720 (2022)	https://doi.org/10.34123/icdsos.v2021i1.237 (2022)	The 'Indonesia’s PPKM Tweets' dataset is used to analyze public sentiment towards the implementation of PPKM policies in Indonesia. Researchers employ various classification methods for sentiment analysis, comparing their effectiveness. This dataset enables the examination of public reactions and attitudes, providing insights into the social impact of PPKM measures.	https://www.kaggle.com/code/mochkholil/ppkm-sentiment-classification/data		1	
5752	Indonesian Corpus	citing_context	NusaX	https://doi.org/10.1109/ICoDSA58501.2023.10277136 (2023)	https://doi.org/10.48550/arXiv.2205.15960 (2022)	The Multilingual Parallel Sentiment Dataset for 10 Indonesian Regional Languages is used for hyperparameter tuning and optimizing sentiment analysis models, focusing on learning rate and epoch adjustments. It supports research into linguistic nuances and sentiment expressions across 10 Indonesian dialects, enhancing understanding of regional sentiment. Additionally, the dataset aids machine translation research by facilitating the implementation of processing steps in the IndoBART-V2 model, specifically for sentiment analysis tasks.	https://github.com/IndoNLP/nusax		1	
5735	Indonesian Corpus	cited_context	Leipzig Indonesian Corpus	https://doi.org/10.1109/ICODSE.2017.8285846 (2017)	https://www.semanticscholar.org/paper/1b560f892432fb853d233c92f9294640bc91de3c (2012)	The Leipzig Indonesian Corpus is used to build a bigram model for computing bigram probabilities in the Indonesian language. This involves analyzing a dataset of a million sentences sourced from web crawls, news, and Wikipedia entries. The corpus enables researchers to focus on linguistic patterns and statistical properties of word sequences in Indonesian, facilitating natural language processing tasks and language modeling.	https://corpora.wortschatz-leipzig.de/en?corpusId=ind_mixed_2013		1	
5704	Indonesian Corpus	citing_context	IndoWiki	https://doi.org/10.1109/IWBIS56557.2022.9924844 (2022)	https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc (2019)	The IndoWiki dataset is used to evaluate and train models for Indonesian language tasks. It supports benchmarking and comparison of models like IndoBERT, DistilBERT, and IndoKEPLER, focusing on performance in sequence labeling and factual knowledge recall. The dataset's specialized corpus enhances model training, enabling researchers to assess and improve model capabilities in handling Indonesian language-specific challenges.	https://huggingface.co/datasets/SEACrowd/indowiki		1	
5612	Indonesian Corpus	citing_context	Indonesian Tweets	https://doi.org/10.1145/3625821 (2023)	https://doi.org/10.18653/v1/W19-3506 (2019)	multi-label hate speech and abusive language detection in the Indonesian Twitter.	https://github.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection		1	
5644	Indonesian Corpus	citing_context	The Gede Manggala Putra and Nurjanah [25] tweet dataset	https://doi.org/10.1145/3625821 (2023)	https://doi.org/10.1109/ICACSIS51025.2020.9263084 (2020)	a hate speech dataset in Indonesian language from Instagram. The dataset was extracted in the range of June 2019 until April 2020.	https://github.com/nurindahpratiwi/dataset-hate-speech-instagram		1	
5534	Indonesian Corpus	cited_context	abusive words lexicon	https://doi.org/10.1145/3639233.3639247 (2023)	https://doi.org/10.20473/jisebi.6.1.9-17 (2020)	The abusive words lexicon dataset is primarily used to enhance the detection of hate speech in Indonesian social media. It serves as both an implicit and explicit lexicon, providing a structured list of abusive terms and a dictionary of harmful words. This lexicon is integrated into machine learning models to improve their accuracy in identifying hate speech. Additionally, the dataset has been expanded to include abusive language specific to Betawi, Madurese, Sundanese, and Javanese dialects, further refining hate speech detection in these regional languages.	https://e-journal.unair.ac.id/JISEBI/article/view/17372		1	
5575	Indonesian Corpus	citing_context	Desrul and Romadhony dataset	https://doi.org/10.11591/ijeecs.v33.i1.pp450-462 (2024)	https://doi.org/10.1109/ISRITI48646.2019.9034620 (2019)	The Desrul and Romadhony dataset is primarily used for detecting abusive language in Indonesian online news comments. It is employed to evaluate model performance, particularly in addressing underperformance issues and adapting to diverse annotation styles. The dataset facilitates systematic collection and extraction of user comments, enabling researchers to train and assess models for abusive language detection.	https://doi.org/10.1109/ISRITI48646.2019.9034620		1	
5729	Indonesian Corpus	citing_context	Kompas newspaper	https://doi.org/10.14569/IJACSA.2021.0120327 (2021)		The Kompas newspaper dataset is used to derive text formats for Indonesian language processing, specifically focusing on the structural elements of news articles. This involves analyzing the layout and content structure of articles to enhance text processing methodologies. The dataset enables researchers to develop and refine techniques for handling and extracting information from Indonesian news texts, improving the accuracy and efficiency of language processing systems.	https://github.com/kmkurn/id-nlp-resource		1	
5637	Indonesian Corpus	citing_context	IndoBERTweet	https://doi.org/10.14569/ijacsa.2023.0141053 (2023)	https://doi.org/10.18653/v1/2020.emnlp-demos.2 (2020)	The IndoBERTweet dataset is used to train a pretrained language model specifically tailored for Indonesian Twitter data. It focuses on initializing domain-specific vocabulary effectively to enhance the model's performance on Indonesian tweets. This dataset enables researchers to address challenges unique to Indonesian social media content, improving the accuracy and relevance of natural language processing tasks in this domain.	https://github.com/indolem/IndoBERTweet		1	
a5552	Indonesian Corpus	cited_context | citing_context	EmoT	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://doi.org/10.1109/IALP.2018.8629262 (2018)		https://github.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset		1	
5660	Indonesian Corpus	citing_context	Indonesian (IND)	https://doi.org/10.18653/v1/2020.emnlp-main.423 (2020)	https://doi.org/10.18653/v1/P16-1156 (2016)	The Indonesian (IND) dataset is used to train and evaluate morphological segmentation models. It consists of 8000 training, 1000 development, and 1000 test examples, enabling researchers to develop and assess algorithms that accurately segment words into their constituent morphemes. This dataset supports research focused on improving natural language processing techniques for the Indonesian language.	https://www.repository.cam.ac.uk/items/ed4a71ef-7c11-4958-ae34-4fb1cde891e2/full		1	
5701	Indonesian Corpus	cited_context | citing_context	IndoNLU	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021), https://doi.org/10.18653/v1/2021.emnlp-main.833 (2021), https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021), https://doi.org/10.48550/arXiv.2502.18148 (2025), https://doi.org/10.14569/ijacsa.2022.0131110 (2022), https://doi.org/10.1109/ICORIS56080.2022.10031483 (2022), https://doi.org/10.1109/ICoDSA58501.2023.10277136 (2023), https://doi.org/10.20532/cit.2022.1005478 (2023), https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020), https://www.semanticscholar.org/paper/8ae8e5e840c5f43d4d72cbc4595690fba01aa799 (2020)	The IndoNLU dataset is primarily used as a benchmark for evaluating Indonesian natural language understanding (NLU) across various NLP tasks, including emotion classification, text summarization, and sentiment analysis. It provides a comprehensive resource for assessing model performance, focusing on multilingual and low-resource language understanding. The dataset supports benchmarking and model comparison, offering annotated data for diverse linguistic challenges and tasks.; The IndoNLU dataset is primarily used to evaluate and benchmark the performance of natural language understanding (NLU) and language modeling tasks in Indonesian. It focuses on assessing pretrained multilingual models, particularly in low-resource settings, and evaluates their capabilities across various NLU tasks such as semantic parsing, part-of-speech tagging, and named entity recognition. The dataset also supports the creation and evaluation of multimodal datasets, incorporating text, speech, and image data, to address a wide range of NLP tasks.	https://github.com/IndoNLP/indonlu		1	
5762	Indonesian Corpus	citing_context	news articles	https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021)	https://doi.org/10.18653/v1/2020.acl-main.156 (2020)	The 'news articles' dataset is primarily used to train translation models, providing a large, diverse, and structured source of information. It offers specialized vocabulary, formal language structures, and captures both current and colloquial language usage across various topics and styles. This dataset enhances the model's ability to handle low-resource languages through synthetic data generation, improving translation accuracy and robustness.	https://github.com/gunnxx/indonesian-mt-data		1	
5737	Indonesian Corpus	cited_context | citing_context	Liputan6	https://doi.org/10.18653/v1/2024.emnlp-main.1085 (2024), https://doi.org/10.1109/ICAICTA59291.2023.10390436 (2023), https://doi.org/10.48550/arXiv.2310.04928 (2023), https://doi.org/10.1109/ICITACEE58587.2023.10277395 (2023), https://doi.org/10.33795/jip.v10i4.5242 (2024), https://doi.org/10.1109/EIConCIT50028.2021.9431880 (2021)	https://doi.org/10.18653/v1/2020.aacl-main.60 (2020)	The Liputan6 dataset is primarily used for text summarization tasks in Indonesian, providing a large-scale resource to train and evaluate summarization models. It supports various NLP tasks, including abstractive and multidocument summarization, and has been employed in experiments with models like mBERT, IndoBERT, BERT2BERT, and Pegasus. The dataset contains 10,972 documents and is used to measure summarization quality using ROUGE metrics, focusing on improving F1 scores and generating coherent summaries. Additionally, it is used for text simplification, providing input documents and sentence positions to generate simplified versions of complex sentences.; The Liputan6 dataset is used for evaluating and training text summarization models, particularly focusing on Indonesian text. It serves as a benchmark for assessing the performance of models like BertSum. The dataset's large scale and specific focus on Indonesian language enable researchers to develop and refine summarization techniques tailored to this linguistic context.	https://github.com/fajri91/sum_liputan6		1	
5536	Indonesian Corpus	cited_context | citing_context	A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media	https://doi.org/10.18653/v1/W19-3506 (2019)	https://doi.org/10.1109/SocialCom-PASSAT.2012.55 (2012)	The dataset is used to detect abusive language in Indonesian social media, specifically focusing on identifying offensive content to improve online safety. Machine learning models are employed to analyze the data, enabling researchers to develop more effective methods for recognizing and mitigating abusive language.; The dataset is used to detect and classify abusive language in Indonesian social media, focusing on offensive content across various social media genres. Researchers employ this dataset to develop and evaluate models for identifying abusive language, enhancing the understanding and management of online toxicity in Indonesian social media platforms.	https://github.com/okkyibrohim/id-abusive-language-detection		1	
5556	Indonesian Corpus	cited_context | citing_context	CLICK-ID	https://doi.org/10.25139/inform.v7i2.4686 (2022), https://doi.org/10.1109/ICAICTA59291.2023.10389952 (2023), https://doi.org/10.1016/j.dib.2020.106231 (2020)	https://doi.org/10.1016/j.dib.2020.106231 (2020)	The CLICK-ID dataset is primarily used for training and evaluating models to detect clickbait in Indonesian news headlines. Researchers employ various deep learning models, including IndoBERT, Bi-LSTM, and CNN, to classify headlines as clickbait or non-clickbait. The dataset, consisting of 15,000 annotated headlines from 12 online news publishers, enables detailed analysis of clickbait patterns and linguistic features, focusing on the performance and accuracy of these models in identifying sensationalist content.; The CLICK-ID dataset is used for training and evaluating classifiers to detect Indonesian clickbait headlines. Research focuses on the accuracy and effectiveness of models, employing deep learning techniques to analyze and classify clickbait data. This dataset enables researchers to develop and test algorithms specifically tailored for identifying clickbait in Indonesian content.	https://data.mendeley.com/datasets/k42j7x2kpn/1		1	
5628	Indonesian Corpus	cited_context	IDN Tagged Corpus	https://doi.org/10.26418/jlk.v2i2.20 (2019)	https://doi.org/10.1109/IALP.2014.6973521 (2014)	The IDN Tagged Corpus is primarily used for training and evaluating part-of-speech (POS) taggers for Indonesian. It contains manually labeled tokens ranging from 14,165 to 355,000, which are utilized to develop and assess rule-based, bidirectional LSTM, and CRF models. This dataset serves as a standard resource in Indonesian language research, enabling the comparison of different tagging methods and focusing on metrics such as F1 scores and POS tagging accuracy.	https://github.com/famrashel/idn-tagged-corpus		1	
5676	Indonesian Corpus	citing_context	Indonesian sentiment dictionary	https://doi.org/10.29207/resti.v7i3.4726 (2023)	https://doi.org/10.1109/IALP.2017.8300625 (2017)	The Indonesian sentiment dictionary is used to label datasets with sentiment scores, primarily for developing and applying lexicon-based techniques in Indonesian sentiment analysis. This dataset enables researchers to focus on the nuances of the Indonesian language, enhancing the accuracy of sentiment classification in text data.	https://huggingface.co/datasets/SEACrowd/inset_lexicon		1	
5709	Indonesian Corpus	cited_context | citing_context	InSet Lexicon	https://doi.org/10.31937/ti.v14i1.2540 (2022), https://doi.org/10.18653/v1/W19-3506 (2019), https://doi.org/10.31098/cset.v1i1.397 (2021)	https://doi.org/10.31098/cset.v1i1.397 (2021), https://doi.org/10.1109/IALP.2017.8300625 (2017)	The InSet Lexicon is used for sentiment analysis in Indonesian, specifically to analyze and classify sentiments in social media content such as microblogs. It provides high accuracy in distinguishing between negative and positive sentiments, supporting the evaluation and improvement of sentiment analysis models in Indonesian language datasets.; The InSet Lexicon is used in sentiment analysis research, specifically for classifying and evaluating sentiments in Indonesian text. It supports the development of classification models that categorize text into positive, negative, and neutral classes. The lexicon is particularly useful for analyzing social media content, such as microblogs, enabling researchers to focus on the nuances of negative and positive sentiments in online discussions.	https://github.com/fajri91/InSet		1	
5668	Indonesian Corpus	cited_context	minangNLP	https://doi.org/10.48550/arXiv.2203.13357 (2022)	https://www.semanticscholar.org/paper/b50c7c60416e334440e18832a9ee9b6fddf636d2 (2020)	The Indonesian Minangkabau parallel corpus is used to develop sentiment analysis tools and build a parallel corpus for Minangkabau, enhancing computational linguistics and machine translation. This dataset supports research in linguistic studies by providing a structured resource for analyzing and translating Minangkabau text, facilitating advancements in both natural language processing and linguistic analysis.	https://github.com/fajri91/minangNLP		1	
5545	Indonesian Corpus	cited_context | citing_context	Barasa	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IALP.2018.8629181 (2018)	The Barasa Barasa dataset is primarily used for sentiment analysis in Indonesian, serving as a SentiWordNet resource that provides sentiment scores for words and phrases. It is utilized in both general sentiment analysis and aspect-based sentiment analysis, particularly for car reviews. The dataset, which includes around a thousand car reviews from Indonesian online platforms, aids in detecting aspects and classifying sentiments using deep neural networks. Its 16 MB size and lack of specified dataset splits are notable characteristics.; The Barasa Barasa dataset is primarily used for sentiment analysis in Indonesian, particularly for aspect-based sentiment analysis of car reviews. It consists of around a thousand car reviews and provides sentiment scores for words and phrases. Researchers use it to train and evaluate models, often employing deep neural networks to detect aspects and classify sentiments. The dataset, which is 16 MB and lacks a predefined split, serves as a valuable resource for enhancing sentiment classification accuracy in Indonesian text.	https://github.com/neocl/barasa		1	
5581	Indonesian Corpus	cited_context | citing_context	Emotion dataset from Indonesian public opinion	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1016/j.dib.2022.108465 (2022)	The 'Emotion dataset from Indonesian public opinion' is used to analyze and classify emotions in Indonesian tweets, categorizing posts into six emotion values: anger, fear, joy, love, sadness, and neutrality. It consists of 582 annotated sentences and is employed to train and evaluate emotion classification models, enhancing the understanding of emotional expressions in social media and public forums. Additionally, the dataset supports multilingual NLP research, covering 12 languages across various tasks, including emotion classification and sentiment analysis.; The Emotion dataset from Indonesian public opinion is used to train and evaluate emotion classification models, particularly for public opinion analysis in Indonesian social media. It supports multilingual NLP research, covering 12 languages, and includes 582 annotated sentences. The dataset is employed to classify emotions such as anger, fear, joy, love, sadness, and neutrality, enhancing the understanding of emotional expressions in online public forums.	https://github.com/Ricco48/Emotion-Dataset-from-Indonesian-Public-Opinion		1	
5618	Indonesian Corpus	citing_context	HoASA (IndoNLU Split)	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICEEI47359.2019.8988898 (2019)	The HoASA (IndoNLU Split) dataset is used for aspect-based sentiment analysis of hotel reviews, specifically focusing on categorizing aspects and sentiments in user-generated content from AiryRooms. It is split into training, development, and test sets to facilitate the development and evaluation of models that can accurately identify and classify aspects and their associated sentiments in review texts.	https://github.com/annisanurulazhar/absa-playground		1	
5623	Indonesian Corpus	cited_context | citing_context	ID Abusive	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1016/J.PROCS.2018.08.169 (2018)	The 'ID Abusive' dataset is used to detect abusive language in Indonesian social media, primarily focusing on Twitter and comments from top news stories in 2019 from sources like Kompas, Kaskus, and Detik. It is employed to train and evaluate models for detecting various forms of hate speech and abusive content, with a focus on categorizing specific types of abusive language. The dataset is annotated and contains 39,246 documents, enabling researchers to develop and refine abusive language detection systems.; The 'ID Abusive' dataset is used for detecting and classifying abusive language in Indonesian social media, particularly on platforms like Twitter and in comments from online news and forums. It employs a labeled dataset with a detailed label scheme to identify and categorize various types of abusive content, including hate speech. This dataset enables researchers to train and evaluate models for the detection and classification of abusive language, focusing on improving the accuracy and reliability of such systems in the Indonesian context.	https://github.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection		1	
5663	Indonesian Corpus	cited_context | citing_context	Indonesian - Lampung Nyo corpus	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1088/1742-6596/1751/1/012036 (2021)	The Indonesian - Lampung Nyo corpus is used to construct a parallel corpus from Lampung language books for elementary and junior high school levels, specifically focusing on the Lampung Nyo dialect. This dataset enables researchers to develop educational materials and linguistic resources, supporting the preservation and teaching of the Lampung Nyo dialect.	https://huggingface.co/datasets/SEACrowd/parallel_id_nyo		1	
5705	Indonesian Corpus	citing_context	INDspeech_NEWS_EthnicSR	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICSDA.2012.6422469 (2012)	The INDspeech_NEWS_EthnicSR dataset is extensively used for automatic speech recognition (ASR) and text-to-speech synthesis in Indonesian and its regional languages, such as Javanese, Sundanese, Batak, and Balinese. It enhances the accuracy and naturalness of speech processing in various contexts, including news broadcasts, telephone dialogues, and digit recognition. The dataset, containing 13,000 utterances from 9000 speakers, is also utilized for machine translation, sentiment analysis, part-of-speech tagging, and hate speech detection, focusing on preserving and analyzing the phonetic and linguistic characteristics of Indonesian ethnic languages.	https://huggingface.co/datasets/SEACrowd/indspeech_news_ethnicsr		1	
5707	Indonesian Corpus	cited_context	INDspeech_NEWS_TTS	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICSDA.2012.6422469 (2012)	The INDspeech_NEWS_TTS dataset is extensively used for automatic speech recognition (ASR) and text-to-speech (TTS) synthesis, particularly in Indonesian and regional languages such as Javanese, Sundanese, Batak, and Balinese. It enhances speech-to-text accuracy, naturalness, and intelligibility in news broadcasts and telephone dialogues. The dataset supports large vocabulary continuous speech recognition, digit recognition, and speaker verification, improving voice-controlled applications and security. It also aids in hate speech detection, sentiment analysis, and machine translation, contributing to language preservation and cross-lingual communication. The dataset's diverse content, including 13,000 utterances from 9000 speakers, facilitates robust training and evaluation of speech technologies.	https://huggingface.co/datasets/SEACrowd/indspeech_news_tts		1	
5718	Indonesian Corpus	cited_context | citing_context	Kamus Alay	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IALP.2018.8629151 (2018)	The Kamus Alay dataset is used to provide a lexicon for text normalization, specifically mapping Indonesian colloquial words to their formal equivalents. This dataset enables researchers to develop and evaluate algorithms for converting informal language into standard Indonesian, enhancing the accuracy of natural language processing tasks such as sentiment analysis and machine translation.; The Kamus Alay dataset is used to provide a lexicon for text normalization of Indonesian colloquial words, supporting research in natural language processing and colloquial language understanding. It focuses on mapping informal to formal language, enabling more accurate processing and understanding of colloquial text in NLP applications.	https://github.com/nasalsabila/kamus-alay		1	
5725	Indonesian Corpus	cited_context | citing_context	KaWAT	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/330d74432b0f5171bdd900ea9093c6864a023b2a (2019)	The KaWAT dataset is used to evaluate word representation models in the Indonesian language by assessing their ability to capture semantic and syntactic relationships through a word analogy task. This dataset enables researchers to test and compare the effectiveness of different word embedding models in handling linguistic nuances specific to Indonesian.; The KaWAT dataset is used to evaluate word representation models in the Indonesian language by assessing their ability to capture semantic and syntactic relationships through a word analogy task. This dataset enables researchers to test and compare the effectiveness of different models in understanding linguistic nuances specific to Indonesian.	https://github.com/kata-ai/kawat		1	
5733	Indonesian Corpus	cited_context | citing_context	Korpus Nusantara	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.21437/SLTU.2018-14 (2018), https://www.semanticscholar.org/paper/1a2e9fe815a3c2a5f02d3e169378240a56bdbff1 (2016)	The Korpus Nusantara dataset is extensively used for machine translation, text-to-speech synthesis, and automatic speech recognition tasks, particularly for Indonesian and its regional languages. It supports the development and evaluation of NLP models, enhancing translation quality, speech synthesis, and transcription accuracy. The dataset is also utilized for hate speech detection, sentiment analysis, and part-of-speech tagging in Indonesian texts, focusing on improving the accuracy and fluency of these tasks. It includes speech data from various sources, such as news broadcasts and telephone dialogues, and employs techniques like deduplication and HMM-based models for training and testing.; The Korpus Nusantara dataset is extensively used for automatic speech recognition (ASR), text-to-speech synthesis, and machine translation, particularly for Indonesian and regional languages like Sundanese, Javanese, Batak, and Balinese. It supports ASR tasks in various contexts, including telephone dialogues, news broadcasts, and digit recognition, enhancing speech-to-text accuracy and voice-controlled applications. The dataset also aids in developing and evaluating NLP models, focusing on Indonesian language data from sources like Common Crawl and NLLB, with deduplication techniques. Additionally, it is used for hate speech detection, sentiment analysis, and speaker verification, improving security, emotional content understanding, and multilingual communication.	https://huggingface.co/datasets/SEACrowd/korpus_nusantara		1	
5580	Indonesian Corpus	cited_context | citing_context	EmoT dataset	https://doi.org/10.48550/arXiv.2212.09648 (2022), https://doi.org/10.29207/resti.v7i4.5035 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.833 (2021)	https://doi.org/10.1109/IALP.2018.8629262 (2018)	The EmoT dataset is primarily used for emotion classification and sentiment analysis tasks, specifically focusing on Indonesian tweets and social media posts. It is employed to train and evaluate models on identifying emotional content and sentiments in text, including single-label multi-class classification and multi-label hate speech detection. This dataset enables researchers to analyze public sentiment and emotional responses in online platforms, providing insights into social dynamics and public opinion.; The EmoT dataset is used for emotion classification tasks, specifically to analyze and classify emotional content in Indonesian tweets and other social media posts. It is employed to train and evaluate natural language understanding models, focusing on sentiment analysis and emotional content. The dataset supports research by enabling the assessment of model performance using metrics like the macro F1 score.	https://huggingface.co/datasets/SEACrowd/emot		1	
5639	Indonesian Corpus	citing_context	IndoLEM sentiment	https://doi.org/10.48550/arXiv.2309.10661 (2023)	https://doi.org/10.1109/IALP.2017.8300625 (2017)	The IndoLEM sentiment dataset is primarily used for sentiment and emotion analysis in the Indonesian language, drawing data from Twitter and hotel reviews. It is employed to train and evaluate models on sentiment and emotion classification, contributing to the development and benchmarking of Indonesian NLP models. The dataset includes 5048 sentiment-labeled and 4401 emotion-labeled samples, enhancing the robustness and performance of these models in understanding and classifying emotional and sentiment content in Indonesian text.	https://github.com/indolem/indolem		1	
a5585	Indonesian Corpus	cited_context | citing_context	IDK-MRC	https://doi.org/10.48550/arXiv.2310.04928 (2023)	https://doi.org/10.18653/v1/2022.emnlp-main.465 (2022)		https://github.com/rifkiaputri/IDK-MRC		1	
5640	Indonesian Corpus	citing_context	IndoMMLU	https://doi.org/10.48550/arXiv.2310.04928 (2023), https://doi.org/10.15587/1729-4061.2024.309972 (2024), https://doi.org/10.48550/arXiv.2506.02573 (2025)	https://doi.org/10.18653/v1/2020.emnlp-main.700 (2020)	The IndoMMLU dataset is used to evaluate the performance of models across various educational and cultural tasks in Indonesia. It assesses problem-solving skills, language understanding, and the impact of safety fine-tuning on downstream tasks such as career-related and primary school exams. The dataset follows the format of English MMLU and is employed to investigate educational assessment and cultural aspects through exam questions, enabling researchers to analyze the effectiveness of fine-tuned models in an Indonesian context.	https://huggingface.co/datasets/indolem/IndoMMLU		1	
5626	Indonesian Corpus	cited_context | citing_context	IDK-MRC test set	https://doi.org/10.48550/arXiv.2310.04928 (2023), https://doi.org/10.48550/arXiv.2210.13778 (2022), https://doi.org/10.1109/ICAICTA59291.2023.10390123 (2023), https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.48550/arXiv.2210.13778 (2022), https://doi.org/10.18653/v1/2020.acl-main.442 (2020)	The IDK-MRC dataset is primarily used to evaluate machine reading comprehension (MRC) models in Indonesian, focusing on both answerable and unanswerable questions. It is applied to assess the models' ability to handle complex and factual questions over news and Wikipedia documents, as well as to conduct unanswerability error analysis. This dataset enables researchers to test and improve the performance of question-answering systems, particularly in handling unanswerable queries and cross-lingual generalization.; The IDK-MRC test set is used to evaluate machine reading comprehension models on Indonesian text, focusing on both answerable and unanswerable questions. It enables researchers to assess model performance and conduct unanswerability error analysis, specifically in the context of handling unanswerable questions in Indonesian language models. This dataset provides a standardized test set for evaluating and improving the capabilities of machine reading comprehension systems in handling unanswerability.	https://github.com/rifkiaputri/IDK-MRC		1	
5702	Indonesian Corpus	citing_context	IndoSMD	https://doi.org/10.48550/arXiv.2311.00958 (2023)	https://doi.org/10.1609/aaai.v36i10.21320 (2021)	The IndoSMD dataset is used to evaluate task-oriented dialog systems, particularly focusing on the Indonesian language. It is employed to assess the performance of the GALAXY model and framework in monolingual settings, addressing convergence issues and comparing results with the English Cam-Rest dataset. The dataset includes dialogues and utterances for restaurant reservation tasks, enabling researchers to analyze system performance in specific linguistic and task contexts.	https://huggingface.co/datasets/SEACrowd/indosmd		1	
5596	Indonesian Corpus	citing_context	GEN-X	https://doi.org/10.48550/arXiv.2311.01012 (2023)	https://doi.org/10.48550/arXiv.2305.14288 (2023)	The GEN-X dataset is used to fine-tune models for handling colloquial texts, enhancing their robust performance. It serves as a substitute for training data in XCOPA-ID and COPAL-ID, focusing on multilingual augmented commonsense reasoning tasks generated by GPT-4. This dataset enables researchers to improve model performance in understanding and processing colloquial language and complex reasoning tasks.	https://github.com/mbzuai-nlp/Gen-X		1	
5520	Indonesian Corpus	cited_context	14 new monolingual STR datasets		https://doi.org/10.48550/arXiv.2402.08638 (2024)	The 14 new monolingual STR datasets are curated to address gaps in monolingual semantic textual similarity, particularly for languages like Indonesian. These datasets enhance cross-lingual evaluation and model training by providing diverse and representative text samples, enabling researchers to improve the performance and robustness of natural language processing models across different languages.	https://huggingface.co/datasets/SemRel/SemRel2024		1	
5564	Indonesian Corpus	citing_context	COPAL	https://doi.org/10.48550/arXiv.2402.17302 (2024)	https://doi.org/10.48550/arXiv.2310.04928 (2023)	The COPAL-ID dataset is used to evaluate language models on Indonesian primary school exams and cultural nuances, focusing on regional topics like Minangkabau and Sundanese cultures. It incorporates native speaker insights, enhancing cultural relevance and enabling researchers to assess model comprehension and reasoning tasks related to these specific cultural contexts.	https://huggingface.co/datasets/haryoaw/COPAL		1	
5597	Indonesian Corpus	citing_context	Gigaspeech2		https://doi.org/10.48550/arXiv.2406.11546 (2024)	The Gigaspeech dataset is used to train Automatic Speech Recognition (ASR) models, demonstrating their effectiveness compared to competitive systems. It supports multilingual speech recognition, enhancing model performance across multiple languages. This dataset enables researchers to improve ASR accuracy and robustness, particularly in multilingual contexts.	https://github.com/SpeechColab/GigaSpeech2		1	
5579	Indonesian Corpus	citing_context	NUSAAKSARA		https://doi.org/10.48550/arXiv.2502.18148 (2025)	The DREAMSEA dataset is used to study Indonesian language materials, particularly focusing on digitized collections and linguistic resources. Researchers employ this dataset to address the preservation and analysis of Indonesian language data, utilizing its digitized content to explore specific research questions related to linguistic preservation and analysis.	https://huggingface.co/datasets/NusaAksara/NusaAksara		1	
5746	Indonesian Corpus	cited_context | citing_context	MinangNLP MT	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://www.semanticscholar.org/paper/b50c7c60416e334440e18832a9ee9b6fddf636d2 (2020)	The MinangNLP MT dataset is used for machine translation tasks, specifically translating sentiment analysis datasets into the Minangkabau language. It consists of 5,000 sentences, divided into training, validation, and test sets. Research focuses on evaluating the quality and accuracy of these translations, employing methodologies that assess the effectiveness of machine translation models in handling nuanced sentiment data.; The MinangNLP MT dataset is used for machine translation tasks, specifically translating sentiment analysis datasets into the Minangkabau language. It consists of 5,000 sentences divided into training, validation, and test sets. Researchers focus on evaluating the quality and accuracy of these translations, employing the dataset to enhance and assess machine translation models in the context of sentiment analysis.	https://huggingface.co/datasets/SEACrowd/minangnlp_mt		1	
5747	Indonesian Corpus	citing_context	mLongRR	https://doi.org/10.18653/v1/2024.mrl-1.18 (2024)		The mLongRR dataset is used to collect and analyze BBC news articles in multiple languages, including Indonesian, for cross-lingual research. It supports multilingual long-form reading comprehension tasks by providing a diverse set of articles, enabling researchers to evaluate models' ability to understand and process lengthy texts across different languages.	https://github.com/PortNLP/mLongRR		1	
5749	Indonesian Corpus	citing_context	M-RewardBench	https://www.semanticscholar.org/paper/3f9531505ff36d7ce363318d7312296f62bdb090 (2025)	https://doi.org/10.48550/arXiv.2410.15522 (2024)	The M-RewardBench dataset is used to evaluate reward models in multilingual settings, particularly focusing on the impact of translated content and cultural mismatches. It is employed in instruction-tuned large language models using reinforcement learning from human feedback to improve model performance. The dataset highlights issues such as degradation of model performance due to translation artifacts and challenges in maintaining naturalness and contextual relevance in prompts, especially in Indonesian. It is also used to evaluate cross-lingual natural language inference models, emphasizing the importance of addressing cultural and linguistic nuances in multilingual AI systems.	https://huggingface.co/datasets/CohereLabsCommunity/multilingual-reward-bench		1	
5750	Indonesian Corpus	citing_context	MSVD-Indonesian	https://doi.org/10.48550/arXiv.2306.11341 (2023)	https://doi.org/10.1109/CVPR.2017.127 (2016)	The MSVD-Indonesian dataset is used to fine-tune CLIP and SCD models pretrained on English data, specifically to enhance their performance in Indonesian language visual captioning. This involves adapting these models to generate more accurate and contextually relevant captions for images in the Indonesian language, leveraging the dataset's focus on visual content annotated with Indonesian text.	https://github.com/willyfh/msvd-indonesian		1	
5754	Indonesian Corpus	cited_context | citing_context	NER-Grit	https://doi.org/10.48550/arXiv.2309.06085 (2023)	https://doi.org/10.1109/KSE.2018.8573337 (2018)	The NER-Grit dataset is used in various natural language processing tasks for Indonesian, including detecting hate speech and abusive language in tweets, evaluating question answering systems, performing named entity recognition, training natural language inference models, and conducting sentiment analysis. It supports multi-label classification, information-seeking queries, monolingual representation evaluation, and cross-lingual transferability assessments. The dataset's labeled data and focus on social media and lay-authored content enable robust model training and evaluation across these applications.; The NER-Grit dataset is used for various natural language processing tasks in Indonesian, including sentiment analysis, hate speech detection, named entity recognition, question answering, and natural language inference. It provides labeled data for training and evaluating models, focusing on nuanced content like abusive language and real-world text. The dataset supports multi-label classification and is part of the IndoNLU benchmark, enhancing monolingual representation and system performance.	https://huggingface.co/datasets/SEACrowd/nergrit		1	
5758	Indonesian Corpus	citing_context	NERSkill.Id	https://doi.org/10.1109/IC2IE63342.2024.10748079 (2024)	https://doi.org/10.1016/j.dib.2024.110192 (2024)	The NERSkill.Id dataset is used to train and evaluate skill entity recognition models in Indonesian, focusing on addressing linguistic differences. This dataset enhances NLP applications by improving the accuracy of recognizing skill-related entities, thereby supporting more effective and contextually appropriate natural language processing in Indonesian.	https://data.mendeley.com/datasets/5s8r9ndfvc/1		1	
5759	Indonesian Corpus	cited_context | citing_context	NER UI	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IWBIS.2017.8275098 (2017)	The NER UI dataset is used for named entity recognition in Indonesian text, focusing on training and evaluating models to identify entities in open-domain information extraction. This dataset enables researchers to develop and test algorithms that can accurately recognize and classify named entities, enhancing the performance of natural language processing systems in the Indonesian language.; The NER UI dataset is used to train and evaluate named entity recognition models specifically for Indonesian text. It focuses on identifying entities in open-domain information extraction, enabling researchers to improve the accuracy of entity recognition in diverse and unstructured Indonesian content. This dataset supports the development of more effective natural language processing tools for the Indonesian language.	https://huggingface.co/datasets/SEACrowd/indolem_nerui		1	
5669	Indonesian Corpus	cited_context	Indonesian multi-label hate speech and abusive dataset	https://www.semanticscholar.org/paper/1ec9cab33a92a80e22ccb49bb1655a9a2342658d (2020)	https://doi.org/10.18653/v1/W19-3506 (2019)	The Indonesian multi-label hate speech and abusive dataset is used to detect and classify multi-label hate speech and abusive language in Indonesian Twitter and other social media platforms. Machine learning models are employed to address the complexity of overlapping labels, classify various types of offensive content, and analyze linguistic patterns. Research focuses on developing classification and annotation methodologies to improve the detection and understanding of hate speech and abusive language in Indonesian online contexts.	https://github.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection		1	
5605	Indonesian Corpus	cited_context	KaWAT		https://www.semanticscholar.org/paper/330d74432b0f5171bdd900ea9093c6864a023b2a (2019)	The Google Word Analogy dataset is used to construct KaWAT, an Indonesian analogy task dataset, by providing a structured framework and examples for morphological and semantic relation tasks. This enables researchers to develop and evaluate models that can handle word analogies in the Indonesian language, enhancing natural language processing capabilities.	https://huggingface.co/datasets/SEACrowd/kawat		1	
5768	Indonesian Corpus	citing_context	NusaCrowd	https://doi.org/10.18653/v1/2024.emnlp-main.1085 (2024), https://www.semanticscholar.org/paper/3f9531505ff36d7ce363318d7312296f62bdb090 (2025)	https://doi.org/10.18653/v1/2020.aacl-main.60 (2020)	The NusaCrowd dataset is used to support various NLP tasks and research initiatives for Indonesian and local languages. It provides open-source resources for text summarization, leveraging large-scale data to enhance model performance. Additionally, it is utilized to compile diverse datasets for multi-task and instruction tuning, specifically aimed at improving NLP models on under-resourced languages. This dataset enables researchers to develop more effective and contextually relevant NLP models for Indonesian and similar languages.	https://github.com/IndoNLP/nusa-crowd		1	
5769	Indonesian Corpus	citing_context	NusaNLU	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.48550/arXiv.2205.06266 (2022)	The NusaNLU dataset is used to benchmark zero-shot techniques across 26 Indonesian and regional language datasets. It covers tasks like emotion classification, sentiment analysis, hate speech detection, and natural language inference. Researchers employ this dataset to evaluate model performance without task-specific training, addressing the effectiveness of zero-shot learning in diverse linguistic contexts.	https://github.com/IndoNLP/nusa-crowd/blob/edf9f19c28dbbcfb7c9afbd6f5d726f69d1ee059/README.md?plain=1#L55		1	
5770	Indonesian Corpus	citing_context	NusaWrites	https://doi.org/10.1109/ACCESS.2024.3416955 (2024)	https://doi.org/10.48550/arXiv.2309.06085 (2023)	The NusaWrites dataset is used to evaluate zero-shot performance on natural language understanding tasks, particularly for Indonesian and other regional languages. It benchmarks large language models on a holistic evaluation suite, focusing on Southeast Asian linguistic and cultural aspects. This dataset enables researchers to assess model capabilities without fine-tuning, providing insights into cross-lingual and cultural generalization.	https://github.com/IndoNLP/nusa-writes		1	
5780	Indonesian Corpus	cited_context	OPUS Ja-Id	https://www.semanticscholar.org/paper/7e036ef1f4c1d0140efb3176a8851c9ad599141f (2019)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The OPUS Ja-Id dataset is used to create a parallel corpus for Japanese-Indonesian translation, comprising 2.9 million sentences from diverse sources such as movie subtitles, localization files, Quran translations, and Tatoeba sentences. It supports translation and linguistic analysis, enhancing translation quality and facilitating multilingual research in Asian language pairs. The dataset is employed in training and evaluating various NLP tasks, providing essential data for improving cross-lingual understanding and performance.	https://opus.nlpl.eu/legacy/		1	
5784	Indonesian Corpus	citing_context	PANL BPPT	https://doi.org/10.1145/3592854 (2023)	https://doi.org/10.3115/1690299.1690312 (2009)	The PANL BPPT dataset is primarily used for creating and enhancing multilingual corpora, particularly for news and blog content translation. It supports the development of parallel text corpora for Indonesian-English translation, aiding in machine translation, cross-lingual information retrieval, and syntactic analysis. The dataset facilitates the construction of treebanks and bilingual corpora, focusing on current events, media, and development-related content, thereby enabling more accurate and contextually relevant translations and linguistic analyses.	https://huggingface.co/datasets/SEACrowd/id_panl_bppt		1	
5714	Indonesian Corpus	cited_context	JATI	https://www.semanticscholar.org/paper/68c1f36518e1e4f99197f894adea2284f5b7aca1 (2019)	https://doi.org/10.18653/v1/W15-3302 (2015)	The JATI dataset, built using the Indonesian Resource Grammar (INDRA), is primarily used as a reference for constructing other treebanks, with a focus on syntactic structures in Indonesian. It supports research in computational grammar, enabling the development and validation of linguistic resources and tools for Indonesian.	https://github.com/davidmoeljadi/INDRA/tree/master/tsdb/gold/jati_edited		1	
5787	Indonesian Corpus	cited_context | citing_context	ParaCotta	https://doi.org/10.1109/ICoICT58202.2023.10262642 (2023), https://doi.org/10.48550/arXiv.2203.13357 (2022)	https://doi.org/10.48550/arXiv.2205.04651 (2022)	The ParaCotta dataset is primarily used for paraphrase identification and generation tasks in Indonesian, providing a substantial amount of data. It is employed to generate paraphrase text pairs, analyzing their lexical diversity in multilingual contexts. Additionally, it is used to create synthetic multilingual paraphrases, focusing on diverse translation sample pairs to enhance cross-lingual understanding and paraphrasing.; The Para-Cotta dataset is used to study paraphrasing in multilingual contexts, particularly focusing on the generation of synthetic corpora and enhancing diversity in translation sample pairs. Researchers employ this dataset to develop and evaluate methods for creating varied and high-quality paraphrases, which are essential for improving machine translation systems and natural language processing tasks.	https://huggingface.co/datasets/SEACrowd/paracotta_id		1	
5646	Indonesian Corpus	cited_context	Indonesian and English dataset		https://www.semanticscholar.org/paper/a70fd36953d654cd585046c43815fa9ce5187172 (2020)	The Indonesian and English dataset is used to pretrain Transformer models for multilingual tasks, specifically focusing on enhancing language understanding and generation capabilities in Indonesian. This dataset enables researchers to develop models that can effectively process and generate text in both languages, contributing to advancements in multilingual natural language processing.	https://github.com/gunnxx/indonesian-mt-data		1	
5793	Indonesian Corpus	citing_context	Protected Health Information Removal	https://doi.org/10.22266/ijies2020.0630.22 (2020)	https://doi.org/10.1109/ICTS.2019.8850995 (2019)	The 'Protected Health Information Removal' dataset is used to anonymize Indonesian text by identifying and removing sensitive health information through named entity recognition. This methodology focuses on accurately detecting and anonymizing protected health information, enabling researchers to handle and analyze medical data while maintaining patient privacy. The dataset's specific focus on Indonesian language text makes it valuable for research in healthcare data privacy and compliance in Indonesian contexts.	https://www.i2b2.org/NLP/DataSets/Main.php		1	
5800	Indonesian Corpus	cited_context | citing_context	ROOTS	https://doi.org/10.48550/arXiv.2402.08638 (2024)	https://doi.org/10.1007/978-3-031-23793-5_34 (2018)	The ROOTS split dataset is used to collect Indonesian sentences from Wikipedia texts, primarily for semi-supervised textual entailment and summarization tasks. It enhances model performance with limited labeled data and contributes to the development of Indonesian NLP resources. The dataset's focus on Wikipedia texts provides a rich source of natural language content, enabling researchers to improve and expand NLP models for Indonesian.; The ROOTS split dataset is used to collect Indonesian sentences from Wikipedia texts, primarily for semi-supervised textual entailment and summarization tasks. It addresses gaps in monolingual semantic textual similarity, enhancing cross-lingual evaluation and model training. This dataset enables researchers to develop and evaluate models in these specific NLP tasks, leveraging its curated content from Wikipedia.	https://huggingface.co/datasets/bigscience-data/roots_id_indonesian_news_corpus		1	
5803	Indonesian Corpus	citing_context	SeaBench	https://doi.org/10.48550/arXiv.2407.19672 (2024)	https://doi.org/10.48550/arXiv.2502.06298 (2025)	The SeaBench dataset is used to benchmark and evaluate language models' performance on multi-turn human instructions in Southeast Asian languages, including Indonesian, Vietnamese, and Thai. It addresses the lack of publicly available datasets for this purpose, focusing on the models' ability to follow complex, multi-turn instructions. This dataset enables researchers to assess and improve the multilingual capabilities of language models in a region-specific context.	https://github.com/DAMO-NLP-SG/SeaBench		1	
5806	Indonesian Corpus	cited_context | citing_context	Singgalang	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/ICACSIS.2017.8355036 (2017)	The Singgalang dataset is used to train and evaluate named entity recognition (NER) models for the Indonesian language. It leverages Wikipedia articles and DBpedia entity types to enhance model performance. This dataset specifically supports research in improving NER accuracy, enabling more effective identification and classification of entities in Indonesian text.; The Singgalang dataset is used to train and evaluate Indonesian Named Entity Recognition (NER) models. It leverages Wikipedia articles and DBpedia for entity type references, enabling researchers to improve the accuracy of NER systems in identifying and classifying named entities in Indonesian text. This dataset supports the development of more effective natural language processing tools for the Indonesian language.	http://sealang.net/indonesia/corpus.htm		1	
5807	Indonesian Corpus	citing_context	slang word dataset	https://doi.org/10.1109/IC2IE60547.2023.10331336 (2023)		The slang word dataset is used to map Indonesian slang words to their formal equivalents, focusing on 1212 word pairs. This mapping helps researchers analyze informal language usage, providing insights into the linguistic transformations between slang and formal language. The dataset enables detailed analysis of how slang terms are integrated into everyday communication, supporting research on language evolution and sociolinguistic patterns.	https://github.com/louisowen6/NLP_bahasa_resources		1	
5811	Indonesian Corpus	citing_context	SPECIL	https://doi.org/10.1109/ACCESS.2024.3422318 (2024)	https://doi.org/10.1109/ACCESS.2023.3307712 (2023)	The SPECIL dataset is used to analyze spell errors in the Indonesian language by examining 21,500 sentences for each type of error. Researchers employ this dataset to understand error patterns and improve correction methods, focusing on specific types of spelling mistakes to enhance accuracy and effectiveness in language processing.	https://www.kaggle.com/datasets/yanfiyanfi/specil-spell-error-corpus-for-indonesian-language		1	
5814	Indonesian Corpus	cited_context	idner-news-2k	https://doi.org/10.1186/s40537-024-00987-6 (2024)	https://www.semanticscholar.org/paper/574453271bf9dbce7df005e9e1c2e0bb77eb1c6d (2018)	The Standardized Dataset on Indonesian Named Entity Recognition is used to develop and evaluate named entity recognition models specifically for the Indonesian language. It focuses on standardizing annotations to improve model performance. This dataset enables researchers to address the challenge of consistent annotation standards, enhancing the accuracy and reliability of NER models in Indonesian language processing tasks.	https://github.com/khairunnisaor/idner-news-2k		1	
5816	Indonesian Corpus	citing_context	STIF parallel dataset	https://doi.org/10.1109/IC2IE60547.2023.10331336 (2023)	https://doi.org/10.18653/v1/N19-1014 (2019)	The STIF parallel dataset is used to fine-tune pre-trained models for Indonesian informal-to-formal style transfer. This involves enhancing grammatical accuracy and style consistency in the transformed text. The dataset's parallel nature, featuring informal and formal versions of sentences, enables researchers to train models that effectively convert informal language into more formal, grammatically correct forms.	https://github.com/haryoa/stif-indonesia		1	
5819	Indonesian Corpus	cited_context | citing_context	Sundanese-Indonesian Parallel Corpus	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.52549/ijeei.v10i1.3565 (2022)	The Sundanese-Indonesian Parallel Corpus is utilized for machine translation research, specifically to improve translation quality between Sundanese and Indonesian. Researchers employ this dataset to address linguistic variations, enhancing the accuracy and fluency of translations. The parallel nature of the corpus, containing aligned texts in both languages, is crucial for training and evaluating machine translation models.	https://dataverse.telkomuniversity.ac.id/file.xhtml?persistentId=doi:10.34820/FK2/HDYWXW/IVP3G5		1	
5820	Indonesian Corpus	cited_context | citing_context	Sundanese Twitter Dataset for Emotion Classification	https://doi.org/10.48550/arXiv.2205.15960 (2022)	https://doi.org/10.1109/CENIM51130.2020.9297929 (2020)	The Sundanese Twitter Dataset for Emotion Classification is used to analyze emotional expressions in tweets, specifically focusing on social media content in the Sundanese language. Researchers employ this dataset for emotion classification tasks, utilizing it to understand and categorize emotional states expressed in Sundanese tweets. This dataset enables detailed analysis of emotional content, contributing to the development of more nuanced natural language processing models for social media data.; The Sundanese Twitter Dataset for Emotion Classification is used to analyze emotional expressions in tweets, specifically focusing on social media content in the Sundanese language. Researchers employ this dataset for emotion classification tasks, utilizing it to develop and evaluate models that can accurately identify emotions expressed in Sundanese tweets. This dataset enables the exploration of linguistic and cultural nuances in emotional expression on social media platforms.	https://www.kaggle.com/datasets/rabieelkharoua/sundanese-twitter-dataset		1	
5826	Indonesian Corpus	cited_context | citing_context	TED En-Id	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.18653/v1/N18-2084 (2018)	The TED En-Id dataset is used to train and evaluate neural machine translation models, specifically for translating between Indonesian and English. It employs parallel sentences from TED talk transcripts to enhance translation quality. This dataset facilitates research focused on improving the accuracy and fluency of Indonesian-English translations through neural network methodologies.; The TED En-Id dataset is used to train and evaluate neural machine translation models, specifically for translating Indonesian to English. It employs parallel sentences from TED talk transcripts, enabling researchers to focus on improving translation accuracy and performance in this language pair. The dataset's parallel sentence structure facilitates the development and testing of machine translation systems.	https://huggingface.co/datasets/SEACrowd/ted_en_id		1	
5828	Indonesian Corpus	cited_context | citing_context	TITML-IDN	https://doi.org/10.48550/arXiv.2401.06832 (2024), https://doi.org/10.1109/ACCESS.2020.3027619 (2020), https://www.semanticscholar.org/paper/5f25f0dd39b88f3a17b4035a16d248c7ff9185ae (2019)	https://www.semanticscholar.org/paper/da19f6f033257a7831f53f238e37b4e5768e9f43 (2006)	The TITML-IDN dataset is primarily used for training and evaluating speech recognition and text-to-speech models for Indonesian, Javanese, and Sundanese languages. It enhances the XLS-R model's accuracy, robustness, and generalization across diverse speakers and accents, particularly in controlled and natural contexts. The dataset includes community-contributed audio and has an average recording time of 43 minutes per speaker for Indonesian, 10 minutes for Javanese, and 7 minutes for Sundanese, making it valuable for developing high-quality speech processing systems.; The TITML-IDN dataset is used for speech-related research in Indonesian, focusing on phonetic, phonological, and acoustic characteristics. It includes a corpus of twenty speakers (eleven male, nine female) and is employed to analyze speech patterns, high-frequency sounds, and selected sentences. This dataset enables detailed linguistic and acoustic studies, particularly in a multilingual context.	https://huggingface.co/datasets/holylovenia/TITML-IDN		1	
5829	Indonesian Corpus	cited_context | citing_context	Tokyo University Frog Storytelling	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://www.semanticscholar.org/paper/278bdf4ed974541265b0208eac9f421cd5486000 (2013)	The Tokyo University Frog Storytelling dataset is used to analyze Indonesian possessive verbal predicates through storytelling surveys. Researchers focus on identifying and examining statistical patterns in language use, employing surveys to gather data. This dataset enables detailed linguistic analysis, providing insights into the structural and functional aspects of possessive verbs in Indonesian.; The Tokyo University Frog Storytelling dataset is used to analyze Indonesian possessive verbal predicates through storytelling surveys. Researchers focus on identifying and examining statistical patterns in language use, employing a survey methodology to gather data. This dataset enables detailed linguistic analysis, specifically addressing how possessive verbs are used in storytelling contexts, providing insights into Indonesian language structure and usage.	https://github.com/davidmoeljadi/corpus-frog-storytelling		1	
5830	Indonesian Corpus	citing_context	translated Hadiths in Bahasa Indonesia	https://doi.org/10.1109/ICITACEE62763.2024.10762762 (2024)	https://doi.org/10.47065/josyc.v4i4.3905 (2023)	The 'translated Hadiths in Bahasa Indonesia' dataset is used to import Hadith translations into a MySQL database for further processing and analysis. This focuses on enhancing the accessibility and usability of the translated content, enabling researchers to explore and utilize the dataset in a structured format. The dataset's primary application is in improving the digital availability and user experience of Hadith translations in Bahasa Indonesia.	https://github.com/irsyadulibad/hadits-database		1	
5837	Indonesian Corpus	citing_context	Twitter comments	https://doi.org/10.1109/IoTaIS56727.2022.9975975 (2022)	https://doi.org/10.1109/iccsai53272.2021.9609746 (2021)	The Twitter comments dataset is used to train and validate models for sentiment analysis and understanding Indonesian stock trader slang. It focuses on document-level sentiment and social media comments, labeled by stock experts for domain-specific accuracy. This dataset enables research in sentiment analysis and financial language processing, enhancing model performance in these areas.	https://github.com/ridife/dataset-idsa		1	
5843	Indonesian Corpus	citing_context	UD-Indo-GSD	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	https://www.semanticscholar.org/paper/e95ad36302f92f45abe169fbc5185e55407bcc34 (2013)	The UD-Indo-GSD dataset is primarily used for dependency parsing in Indonesian, focusing on both manually translated and originally authored texts. It contains 5,593 annotated sentences, enabling researchers to study syntactic structures and benchmark parsing models. The dataset's pre-defined test set and corrected version facilitate the evaluation of parsing models, including comparisons with methods like I NDO BERT, using dependency annotation metrics.	https://universaldependencies.org/treebanks/id_gsd/index.html		1	
5846	Indonesian Corpus	cited_context | citing_context	UD-Indo-PUD	https://doi.org/10.1109/IoTaIS56727.2022.9975975 (2022), https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020), https://www.semanticscholar.org/paper/e95ad36302f92f45abe169fbc5185e55407bcc34 (2013)	The UD-Indo-PUD dataset is used to evaluate the performance of IndoBERT, a model designed for Indonesian Natural Language Processing (NLP) tasks. It is specifically employed to compare IndoBERT's effectiveness against other models, focusing on tasks relevant to the Indonesian language. This dataset enables researchers to benchmark and assess the relative strengths and weaknesses of different NLP models in the Indonesian context.; The UD-Indo-PUD dataset is primarily used for dependency parsing in Indonesian, focusing on syntactic structures and grammatical relations. It leverages a pre-defined test set to benchmark models, including I NDO BERT, against previous work. The dataset is also used to analyze manually translated and authored Indonesian texts, with specific attention to dependency annotation metrics and universal dependency annotations in both large and small subsets of sentences.	https://universaldependencies.org/treebanks/id_pud/index.html		1	
5849	Indonesian Corpus	cited_context | citing_context	UI-CTB	https://doi.org/10.1109/IALP48816.2019.9037723 (2019)	https://doi.org/10.3115/v1/P15-1113 (2015)	The UI-CTB dataset is used to build an Indonesian constituency parser model, employing a shift-reduce neural constituent parser. Research focuses on improving parsing accuracy and evaluating model performance. This dataset enables researchers to develop and test parsing algorithms specifically tailored for the Indonesian language, contributing to advancements in natural language processing for Indonesian.; The UI-CTB dataset is used to build an Indonesian constituency parser model, employing a shift-reduce neural constituent parser. This dataset focuses on parsing Indonesian sentences, enabling researchers to develop and evaluate parsing models specifically tailored for the Indonesian language. The dataset's annotated sentence structures facilitate the training and testing of these models, enhancing their accuracy and performance in linguistic analysis tasks.	https://universaldependencies.org/treebanks/id_csui/index.html		1	
5850	Indonesian Corpus	citing_context	UKARA dataset	https://doi.org/10.20532/cit.2022.1005478 (2023)	https://www.semanticscholar.org/paper/3e795f8d94f552f2abeb09bcadec480f9e8a4586 (2020)	The UKARA dataset is used to evaluate various methods, including single, ensemble, and deep learning (LSTM) approaches, for automatic short-answer scoring in Bahasa Indonesia. Research focuses on assessing model performance and accuracy, enabling the development and refinement of automated scoring systems for educational assessments in the Indonesian language.	https://github.com/ilhamfp/ukara-1.0-challenge		1	
5860	Indonesian Corpus	cited_context	WordNet Bahasa	https://www.semanticscholar.org/paper/41713cdd3af12f4fe8e50fee0d71ffb1a904a49e (2018)	https://www.semanticscholar.org/paper/288ef622fe53317f92ddb66677a7a0b691719c38 (2011)	The WordNet Bahasa dataset is used as a lexical database to support Indonesian language processing, enhancing semantic and lexical resources for research studies. It provides a structured vocabulary that aids in the development and improvement of natural language processing systems, particularly in the Indonesian context. This dataset enables researchers to build more robust and contextually accurate language models and tools.	https://wn-msa.sourceforge.net/index.eng.html		1	
5862	Indonesian Corpus	citing_context	WP2	https://doi.org/10.1007/978-3-031-24337-0_29 (2019)	https://www.semanticscholar.org/paper/04ca48e573c0800fc572f2af1d475dd2645e840a (2008)	The WP2 dataset is primarily used for training and evaluating named entity recognition (NER) models, particularly in the Indonesian language. It is utilized to train character-embeddings and pre-trained monolingual word-embeddings, and to measure vocabulary overlap with English datasets. WP2 shows better performance on MDEE and +Gazz datasets compared to CoNLL, and is also used to train a BiLSTM-CRF model for cross-lingual NER transfer.	https://github.com/dice-group/FOX/tree/master/input/Wikiner		1	
5863	Indonesian Corpus	cited_context | citing_context	WReTE	https://doi.org/10.1109/IWAIIP58158.2023.10462854 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021), https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1007/978-3-031-23793-5_34 (2018)	The WReTE dataset is primarily used to evaluate and train textual entailment and natural language inference models in Indonesian. It consists of sentence pairs derived from Indonesian Wikipedia and other sources, labeled for entailment and non-entailment. Researchers use it to assess model performance on entailment, contradiction, and neutral relations, often employing semi-supervised methods to improve entailment recognition. The dataset supports binary classification tasks and is used to construct and evaluate models across various textual entailment scenarios.; The WReTe dataset is used to construct an entailment dataset from Indonesian Wikipedia revision history, comprising 450 sentence pairs. It is specifically employed to evaluate textual entailment models, focusing on the accuracy and performance of these models in understanding and processing Indonesian language text. The dataset's unique feature is its derivation from revision history, providing a rich source of sentence pairs for this evaluation.	https://huggingface.co/datasets/SEACrowd/wrete		1	
5870	Indonesian Corpus	citing_context	XStoryCloze	https://doi.org/10.48550/arXiv.2311.01012 (2023)	https://www.semanticscholar.org/paper/1403e6b9adf7712c35ae56327d52fe54603b87e1 (2021)	The XStoryCloze dataset is used to study causal and temporal relations in multilingual stories, specifically across 10 languages including Indonesian. It enhances few-shot learning with multilingual language models by providing a diverse set of narrative contexts. This dataset enables researchers to evaluate and improve the ability of language models to understand and generate coherent stories in multiple languages.	https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/XStoryCloze.md		1	
5786	Indonesian Corpus	cited_context	PAN Localization project output	https://doi.org/10.1007/978-3-642-23138-4_8 (2011)		The 'PAN Localization project output' dataset is used to collect Indonesian language texts for research studies. While the specific research questions and methodologies are not detailed, the dataset's primary role is to provide a source of Indonesian language content, enabling studies that require authentic linguistic data. This supports research in areas such as language analysis, cultural studies, and content localization.	https://huggingface.co/datasets/SEACrowd/id_panl_bppt		2	
5555	Indonesian Corpus	citing_context	CIL dataset	https://doi.org/10.1109/IALP51396.2020.9310508 (2020)	https://doi.org/10.1109/IALP.2018.8629151 (2018)	The CIL dataset is used to study colloquial Indonesian language, focusing on lexical variations and usage patterns in informal contexts such as Instagram comments. Researchers employ the dataset to build a lexicon of colloquial terms and to normalize colloquial phrases using Statistical Machine Translation, aiming to improve translation accuracy and consistency.	https://github.com/nasalsabila/kamus-alay		2	
5708	Indonesian Corpus	citing_context	InSet	https://doi.org/10.1109/ICAIIT.2019.8834531 (2019)	https://doi.org/10.1109/IALP.2017.8300625 (2017)	The InSet dataset is used as a benchmark lexicon for evaluating and comparing the performance of word lists in Indonesian sentiment analysis, particularly in microblogs. It facilitates the assessment of different lexicons' effectiveness in sentiment classification tasks, enabling researchers to identify the most accurate tools for analyzing sentiment in Indonesian social media content.	https://github.com/fajri91/InSet		2	
5720	Indonesian Corpus	cited_context | citing_context	Kamus Besar Bahasa Indonesia (KBBI)	https://doi.org/10.1109/ICoDSA50139.2020.9212943 (2020), https://doi.org/10.1007/s10772-019-09619-4 (2019)	https://www.semanticscholar.org/paper/c04b9d1996f0178207095d8f47a5c529ee620bdb (2019)	The Kamus Besar Bahasa Indonesia (KBBI) is used to evaluate the word error rate (WER) of a fuzzy nearest neighbor method with stemming, focusing on 50,000 words from the dictionary. This dataset enables researchers to assess the accuracy and effectiveness of natural language processing techniques specifically tailored for the Indonesian language.; The Kamus Besar Bahasa Indonesia (KBBI) dataset is used to train and evaluate Indonesian language models, specifically focusing on grapheme-to-phoneme conversion accuracy. Researchers employ 5-fold cross-validation to assess model performance, highlighting challenges with the grapheme ⟨e⟩, especially in prefixes. This dataset enables detailed analysis and improvement of phonetic conversion in Indonesian language processing.	https://kbbi.web.id/		2	
5692	Indonesian Corpus	cited_context | citing_context	Indonesian universal POS tag dataset	https://doi.org/10.1109/IWBIS.2017.8275098 (2017)	https://doi.org/10.3115/1073445.1073478 (2003)	The Indonesian universal POS tag dataset is used to train part-of-speech taggers, specifically by converting data from a dependency parsing dataset to improve tagging accuracy for Indonesian text. This enhances the precision of linguistic analysis tools, facilitating more accurate processing and understanding of Indonesian language data.; The Indonesian universal POS tag dataset is used to train part-of-speech taggers for the Indonesian language. Researchers convert this dataset from a dependency parsing format to align with universal POS tags, enhancing the accuracy and consistency of POS tagging. This conversion facilitates more reliable linguistic analysis and supports natural language processing tasks specific to Indonesian.	https://github.com/UniversalDependencies/UD_Indonesian-GSD		2	
5523	Indonesian Corpus	cited_context | citing_context	300-document chat summarization dataset	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	https://www.semanticscholar.org/paper/e0e68cedb8d1cd2643da97fe223b096ad851aa4c (2016)	The 300-document chat summarization dataset is used to develop and evaluate chat summarization models, specifically for generating concise and accurate summaries from Indonesian chat conversations and documents. Researchers employ both abstractive and extractive summarization techniques to train and test these models, focusing on improving summary quality and coherence. This dataset enables the assessment of model performance in handling the nuances of the Indonesian language, enhancing the accuracy and relevance of generated summaries.; The 300-document chat summarization dataset is used to train and test both abstractive and extractive summarization models, focusing on generating accurate and concise summaries from Indonesian chat conversations. This dataset enables researchers to develop and evaluate models that can effectively summarize chat data, enhancing the accuracy and relevance of the summaries produced.	https://github.com/kata-ai/indosum		2	
5551	Indonesian Corpus	cited_context | citing_context	canonical splits provided by Kurniawan and Louvan (2018)	https://doi.org/10.18653/V1/2020.COLING-MAIN.66 (2020)	https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008 (2004)	The canonical splits provided by Kurniawan and Louvan (2018) are used to evaluate summarization models in Indonesian, specifically focusing on discourse coherence. Researchers employ ROUGE metrics (R1, R2, RL) to measure the performance of these models. The dataset enables consistent evaluation and comparison of summarization techniques by providing standardized training, validation, and test sets.; The canonical splits provided by Kurniawan and Louvan (2018) are used to evaluate summarization models in Indonesian. These splits provide standard training, validation, and test sets, enabling consistent assessment of summary quality using ROUGE metrics. This dataset facilitates the comparison of different summarization techniques and helps researchers address the specific challenges of generating high-quality summaries in the Indonesian language.	https://github.com/kata-ai/indosum		2	
5764	Indonesian Corpus	citing_context	news translation dataset	https://doi.org/10.18653/v1/2021.emnlp-main.699 (2021)	https://doi.org/10.18653/v1/N18-2084 (2018)	The news translation dataset is primarily used for training and evaluating neural machine translation models, with a focus on cross-lingual performance, particularly in the news and TED talk domains. It provides parallel texts and multilingual content, enabling researchers to assess translation quality, localization challenges, and model performance across diverse topics and domains, including news articles and cultural content. The dataset also supports benchmarking and pretraining of natural language generation models in Indonesian and related languages.	https://github.com/gunnxx/indonesian-mt-data		2	
5782	Indonesian Corpus	citing_context	pairs of formal-informal Indonesian words	https://doi.org/10.18653/v1/2021.findings-acl.280 (2021)	https://doi.org/10.1109/IALP.2018.8629151 (2018)	The dataset of pairs of formal-informal Indonesian words is used to study lexical variation in Indonesian, specifically focusing on the differences between formal and informal word pairs. Research employs this dataset to analyze and understand the patterns and usage of these word pairs without delving into the mechanisms of word formation. This enables researchers to explore how formal and informal language variants are distributed and utilized in various contexts, providing insights into the linguistic dynamics of Indonesian.	https://github.com/nasalsabila/kamus-alay		2	
5550	Indonesian Corpus	cited_context	dataset built by Sihui Fu 2018	https://doi.org/10.26418/jlk.v2i2.20 (2019)	https://www.semanticscholar.org/paper/da9400de345f5102e054911183325431a8baef65	The dataset built by Sihui Fu in 2018 is used to develop and evaluate part-of-speech tagging models for Indonesian. It focuses on corpus construction and assessing model performance, enabling researchers to improve the accuracy and reliability of part-of-speech tagging in Indonesian language processing tasks.			2	
5630	Indonesian Corpus	cited_context | citing_context	Id-tagged-corpus-CSUI	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IALP.2014.6973519 (2014)	The Id-tagged-corpus-CSUI dataset is used for part-of-speech (POS) tagging in Indonesian, featuring 10,000 sentences annotated with 23 POS tag classes. This dataset, sourced from the PAN Localization Project, is employed to train and evaluate POS tagging models, enhancing the accuracy of linguistic analysis in Indonesian text. It supports research focused on improving natural language processing techniques for the Indonesian language.; The Id-tagged-corpus-CSUI dataset is used for part-of-speech (POS) tagging in Indonesian, featuring 10,000 sentences annotated with 23 POS tag classes sourced from the PAN Localization Project. It is employed to train and evaluate POS tagging models, enabling researchers to improve the accuracy of linguistic analysis tools for the Indonesian language.	https://universaldependencies.org/treebanks/id_csui/index.html		2	
5726	Indonesian Corpus	cited_context | citing_context	Kethu	https://doi.org/10.48550/arXiv.2212.09648 (2022)	https://doi.org/10.1109/IALP48816.2019.9037723 (2019)	The Kethu dataset serves as the source corpus for converting the Universitas Indonesia Constituency Treebank (UI-CTB) and other Indonesian Constituency Treebanks into the Penn Treebank format. It focuses on adjusting bracketing and part-of-speech (POS) tags for compound words, enhancing syntactic structure and annotation quality. This conversion facilitates linguistic analysis and supports research in Indonesian syntax and treebank standardization.; The Kethu dataset is used to convert the UI-CTB treebank format into the Penn Treebank format, focusing on adjusting bracketing and POS tags for compound words. It serves as a constituency treebank derived from the UI-CTB, enabling researchers to enhance syntactic structures and annotation quality in Indonesian language processing tasks.	https://universaldependencies.org/treebanks/id_csui/index.html		2	
5745	Indonesian Corpus	citing_context	Microsoft Video Description (MSVD)	https://doi.org/10.48550/arXiv.2306.11341 (2023)	https://www.semanticscholar.org/paper/72729882f8fa3d9084eaece513f6bf9630be5901 (2011)	The Microsoft Video Description (MSVD) dataset is primarily used as a source for translating English sentences to Indonesian, facilitating the creation of a new Indonesian video-text dataset. This process supports video description tasks, enabling researchers to develop and evaluate models for generating Indonesian-language video descriptions. The dataset's English captions serve as the foundation for translation, enhancing cross-lingual video understanding and description capabilities.	https://github.com/willyfh/msvd-indonesian		2	
5700	Indonesian Corpus	cited_context | citing_context	I NDO NLI	https://doi.org/10.48550/arXiv.2309.06085 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021)	https://doi.org/10.18653/v1/2021.emnlp-main.821 (2021)	The I NDO NLI dataset is used to evaluate and compare the performance of human and fine-tuned models on natural language inference tasks in Indonesian. It serves as a benchmark for assessing model accuracy and human performance, particularly using lay-authored data. This dataset facilitates research on cross-lingual transferability and model evaluation, enabling detailed comparisons on test sets.; The I NDO NLI dataset is used to train and evaluate natural language inference models for Indonesian, particularly focusing on lay-authored data to enhance real-world text performance. It is also employed to compare the performance of fine-tuned models and humans on natural language inference tasks, assessing model accuracy and the differences between human and machine capabilities.	https://huggingface.co/datasets/afaji/indonli		2	
5755	Indonesian Corpus	cited_context | citing_context	NERGRIT CORPUS	https://doi.org/10.1109/ICIC50835.2020.9288566 (2020)		The NERGRIT CORPUS is used for training and testing named entity recognition (NER) models, specifically tailored for the Indonesian language. It is modified into four types (standard, lowercase with punctuation, lowercase without punctuation, lowercase and clean) to enhance flexibility and robustness. This dataset supports the development and evaluation of NLP systems, focusing on improving the accuracy and performance of entity recognition in Indonesian text.; The NERGRIT CORPUS is used for training and testing named entity recognition (NER) models, specifically focusing on Indonesian language entities. It provides annotated text to evaluate model performance. The dataset has been modified into four types (standard, lowercase with punctuation, lowercase without punctuation, lowercase and clean) to enhance model robustness across different preprocessing conditions, addressing research questions related to improving NER accuracy and reliability in Indonesian text.	https://huggingface.co/datasets/SEACrowd/nergrit		2	
5647	Indonesian Corpus	cited_context	Indonesian colloquial dictionary	https://www.semanticscholar.org/paper/3e795f8d94f552f2abeb09bcadec480f9e8a4586 (2020)	https://doi.org/10.1109/IALP.2018.8629151 (2018)	The Indonesian colloquial dictionary dataset is used to correct typos and slang in responses, thereby enhancing the accuracy of data processing pipelines. This dataset is employed in natural language processing tasks where the correction of informal language and errors is crucial for improving the reliability of the processed data.	https://github.com/nasalsabila/kamus-alay		2	
5651	Indonesian Corpus	cited_context	Indonesian Dependency Treebank	https://www.semanticscholar.org/paper/68c1f36518e1e4f99197f894adea2284f5b7aca1 (2019)	https://doi.org/10.1109/ICSDA.2016.7918974 (2016)	The Indonesian Dependency Treebank is used for syntactic annotation, primarily focusing on dependency structures and part-of-speech tagging of approximately 20,000 sentences sampled from English Wikinews in 2014. It employs Penn Treebank bracketing guidelines and various NLP tools, enabling researchers to build and analyze syntactic structures specific to the Indonesian language. This dataset facilitates the development and evaluation of NLP models for Indonesian, enhancing understanding of its linguistic properties.	https://universaldependencies.org/treebanks/id_pud/index.html		2	
5685	Indonesian Corpus	cited_context	Indonesian treebank developed by the University of Indonesia	https://www.semanticscholar.org/paper/68c1f36518e1e4f99197f894adea2284f5b7aca1 (2019)	https://doi.org/10.1109/IALP.2014.6973519 (2014)	The Indonesian treebank developed by the University of Indonesia is primarily used to create and enhance part-of-speech (POS) tagged corpora for Indonesian, adhering to Penn Treebank guidelines. It serves as a training set for POS tagging models, addressing data scarcity in recent treebanks and improving model performance. The dataset provides a manually annotated resource for formal Indonesian, facilitating linguistic analysis, syntactic, and morphological studies.	https://universaldependencies.org/treebanks/id_csui/index.html		2	
5844	Indonesian Corpus	cited_context	UD Indonesian-GSD	https://www.semanticscholar.org/paper/06d77fa89c6cd8ebb94a6830fc9c7f1ace127a96 (2019)	https://doi.org/10.18653/v1/K17-3001 (2017)	The UD Indonesian-GSD dataset is primarily used to enhance and refine the gold standard dependency treebank for Indonesian, focusing on parsing sentences into Universal Dependencies. It consists of 5,593 sentences for training and evaluation, with additional sets for testing and validation. The dataset supports research on syntactic structures, tokenization of complex words, and cross-lingual dependency parsing, contributing to the improvement of linguistic annotations and the adequacy of training data for dependency parsing in Indonesian.	https://universaldependencies.org/treebanks/id_pud/index.html		2	
5845	Indonesian Corpus	cited_context | citing_context	UD_IndonesianPUD	https://doi.org/10.48550/arXiv.2212.09648 (2022), https://www.semanticscholar.org/paper/06d77fa89c6cd8ebb94a6830fc9c7f1ace127a96 (2019)	https://www.semanticscholar.org/paper/06d77fa89c6cd8ebb94a6830fc9c7f1ace127a96 (2019)	The UD_IndonesianPUD dataset is used to create a gold standard dependency treebank for Indonesian, focusing on syntactic structures and high-quality linguistic annotations. It serves as a benchmark for natural language processing tasks, particularly in evaluating annotation consistency and syntactic analysis. The dataset is part of a broader PUD treebank collection, enabling cross-linguistic research and comparisons.; The UD_IndonesianPUD dataset is used to create and evaluate dependency treebanks for Indonesian, focusing on syntactic structures and linguistic annotations. It consists of 5,593 sentences for training and evaluation, and an additional 1,000 sentences for testing and validation. This dataset supports natural language processing tasks, particularly in parsing Indonesian sentences into Universal Dependencies, and is also used to develop parallel treebanks for multilingual parsing, enhancing cross-lingual dependency parsing.	https://universaldependencies.org/treebanks/id_pud/index.html		2	
5847	Indonesian Corpus	citing_context	UD Javanese-CSUI	https://doi.org/10.11591/ijai.v13.i3.pp3498-3509 (2024)	https://doi.org/10.30564/fls.v6i5.6957 (2022)	The UD Javanese-CSUI dataset is primarily used for linguistic annotation and parsing tasks in Javanese, including tokenization, part-of-speech (POS) tagging, morphological feature tagging, and dependency parsing. It serves as a gold standard for these tasks, enabling researchers to improve NLP models and conduct linguistic analyses specific to the Javanese language. This dataset supports the development and evaluation of NLP tools tailored for Indonesian languages.	https://universaldependencies.org/treebanks/id_csui/index.html		2	
5848	Indonesian Corpus	cited_context | citing_context	UI Constituency Treebank	https://doi.org/10.1109/IALP48816.2019.9037723 (2019)	https://doi.org/10.1109/IALP.2014.6973519 (2014)	The UI Constituency Treebank is used in research to enhance and standardize part-of-speech (POS) tagging for Indonesian. It is employed to map and compare the Indonesian POS tagset to the Penn Treebank (PTB) tagset, addressing differences to improve annotation consistency. The dataset supports the development of a POS tagger corpus, building on existing Indonesian linguistic resources. This work focuses on refining the number and types of tags used, contributing to more accurate and consistent linguistic analysis.; The UI Constituency Treebank is used to develop and refine part-of-speech (POS) tagging for Indonesian, involving the creation and application of a 23-tag set. It serves as a corpus for building POS taggers, extending previous work by Dinakaramani et al. (2015). The dataset facilitates the mapping of Indonesian POS tags to the Penn Treebank (PTB) tagset, highlighting differences in tagset design and annotation practices. This enables detailed linguistic analysis and enhances cross-linguistic comparability.	https://universaldependencies.org/treebanks/id_csui/index.html		2	
5858	Indonesian Corpus	cited_context | citing_context	Wiki Revision Edits Textual Entailment dataset	https://doi.org/10.18653/v1/2020.aacl-main.85 (2020)	https://doi.org/10.1007/978-3-031-23793-5_34 (2018)	The Wiki Revision Edits Textual Entailment dataset is primarily used for Indonesian language processing tasks, including information retrieval, summarization, and named entity recognition. It is also utilized to evaluate textual entailment by assessing the logical relationships between sentence pairs derived from Wikipedia revision history. This dataset supports semi-supervised learning approaches and provides a foundational resource for various NLP applications in Indonesian.; The Wiki Revision Edits Textual Entailment dataset is primarily used for evaluating textual entailment in Indonesian Wikipedia data, focusing on the logical relationships between sentence pairs. It is also employed for named entity recognition tasks, identifying and classifying named entities in Indonesian text, and for extracting relevant text spans for information extraction. The dataset includes 450 sentence pairs derived from Wikipedia revision history, enabling semi-supervised textual entailment tasks and enhancing the accuracy of natural language processing models in the Indonesian language.	https://metatext.io/datasets/the-wiki-revision-edits-textual-entailment-(wrete)-(indonlu)		2	
6407	Javanese Corpus	cited_context	Javanese Speech Corpus	https://doi.org/10.1109/ACCESS.2020.3027619 (2020)		The Javanese Speech Corpus is primarily used for studying Javanese speech, with each speaker contributing an average of 10 minutes of audio. This dataset supports the development and testing of Javanese language models, enhances the analysis of Javanese phonetics and prosody, and is also utilized as part of a broader Indonesian speech corpus for training and evaluating speech models.	https://www.openslr.org/41/		2	
6410	Javanese Corpus	cited_context | citing_context	OpenSLR jv-ID	https://doi.org/10.1109/ACCESS.2020.3027619 (2020)	https://www.semanticscholar.org/paper/da19f6f033257a7831f53f238e37b4e5768e9f43 (2006)	The OpenSLR jv-ID dataset is employed as a Javanese speech corpus, featuring an average of 10 minutes of data per speaker. It is primarily used for training and evaluating speech recognition models, enabling researchers to develop and test algorithms specific to the Javanese language. The dataset's structured speaker data supports robust model performance assessment.; The OpenSLR jv-ID dataset is used to analyze Javanese speech, focusing on phonetics and language processing. Each recording averages 10 minutes per speaker, enabling detailed phonetic analysis and supporting the development of speech processing techniques for the Javanese language. This dataset facilitates research into the acoustic and linguistic properties of Javanese, enhancing understanding and technological applications in Javanese speech recognition and synthesis.	https://www.openslr.org/41/		1	
6414	Javanese Corpus	citing_context	SLR41	https://doi.org/10.48550/arXiv.2401.06832 (2024)	https://doi.org/10.21437/SLTU.2018-14 (2018)	The SLR41 dataset is primarily used for training and evaluating models in Javanese language processing, with a focus on speech recognition and synthesis tasks. It provides a diverse set of high-quality voice samples, enabling researchers to build and assess TTS voices for Javanese, emphasizing model performance, speech synthesis quality, and naturalness.	https://www.openslr.org/41/		1	
6400	Javanese Corpus	cited_context	handwritten Javanese character dataset	https://doi.org/10.22146/IJCCS.31144 (2018)	https://www.semanticscholar.org/paper/7f5081cc48368e0a528f8a27bb19b1d86d9cef42 (2016)	The handwritten Javanese character dataset is used for recognizing directional element features from handwritten Javanese characters. Researchers employ multi-class SVM for character recognition and feature extraction, focusing on improving the accuracy of identifying specific features within the characters. This dataset enables detailed analysis and development of algorithms tailored to the unique characteristics of Javanese script.	https://www.kaggle.com/datasets/arifrizqy/javanese-script		1	
6399	Javanese Corpus	citing_context	Google Crowdsourced Speech Corpora	https://doi.org/10.33480/pilar.v20i1.4633 (2024)	https://www.semanticscholar.org/paper/d3f1c77ea3a4e381cfef8213fa6861928396172a (2020)	The Google Crowdsourced Speech Corpora is used to compile speech audio data, including .wav files, file IDs, and transcriptions, with a focus on low-resource languages and dialects such as Javanese. This dataset enables researchers to address challenges in speech recognition and processing for underrepresented languages by providing a diverse and annotated corpus.	https://research.google/pubs/crowd-sourced-speech-corpora-for-javanese-sundanese-sinhala-nepali-and-bangladeshi-bengali/		1	
6402	Javanese Corpus	citing_context	IndoCulture	https://doi.org/10.48550/arXiv.2506.02573 (2025)	https://doi.org/10.48550/arXiv.2310.04928 (2023)	The IndoCulture dataset is used to evaluate the performance of fine-tuned models on cultural tasks in Indonesia, specifically focusing on the impact of safety fine-tuning on cultural sensitivity. This involves assessing how well the models handle culturally sensitive content, ensuring they are appropriate and respectful in their outputs. The dataset enables researchers to test and improve the cultural relevance and safety of AI models in Indonesian contexts.	https://huggingface.co/datasets/indolem/IndoCulture		1	
6545	Khmer Corpus	cited_context	Khmer character dataset	https://doi.org/10.3390/JIMAGING4020043 (2018)	https://doi.org/10.1145/2505377.2505394 (2013)	The Khmer character dataset is used to develop an annotation tool for characters and glyphs on document pages, specifically to improve Optical Character Recognition (OCR) for Khmer language documents. This involves annotating and labeling characters to enhance the accuracy of OCR systems, addressing the challenge of recognizing Khmer script in digital documents. The dataset's focus on Khmer characters enables researchers to create more effective tools for processing and analyzing Khmer language texts.	https://www.mdpi.com/2313-433X/4/2/43		1	
6542	Khmer Corpus	citing_context	Khmer	https://doi.org/10.1109/O-COCOSDA202152914.2021.9660421 (2021)	https://doi.org/10.21437/Eurospeech.2003-150 (2003)	The 'Khmer' dataset is used to train and evaluate speech-to-text translation models, specifically focusing on the Khmer language. It includes audio recordings and corresponding transcriptions, enabling researchers to develop and test translation systems that can accurately convert spoken Khmer into written text. This dataset supports the advancement of natural language processing technologies for under-resourced languages like Khmer.	https://www.isca-archive.org/eurospeech_2003/kikui03_eurospeech.html		1	
6550	Khmer Corpus	cited_context	Khmer palm leaf manuscripts of Cambodia	https://doi.org/10.23919/APSIPAASC55919.2022.9980217 (2022)	https://doi.org/10.1109/ICFHR-2018.2018.00012 (2018)	The Khmer palm leaf manuscripts of Cambodia dataset is used to study historical records, culture, and knowledge in Cambodia. Researchers focus on character and text recognition of these historical manuscripts, employing methodologies that enhance the understanding and preservation of Khmer historical texts. This dataset enables detailed analysis of cultural and historical data, contributing to the broader field of Cambodian studies.	https://www.akhmerbuddhistfoundation.org/manuscripts		1	
6553	Khmer Corpus	cited_context	palm leaf manuscript collection	https://doi.org/10.3390/JIMAGING4020043 (2018)		The palm leaf manuscript collection is used to test and evaluate word recognition and transliteration tasks, specifically focusing on the accuracy and effectiveness of OCR frameworks for historical Khmer texts. This dataset enables researchers to assess and improve the performance of OCR technologies in processing and interpreting ancient manuscripts, contributing to the preservation and accessibility of historical Khmer documents.	https://www.akhmerbuddhistfoundation.org/manuscripts		1	
6543	Khmer Corpus	citing_context	Khmer and Burmese ALT data	https://doi.org/10.46223/hcmcoujs.tech.en.12.1.2219.2022 (2022)	https://doi.org/10.1145/3325885 (2019)	The Khmer and Burmese ALT data dataset is primarily used for linguistic analysis and annotation of Khmer language data. It supports research on Khmer language processing by providing annotated data that facilitates the study of linguistic structures and features, enabling detailed analysis and the development of language processing techniques.	https://seacrowd.github.io/seacrowd-catalogue/dataset?id=14		1	
6540	Khmer Corpus	cited_context	Common Crawl corpus for Khmer	https://doi.org/10.18653/v1/D19-5206 (2019)	https://www.semanticscholar.org/paper/774e560a2cadcb84f4b1def7b152e5398b062efb (2013)	The Common Crawl corpus for Khmer is used to train 4-gram language models, which are concatenated to the target side of parallel data. This enhances the model's understanding and performance in machine translation tasks, particularly for the Khmer language. The dataset's large-scale, web-scraped nature provides extensive text data, improving the model's ability to handle Khmer text effectively.	https://metatext.io/datasets/cc100-khmer		1	
6551	Khmer Corpus	citing_context	Khmer POS dataset by Thu et al. (2017)	https://doi.org/10.46223/hcmcoujs.tech.en.12.1.2219.2022 (2022)		The Khmer POS dataset by Thu et al. (2017) is used to train and evaluate models for Khmer language processing, specifically focusing on part-of-speech tagging. It consists of a manually annotated corpus of 12,000 sentences, enabling researchers to analyze linguistic patterns, improve model accuracy, and address biases in Khmer language structures. This dataset supports the development of robust linguistic analysis tools for the Khmer language.	https://github.com/ye-kyaw-thu/khPOS		1	
6554	Khmer Corpus	cited_context | citing_context	Sleuk-Rith dataset	https://doi.org/10.48550/arXiv.2410.18277 (2024), https://doi.org/10.1109/ACCESS.2023.3332361 (2023)	https://doi.org/10.46223/hcmcoujs.tech.en.12.1.2217.2022 (2022), https://doi.org/10.1109/ICFHR-2018.2018.00012 (2018)	The Sleuk-Rith dataset is used for comparative analysis with new datasets, specifically focusing on historical hand-written and printed Khmer text that has been scanned and annotated. This dataset enables researchers to evaluate and benchmark the performance of new datasets in recognizing and processing Khmer script, facilitating advancements in digital humanities and historical document analysis.; The Sleuk Rith dataset is used for text recognition in historical Khmer palm leaf manuscripts. Researchers employ character and text recognition methods to analyze and interpret these manuscripts. This dataset enables the development and evaluation of algorithms for recognizing Khmer script, facilitating the preservation and study of historical texts.	https://github.com/donavaly/SleukRith-Set		1	
6539	Khmer Corpus	citing_context	Chhoun Nat dictionary	https://doi.org/10.46223/hcmcoujs.tech.en.12.1.2219.2022 (2022)	https://www.semanticscholar.org/paper/44619a377ac96c3d33f2b9cbef820b625af4d80a (2008)	The Chhoun Nat dictionary is used primarily for developing and evaluating automatic word segmentation systems for Khmer speech recognition. It contains a list of 18,000 words and is employed to train and evaluate models such as CRF models, focusing on the longest matching algorithm to study word segmentation and word boundaries. This dataset enables researchers to improve the accuracy of Khmer speech recognition systems by providing a standardized vocabulary reference.	https://angkordatabase.asia/links/chuon-naths-khmer-dictionary		1	
6548	Khmer Corpus	citing_context	Khmer-English ECCC corpus	https://doi.org/10.1109/O-COCOSDA202152914.2021.9660421 (2021)	https://doi.org/10.18653/v1/D19-5201 (2019)	The Khmer-English ECCC corpus is used for machine translation research, specifically to develop and evaluate translation models between Khmer and English. Researchers focus on text data to enhance the accuracy and fluency of translations, leveraging the dataset's bilingual content to train and test their models. This enables advancements in cross-lingual communication technologies.	https://ieeexplore.ieee.org/document/9660421		2	
6552	Khmer Corpus	cited_context | citing_context	KHPOS corpus	https://doi.org/10.1109/ACCESS.2023.3332361 (2023)		The KHPOS corpus is used to generate a validation dataset for evaluating Khmer language processing systems. It focuses on assessing the accuracy and robustness of these models, ensuring they perform well across various linguistic challenges. This dataset enables researchers to systematically test and improve Khmer language processing technologies.; The KHPOS corpus is used to generate a validation dataset for Khmer language processing, specifically to evaluate the accuracy and performance of synthetic text generation methods. This dataset enables researchers to test and refine models designed for generating synthetic Khmer text, ensuring they meet high standards of linguistic fidelity and utility.	https://github.com/ye-kyaw-thu/khPOS		2	
6547	Khmer Corpus	citing_context	Khmer ECCC	https://doi.org/10.1109/ICASSP49357.2023.10095644 (2023)	https://www.semanticscholar.org/paper/9b4ec1028823d15731da12bdaf78de57af69e1cd (2021)	The Khmer ECCC dataset is used for training and evaluating Automatic Speech Recognition (ASR) models specifically for the Khmer language. It involves training models on Khmer language data and measuring their performance using character error rates on a 10-hour test set. The dataset also includes synthetic Khmer speech data to enhance model performance and improve speech recognition accuracy.	https://ieeexplore.ieee.org/document/9660421		2	
6544	Khmer Corpus	citing_context	Khmer BTEC	https://doi.org/10.1109/APSIPAASC47483.2019.9023137 (2019)		The Khmer BTEC dataset is used to enhance the vocabulary of the Khmer language by gathering additional words from Khmer websites and collecting new words appearing in the BTEC data. This expansion of the dataset contributes to a richer and more comprehensive Khmer language resource, supporting research in Khmer language development and lexicon expansion.	https://ieeexplore.ieee.org/document/9023137		2	
6565	Kinyarwanda Corpus	cited_context	KIN data	https://doi.org/10.3115/v1/P15-1134 (2015)	https://www.semanticscholar.org/paper/727209a57c643472c8fc166ba3cc373936acc8d0 (2013)	The KIN data dataset is used to study testimonies by survivors of the Rwandan genocide, focusing on linguistic and thematic analysis in the Kinyarwanda language. Researchers employ qualitative methods to analyze the language and themes within these testimonies, addressing specific research questions related to the linguistic expression and thematic content of survivor narratives. This dataset enables detailed examination of how survivors articulate their experiences, providing insights into the cultural and psychological dimensions of trauma and resilience.	https://github.com/dhgarrette/low-resource-pos-tagging-2013		1	
6567	Kinyarwanda Corpus	citing_context	Kinyarwanda dataset	https://doi.org/10.48550/arXiv.2402.08638 (2024)		The Kinyarwanda dataset is used to create a monolingual STR dataset for Kinyarwanda, focusing on semantic textual similarity in multilingual research. It involves collecting pairs of sentences from news articles to study linguistic patterns and structures in news media. This dataset enables researchers to analyze and understand the nuances of the Kinyarwanda language in specific contexts, enhancing multilingual research methodologies.	https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus		2	
6570	Kinyarwanda Corpus	citing_context	machine-translated subset of GLUE	https://doi.org/10.18653/v1/2022.acl-long.367 (2022)	https://doi.org/10.18653/v1/W18-5446 (2018)	The machine-translated subset of GLUE is used to evaluate Kinyarwanda language models on natural language understanding tasks, specifically adapted from the GLUE benchmark. It assesses model performance in news article classification, focusing on category prediction accuracy. This dataset enables researchers to test and improve the effectiveness of Kinyarwanda language models in specific NLP tasks.	https://github.com/anzeyimana/kinyabert-acl2022		1	
6575	Kinyarwanda Corpus	cited_context	news categorization dataset	https://doi.org/10.18653/v1/2022.acl-long.367 (2022)	https://doi.org/10.18653/v1/W18-5446 (2018)	The news categorization dataset is used to evaluate the accuracy and reliability of category classification in local news articles in Kinyarwanda. Researchers employ this dataset to assess machine translation and categorization models, focusing on how well these systems can reliably classify news content into appropriate categories. This enables the development and improvement of news categorization tools tailored for the Kinyarwanda language.	https://github.com/anzeyimana/kinyabert-acl2022		1	
6574	Kinyarwanda Corpus	citing_context	MSWC corpus	https://www.semanticscholar.org/paper/3c0b89d19b49b2f314ebb2ef2459832f9ac83d89 (2025)	https://doi.org/10.1007/978-3-030-57077-4_10 (2021)	The MSWC corpus is used to train and evaluate pre-trained models for the Kinyarwanda language, specifically focusing on identifying and analyzing the most prevalent keywords within the corpus. This dataset enables researchers to enhance the performance and relevance of natural language processing models for Kinyarwanda by providing a rich source of linguistic data.	https://mlcommons.org/datasets/multilingual-spoken-words/		1	
6560	Kinyarwanda Corpus	cited_context | citing_context	AfriSenti datasets	https://doi.org/10.1109/ICMI60790.2024.10585867 (2024), https://doi.org/10.48550/arXiv.2302.08956 (2023)	https://doi.org/10.1109/AIBThings58340.2023.10292494 (2023), https://www.semanticscholar.org/paper/f5808755486ad4fe58d832e752e624382c5a2d5a (2020)	The AfriSenti dataset is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, based on the provided evidence, it cannot be accurately described how AfriSenti is used in research.; The AfriSenti datasets are used to perform sentiment analysis on African languages, with a focus on polarity classification. These datasets support the development of sentiment analysis systems, particularly for Kinyarwanda, enabling researchers to study the nuances of sentiment and language use in this linguistic context. The datasets facilitate the building of models that can accurately classify sentiments, enhancing understanding of diverse linguistic expressions.	https://huggingface.co/datasets/HausaNLP/AfriSenti-Twitter		1	
6566	Kinyarwanda Corpus	cited_context	KINNEWS	https://www.semanticscholar.org/paper/2ac19d63e1adba20473a6d1122c598f81efc3c58 (2022)	https://doi.org/10.18653/V1/2020.COLING-MAIN.480 (2020)	The KINNEWS dataset is used for news topic classification and Named Entity Recognition (NER) in Kinyarwanda. It evaluates model performance on Kinyarwanda news articles and analyzes linguistic features using a treebank approach. This dataset enables researchers to assess and improve NLP models specifically tailored for the Kinyarwanda language.	https://huggingface.co/datasets/andreniyongabo/kinnews_kirnews		1	
6563	Kinyarwanda Corpus	citing_context	Formality dataset	https://doi.org/10.48550/arXiv.2402.08638 (2024)	https://doi.org/10.18653/v1/N18-1012 (2018)	The Formality dataset is used to collect Kinyarwanda sentences for formality style transfer, focusing on pairs of sentences that convey the same meaning but differ in formality levels. This dataset enables researchers to develop and evaluate algorithms that can transform text between formal and informal styles, enhancing natural language processing applications in Kinyarwanda.	https://github.com/s-nlp/formality		1	
6562	Kinyarwanda Corpus	citing_context	AfriSenti-Sent 2023 Shared Task 12 datasets	https://doi.org/10.1109/AIBThings58340.2023.10292494 (2023)	https://doi.org/10.18653/v1/2021.mrl-1.11 (2021)	The 'AfriSenti-Sent 2023 Shared Task 12 datasets' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on the methodology, research questions, or specific characteristics of the dataset. Therefore, it cannot be accurately described beyond its mere mention in the literature.	https://github.com/afrisenti-semeval/afrisent-semeval-2023		1	
6797	Kyrgyz Corpus	cited_context | citing_context	780 dependency trees	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://www.semanticscholar.org/paper/e94e7940851d2908a993b9f90e3dd87c6de7c2f3 (2016)	The dataset '780 dependency trees' is mentioned in the citation context but lacks detailed descriptions of its usage. Therefore, there is no specific information available regarding its application, methodology, research questions, or enabling capabilities in any particular research area.; The '780 dependency trees' dataset is used to model the grammar, syntax, and morphology of the Kyrgyz language, contributing to the Universal Dependencies initiative. This dataset enables researchers to analyze and standardize linguistic structures, facilitating cross-linguistic research and computational linguistics applications.	https://github.com/UniversalDependencies/UD_Kyrgyz-KTMU/		1	
6801	Kyrgyz Corpus	cited_context | citing_context	Kyrgyz article titles	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://doi.org/10.18653/v1/D19-1410 (2019)	The 'Kyrgyz article titles' dataset is used to analyze language patterns in Kyrgyz by examining a subset of 500 randomly sampled and annotated article titles. This methodology involves deriving meaningful conclusions about specific topics within the language, enabling researchers to explore linguistic trends and content themes in Kyrgyz media.; The 'Kyrgyz article titles' dataset is used to derive meaningful conclusions about topics by translating and sampling 500 titles from a total of 23,284 Kyrgyz articles. These titles are then processed to obtain their embeddings using SentenceBERT, enabling researchers to analyze and understand the thematic content and structure of Kyrgyz articles through computational methods.	https://github.com/alexeyev/kyrgyz-multi-label-topic-classification		1	
6806	Kyrgyz Corpus	citing_context	UD-Kyrgyz	https://doi.org/10.48550/arXiv.2308.15952 (2023)	https://www.semanticscholar.org/paper/ba233d75aa403092bda0bffc026be7913673ad69 (2021)	The UD-Kyrgyz dataset is used to train and evaluate natural language processing (NLP) models specifically for the Kyrgyz language. It focuses on syntactic and morphological analysis, enabling researchers to develop and refine NLP tools that can accurately process and understand the complex grammatical structures of Kyrgyz. This dataset supports the advancement of NLP technologies tailored to under-resourced languages like Kyrgyz.	https://universaldependencies.org/ky/index.html		1	
6803	Kyrgyz Corpus	citing_context	KyrgyzTTS	https://doi.org/10.48550/arXiv.2407.05006 (2024), https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.21437/Interspeech.2021-2124 (2021)	The KyrgyzTTS dataset is used for training and testing text-to-speech (TTS) systems in the Kyrgyz language, focusing on enhancing the quality and accuracy of synthesized speech. This dataset enables researchers to develop more effective TTS models for Kyrgyz, addressing the specific phonetic and linguistic challenges of the language.	https://github.com/IS2AI/Kazakh_TTS		1	
6805	Kyrgyz Corpus	citing_context	Manas-UdS corpus	https://doi.org/10.48550/arXiv.2407.05006 (2024)		The Manas-UdS corpus is a recently developed dataset primarily used for training and testing text-to-speech (TTS) systems in the Kyrgyz language. It aims to enhance the quality and accuracy of synthesized speech, addressing the specific challenges and nuances of the Kyrgyz language. This dataset enables researchers to develop more effective TTS models tailored to Kyrgyz, contributing to advancements in natural language processing for under-resourced languages.	https://fedora.clarin-d.uni-saarland.de/kyrgyz/index.html		1	
6809	Magahi Corpus	citing_context	Magahi	https://doi.org/10.18653/v1/2024.findings-acl.857 (2024)	https://www.semanticscholar.org/paper/cc58ebb219feb0f6da5c0f52bed1721dcc4068a1 (2020)	The Magahi dataset is used to evaluate Part-of-Speech (POS) tagging performance in low-resource Indian languages, specifically focusing on Magahi within the Universal Dependency framework. This dataset enables researchers to assess the accuracy and effectiveness of POS tagging models in a resource-constrained setting, contributing to the development of linguistic tools for under-documented languages.	https://github.com/UniversalDependencies/UD_Bhojpuri-BHTB/tree/master		1	
6834	Magahi Corpus	citing_context	Magahi Dataset	https://doi.org/10.48550/arXiv.2403.04639 (2024)	https://doi.org/10.18653/v1/2023.wassa-1.12 (2023)	The Magahi Dataset is used for conducting sentiment analysis on Magahi language tweets. Native speakers manually annotate the data to ensure accuracy and relevance. This approach helps researchers address specific research questions related to sentiment detection in the Magahi language, leveraging the dataset's authentic and culturally relevant content.	https://github.com/MG1800/gsac		1	
7175	Marathi Corpus	cited_context | citing_context	Dakshina corpus	https://doi.org/10.1145/3618057 (2023), https://doi.org/10.48550/arXiv.2205.03018 (2022)	https://www.semanticscholar.org/paper/c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c (2020), https://doi.org/10.3115/v1/N15-3017 (2015)	The Dakshina corpus is used to train a BERT-based model for 17 Indic languages, including Marathi, with a focus on processing South Asian languages written in the Latin script. This dataset enables researchers to develop multilingual models that can handle the complexities of these languages, enhancing natural language processing capabilities in a diverse linguistic context.; The Dakshina corpus is used for developing and evaluating transliteration and script conversion systems, particularly for languages of the Indian subcontinent, including Marathi. It focuses on cross-script mapping and phonetic accuracy, enabling researchers to enhance the precision of transliteration systems across multiple Indic scripts. The dataset's multilingual nature supports comprehensive linguistic data analysis for these tasks.	https://github.com/google-research-datasets/dakshina		1	
7178	Marathi Corpus	cited_context | citing_context	EDNIL	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1145/3441501.3441516 (2020)	The EDNIL dataset is used for event detection from news in Indian languages, with a specific focus on the Marathi language. Researchers employ NLP and IR techniques to identify and classify events, enabling the analysis of news content and the extraction of meaningful information. This dataset facilitates the development and evaluation of algorithms tailored for event detection in Marathi news articles.; The EDNIL dataset is used for event detection from news in Indian languages, with a specific focus on the Marathi language. Researchers employ NLP and IR techniques to identify and classify events within news articles. This dataset enables the development and evaluation of algorithms tailored for Marathi, enhancing the accuracy of event detection in this language.	https://ednilfire.github.io/ednil/2020/dataset.html		1	
7186	Marathi Corpus	cited_context	HASOC 2021 Marathi	https://www.semanticscholar.org/paper/7558bdd18c1b52a2953fe80e7bcdbb5a09050ee2 (2021)	https://doi.org/10.26615/978-954-452-072-4_050 (2021)	The HASOC 2021 Marathi dataset is used to identify offensive language in Marathi social media posts, employing cross-lingual approaches to enhance NLP models for low-resource languages. It supports research aimed at improving the detection of offensive content, addressing the challenge of limited resources in under-resourced languages like Marathi.	https://hasocfire.github.io/hasoc/2021/dataset.html		1	
7187	Marathi Corpus	citing_context	HASOC 2022	https://doi.org/10.48550/arXiv.2212.10039 (2022)	https://www.semanticscholar.org/paper/062092a2b27af84b3becffd7439bf8c43574427b (2021)	The HASOC 2022 dataset is used to fine-tune models for identifying hate speech and offensive content in Indo-Aryan languages, including Marathi. Researchers focus on improving the detection accuracy of these models, employing methodologies that enhance the precision and reliability of hate speech identification in multilingual contexts. This dataset enables the development of more effective and culturally sensitive content moderation tools.	https://hasocfire.github.io/hasoc/2022/index.html		1	
7192	Marathi Corpus	citing_context	IIIT-H Voices	https://doi.org/10.18653/v1/2023.iwslt-1.43 (2023)	https://doi.org/10.21437/blizzard.2013-2 (2013)	The IIIT-H Voices dataset is primarily used for developing Text-to-Speech (TTS) systems, particularly for Indian languages, to enhance the quality and naturalness of synthesized speech. It supports research under unconstrained conditions, improving the robustness of speech synthesis models. Additionally, the dataset serves as a multi-language speech corpus, leveraging crowd-sourced data for various languages, including Marathi, to advance both speech recognition and synthesis research.	https://www.kaggle.com/datasets/sizlingdhairya1/iiit-spoken-language-datasets		1	
7195	Marathi Corpus	citing_context	In22-Gen	https://doi.org/10.48550/arXiv.2504.10335 (2025)	https://doi.org/10.48550/arXiv.2305.16307 (2023)	The In22-Gen dataset is used to evaluate machine translation models for 22 scheduled Indian languages, including Marathi. It focuses on achieving high-quality and accessible translations. Researchers employ this dataset to assess the performance and accuracy of translation models, ensuring they can effectively handle the linguistic complexities of these languages. This evaluation helps in improving the accessibility and reliability of machine translation systems for a diverse range of Indian languages.	https://huggingface.co/datasets/ai4bharat/IN22-Gen		1	
7202	Marathi Corpus	cited_context | citing_context	IndicNLP	https://doi.org/10.1145/3548457 (2022)	https://www.semanticscholar.org/paper/7997d432925aff0ba05497d2893c09918298ca55 (2020)	The IndicNLP dataset is used to develop monolingual corpora and word embeddings for Indian languages, with a focus on Marathi. It is employed to train and evaluate models for paraphrasing and embedding tasks, particularly using headlines and news articles in Indic languages, including Marathi. This dataset enables researchers to enhance natural language processing capabilities for under-resourced languages by providing extensive textual data for model training and evaluation.; The IndicNLP dataset is used to develop monolingual corpora and word embeddings for Indian languages, with a focus on Marathi. It trains and evaluates deep learning models for NLP tasks such as information retrieval, question answering, text summarization, and plagiarism detection. This dataset enables researchers to enhance model performance and linguistic understanding in Marathi through robust data resources.	https://github.com/AI4Bharat/indicnlp_corpus		1	
7205	Marathi Corpus	citing_context	Indic-Trans2	https://doi.org/10.48550/arXiv.2412.15471 (2024)	https://doi.org/10.18653/v1/2020.emnlp-main.213 (2020)	The Indic-Trans2 dataset is used to evaluate machine translation quality in 13 Indic languages, including Marathi, by reporting COMET-DA scores. This dataset facilitates the assessment of translation accuracy and fluency, enabling researchers to compare and improve machine translation models across these languages. The dataset's comprehensive coverage of multiple Indic languages makes it a valuable resource for cross-lingual translation research.	https://github.com/AI4Bharat/IndicTrans2		1	
7212	Marathi Corpus	cited_context | citing_context	L3Cube-MahaHate	https://doi.org/10.1145/3548457 (2022), https://doi.org/10.1109/I2CT61223.2024.10543946 (2024), https://doi.org/10.48550/arXiv.2506.00863 (2025)	https://doi.org/10.48550/arXiv.2203.13778 (2022)	The L3Cube-MahaHate dataset is primarily used for developing and evaluating hate speech detection models in Marathi, particularly focusing on tweet-based content. It supports classification tasks using transformer models like BERT and is also utilized for multi-domain sentiment analysis. The dataset includes manually labeled data, enabling researchers to train and evaluate models for detecting hate speech and analyzing sentiment across various domains.; The L3Cube-MahaHate dataset is used to develop and evaluate hate speech detection models specifically for the Marathi language, focusing on tweet-based content. Researchers employ this dataset to train and test machine learning models, addressing the challenge of identifying hate speech in social media. The dataset's focus on Marathi tweets enables nuanced analysis and improvement of hate speech detection algorithms in this language.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaHate		1	
7213	Marathi Corpus	cited_context | citing_context	L3Cube-MahaNER	https://doi.org/10.1145/3548457 (2022), https://doi.org/10.48550/arXiv.2308.09862 (2023), https://doi.org/10.48550/arXiv.2311.02579 (2023), https://doi.org/10.1007/978-981-99-6550-2_37 (2023), https://doi.org/10.1109/ICAAIC56838.2023.10141204 (2023)	https://doi.org/10.48550/arXiv.2204.06029 (2022)	The L3Cube-MahaNER dataset is primarily used for named entity recognition (NER) in the Marathi language, specifically to train and evaluate NER models. It contains 73,523 tokens and 6,138 sentences, enabling researchers to improve NER tagging performance in low-resource settings through multilingual learning. The dataset supports the development of BERT models, vanilla CNNs, and other NER systems, focusing on identifying person, location, and organization tags. Additionally, it has been utilized for hate speech detection and sentiment analysis in Marathi, enhancing the overall language processing capabilities.; The L3Cube-MahaNER dataset is used for named entity recognition in the Marathi language. It specifically trains and evaluates BERT models to identify entities within Marathi text. This dataset enables researchers to improve the accuracy of entity recognition in Marathi, contributing to advancements in natural language processing for under-resourced languages.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaNER		1	
7215	Marathi Corpus	cited_context | citing_context	L3CubeMahaSent	https://doi.org/10.48550/arXiv.2203.13778 (2022), https://doi.org/10.1109/ICAAIC53929.2022.9793308 (2022), https://doi.org/10.48550/arXiv.2211.11418 (2022), https://www.semanticscholar.org/paper/183ade75855453e7194b25c3db364b405fe98d38 (2021), https://doi.org/10.48550/arXiv.2211.11187 (2022), https://doi.org/10.48550/arXiv.2306.13888 (2023)	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	The L3CubeMahaSent dataset is primarily used for sentiment analysis of Marathi tweets, focusing on classifying them into positive, negative, and neutral categories. It is employed to evaluate the performance of various models, including XLM-R, IndicBERT, and CNN with Indic fastText word embeddings, on Marathi sentiment analysis tasks. This dataset provides a valuable resource for training and assessing model accuracy in classifying sentiments in Marathi social media content.; The L3CubeMahaSent dataset is primarily used for sentiment analysis of Marathi language text, particularly tweets. It supports the development and evaluation of deep learning models, including MahaBERT and XLM-R, for classifying sentiments into positive, negative, and neutral categories. The dataset, containing 15,000 manually labeled records, is used to enhance and expand domain coverage, including political tweets, movie reviews, general tweets, and TV show subtitles, thereby improving the robustness and applicability of sentiment analysis models in Marathi.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3CubeMahaSent%20Dataset		1	
7216	Marathi Corpus	citing_context	L3Cube-MahaSocialNER	https://doi.org/10.1145/3632754.3632764 (2023)	https://doi.org/10.48550/arXiv.2204.06029 (2022)	The L3Cube-MahaSocialNER dataset is primarily used for training and evaluating deep learning and neural network models for Marathi Named Entity Recognition (NER), particularly focusing on social media content. It supports the development and assessment of various neural architectures, including BERT models, to improve accuracy in identifying entities such as locations, organizations, people, and numeric quantities. The dataset facilitates comparisons between different tagging schemes (IOB and Non-IOB) and helps in adhering to gold standards for NER tasks.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaSocialNER		1	
7217	Marathi Corpus	citing_context	L3Cube’s MahaSent-GT	https://doi.org/10.48550/arXiv.2506.00863 (2025)	https://doi.org/10.48550/arXiv.2205.14728 (2022)	The L3Cube’s MahaSent-GT dataset is used for sentiment analysis and hate speech detection in Marathi, supporting tasks such as training and evaluating transformer models. It enables multi-domain sentiment classification and captures the nuances of Marathi language sentiment, enhancing the accuracy and applicability of NLP models in this context.	https://huggingface.co/l3cube-pune/marathi-sentiment-tweets		1	
7223	Marathi Corpus	citing_context	MahaSQuAD	https://doi.org/10.48550/arXiv.2505.03688 (2025)	https://www.semanticscholar.org/paper/7ebbb9f14a08fda8d76b3f299254c2b0d2c59d9a (2022)	The MahaSQuAD dataset is used to develop extensive resources for Hindi and Marathi by translating SQuAD 2.0, addressing data scarcity. It is employed to extend question-answering approaches from Marathi to nine additional Indic languages, focusing on linguistic divides and cross-lingual performance. The dataset ensures precise alignment of translated answers within their contexts, emphasizing linguistic accuracy and context preservation. It also supports the creation of similar datasets through robust translation and transliteration procedures for question-answering tasks.	https://huggingface.co/l3cube-pune/marathi-question-answering-squad-bert		1	
7225	Marathi Corpus	cited_context | citing_context	MOLD	https://doi.org/10.1109/ICAC3N56670.2022.10074266 (2022), https://doi.org/10.26615/978-954-452-072-4_050 (2021), https://www.semanticscholar.org/paper/062092a2b27af84b3becffd7439bf8c43574427b (2021)	https://www.semanticscholar.org/paper/78ba2856b30702c4ec0e6ea331616779ee926dc7 (2014), https://doi.org/10.18653/v1/2022.acl-long.62 (2020)	The Marathi test set is used to develop and evaluate natural language processing models. It supports the creation of a rule-based POS tagger for Marathi text, particularly in the tourism domain, by providing linguistic data for tagging parts of speech. Additionally, it is used to assess the zero-shot learning capabilities of the XLM-R model, focusing on the impact of language similarity in transfer learning. This dataset enables researchers to improve and evaluate the performance of NLP models in the Marathi language.; The Marathi test set is used to evaluate the performance of the XLM-R model in a zero-shot learning setting, focusing on the impact of language similarity in transfer learning. This dataset enables researchers to assess how well the model can generalize to unseen languages, particularly those with varying degrees of similarity to the training languages.	https://github.com/tharindudr/MOLD		1	
7229	Marathi Corpus	cited_context | citing_context	Marathi (Google, 2019d)	https://www.semanticscholar.org/paper/c5ccc4128f8746a765bdd4478f9b63893b5e6bb6 (2020)		The Marathi (Google, 2019d) dataset is used for training and evaluating models focused on the Marathi language. It supports research into linguistic and cultural aspects of Marathi, enabling the development and assessment of language-specific models and tools. This dataset facilitates the creation of more accurate and culturally relevant linguistic resources for Marathi.; The Marathi (Google, 2019d) dataset is used for training and evaluating language models and other NLP tasks in Marathi language research. It provides a resource for developing and testing algorithms that can process and understand Marathi text, enabling advancements in natural language processing for this specific language.	http://www.openslr.org/64		1	
7237	Marathi Corpus	citing_context	Marathi spoken tutorial data		https://doi.org/10.21437/interspeech.2020-3157 (2020)	The Marathi spoken tutorial data is used to train a Time-Delay Neural Network (TDNN) acoustic model, specifically to enhance speech recognition accuracy for Marathi language tutorials. This involves utilizing weakly labeled data, which helps in addressing the challenge of limited high-quality labeled data for under-resourced languages like Marathi. The dataset's focus on spoken tutorials provides context-specific training, enabling more accurate recognition in educational settings.	https://spoken-tutorial.org/		1	
7239	Marathi Corpus	cited_context | citing_context	Marathi Treebank	https://doi.org/10.1145/3548457 (2022)	https://www.semanticscholar.org/paper/f77ec4c85a8505b9c9502ccbd94f79082a4689d5 (2018)	The Marathi Treebank is used to create a manually annotated linguistic resource for Marathi, utilizing Wikisource stories and Wikipedia articles. This dataset supports dependency parsing and syntactic analysis, enabling researchers to study the grammatical structure and dependencies within Marathi sentences. It facilitates the development and evaluation of natural language processing tools tailored for the Marathi language.; The Marathi Treebank is used to create a manually annotated dependency treebank for Marathi, utilizing texts from Wikisource stories and Wikipedia articles. This dataset supports linguistic analysis by providing structured data for studying syntactic dependencies and grammatical relationships within the Marathi language. It enables researchers to explore specific linguistic phenomena and develop natural language processing tools tailored for Marathi.	https://github.com/UniversalDependencies/UD_Marathi-UFAL		1	
7248	Marathi Corpus	citing_context	MODI-HChar dataset	https://doi.org/10.48550/arXiv.2503.13060 (2025)		The MODI-HChar dataset is utilized to train and test machine learning models for recognizing handwritten Modi characters, addressing script variations and handwriting style differences. It is specifically employed for classifying individual characters in the Modi script, with a focus on enhancing the accuracy and robustness of character recognition algorithms. The dataset's variability in handwriting styles enables researchers to evaluate the performance of these models comprehensively.	https://www.kaggle.com/datasets/msd6013/modi-hdc-historical-handwritten-modi-script		1	
7259	Marathi Corpus	cited_context | citing_context	Raw Speech Corpora	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.1007/s10579-020-09523-3 (2021)	The Raw Speech Corpora dataset is used to develop speech recognition and synthesis systems for Indian languages, including Marathi, emphasizing the quality and diversity of speech data. It also supports linguistic and phonetic analysis, particularly through 89 hours of audio from 307 native Marathi speakers, enabling detailed studies of speech patterns and phonetic features.; The Raw Speech Corpora dataset is used to develop speech recognition and synthesis systems for Indian languages, including Marathi, emphasizing the quality and diversity of speech data. It also supports linguistic and phonetic analysis, particularly through 89 hours of audio from 307 native Marathi speakers, enabling detailed studies of speech patterns and phonetic characteristics.	https://www.ldcil.org/		1	
7261	Marathi Corpus	citing_context	SynthMoDe	https://doi.org/10.48550/arXiv.2503.13060 (2025)	https://doi.org/10.18280/isi.290311 (2024)	The SynthMoDe dataset is used to create a synthetic dataset for transcribing Modi script to Devanagari. It focuses on developing resources for script conversion and transcription, enabling researchers to enhance the accuracy and efficiency of historical document transcription. The dataset's synthetic nature supports the development and testing of transcription algorithms, facilitating advancements in digital humanities and historical text processing.	https://huggingface.co/datasets/historyHulk/SynthMoDe		1	
7262	Marathi Corpus	cited_context | citing_context	TaPaCo corpus	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.5281/ZENODO.3707949 (2020)	The TaPaCo corpus is used to provide a multilingual paraphrase corpus for 73 languages, including Marathi, extracted from the Tatoeba8 database. It supports linguistic research by offering a large, diverse set of paraphrases, enabling studies on language structure, semantics, and cross-linguistic comparisons. The dataset's multilingual nature facilitates research into how paraphrasing varies across different languages.; The TaPaCo corpus is used to provide multilingual paraphrase data for 73 languages, including Marathi, supporting research on paraphrasing and language understanding. It serves as a source database for extracting paraphrase data, enabling studies to analyze and improve natural language processing systems across diverse linguistic contexts.	https://zenodo.org/records/3707949		1	
7188	Marathi Corpus	citing_context	HASOC’21 Marathi dataset	https://doi.org/10.1007/978-3-031-20650-4_10 (2022)	https://www.semanticscholar.org/paper/062092a2b27af84b3becffd7439bf8c43574427b (2021)	The HASOC’21 Marathi dataset is used to identify and classify hateful and non-hateful content in Marathi, supporting the development and evaluation of hate speech detection models. This dataset enables researchers to train and test machine learning algorithms, enhancing their ability to accurately detect hate speech in the Marathi language.	https://hasocfire.github.io/hasoc/2021/dataset.html		2	
7220	Marathi Corpus	cited_context | citing_context	MahaHate dataset	https://doi.org/10.48550/arXiv.2306.13888 (2023), https://doi.org/10.48550/arXiv.2210.04267 (2022)	https://doi.org/10.48550/arXiv.2203.13778 (2022), https://doi.org/10.18653/v1/2021.woah-1.3 (2020)	The MahaHate dataset is used for detecting and classifying hate speech in Marathi language tweets. It focuses on identifying offensive and abusive language by fine-tuning BERT models to improve performance in hate speech detection. This dataset enables researchers to develop and evaluate machine learning models specifically tailored for Marathi, enhancing the accuracy of hate speech identification in social media.; The MahaHate dataset is used to detect and classify hate speech in Marathi, particularly in social media contexts like tweets. Researchers fine-tune BERT variants to improve model performance in identifying abusive language, focusing on the nuances of hate speech within the Marathi language. This dataset enables the development and evaluation of models specifically tailored for hate speech detection in Marathi.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaHate		2	
7221	Marathi Corpus	cited_context | citing_context	MahaNER	https://doi.org/10.48550/arXiv.2306.13888 (2023)	https://doi.org/10.48550/arXiv.2204.06029 (2022)	The MahaNER dataset is used for named entity recognition in Marathi, focusing on training and evaluating models to identify entities within Marathi text. This dataset enables researchers to develop and test algorithms that can accurately recognize and categorize named entities, enhancing natural language processing capabilities in the Marathi language.; The MahaNER dataset is used for named entity recognition (NER) in the Marathi language. It specifically supports the development and evaluation of BERT models tailored for Marathi language processing. This dataset enables researchers to enhance the performance of NER tasks by providing annotated data, facilitating the training and testing of machine learning models in the Marathi language context.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3Cube-MahaNER		2	
7222	Marathi Corpus	cited_context | citing_context	MahaSent	https://doi.org/10.48550/arXiv.2205.14728 (2022)	https://www.semanticscholar.org/paper/bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f (2021)	The MahaSent dataset is used for sentiment analysis in the Marathi language, focusing on tweet-based data. It is employed to train and evaluate machine learning models, addressing the resource scarcity in under-resourced languages. This dataset enables researchers to develop and test algorithms that can accurately analyze sentiments expressed in Marathi tweets.	https://github.com/l3cube-pune/MarathiNLP/tree/main/L3CubeMahaSent%20Dataset		2	
7233	Marathi Corpus	cited_context | citing_context	Marathi Ofensive Language Dataset (MOLD)	https://doi.org/10.1145/3548457 (2022)	https://doi.org/10.26615/978-954-452-072-4_050 (2021)	The Marathi Offensive Language Dataset (MOLD) is used to identify and analyze offensive language in the Marathi language, serving as the first publicly available resource for this purpose. It supports research into linguistic patterns and the social implications of offensive content, enabling studies that explore how offensive language is structured and its impact on society.; The Marathi Offensive Language Dataset (MOLD) is used to develop and evaluate models for identifying offensive language in Marathi, a low-resource language. This dataset addresses the scarcity of publicly available resources for Marathi, enabling researchers to build and test algorithms that can detect offensive content, thereby contributing to the improvement of online communication and content moderation in this language.	https://github.com/tharindudr/MOLD		2	
7238	Marathi Corpus	cited_context	Marathi training sets	https://www.semanticscholar.org/paper/c2db0501a11196e9d895e2659d9e18ff1a4aa25e (2021)	https://www.semanticscholar.org/paper/36c4c9fa1ebfa8c686161961bb487a9ee4b1a59e (2020)	The Marathi training sets are used to train and evaluate transformer models for identifying trolling, aggression, cyberbullying, hate speech, and offensive content in the Marathi language. Research focuses on multilingual joint fine-tuning and data integration, assessing model performance in these specific areas. The dataset's multilingual capabilities enable robust evaluation and improvement of model accuracy in detecting harmful online content.	https://hasocfire.github.io/hasoc/2021/dataset.html		2	
7240	Marathi Corpus	citing_context	Marathi tweet dataset	https://www.semanticscholar.org/paper/9a4841855093b6ab434b2e7b70cf34b3a072555b (2021)	https://doi.org/10.26615/978-954-452-072-4_050 (2021)	The Marathi tweet dataset is used to train and evaluate classifiers for identifying offensive language and hate speech in Marathi tweets. It consists of 1874 entries and supports cross-lingual offensive language identification, particularly for low-resource languages. This dataset enables researchers to focus on the challenges of processing and analyzing low-resource languages, enhancing the detection of offensive content in social media.	https://hasocfire.github.io/hasoc/2021/dataset.html		2	
7249	Marathi Corpus	cited_context | citing_context	MOLD	https://doi.org/10.26615/978-954-452-072-4_050 (2021)	https://doi.org/10.18653/v1/N19-1144 (2019)	The MOLD dataset is used for detecting offensive language and aggression in Marathi social media posts. It supports the classification of offensive content and the identification of hate speech. The dataset facilitates multilingual research by enabling data augmentation and the use of multilingual word embeddings, contributing to broader efforts in multilingual aggression detection.; The MOLD dataset is primarily used for studying offensive language detection in social media, specifically in the Marathi language. It is employed to train and test machine learning models for identifying and classifying offensive posts and aggressive content. The dataset supports research on hate speech and offensive content detection, enabling the evaluation of algorithms in multilingual contexts.	https://github.com/tharindudr/MOLD		2	
7274	Min Nan Chinese Corpus	citing_context	Taiwan Southern Min corpus	https://www.semanticscholar.org/paper/ab3d2ba190cb3539e1e363d6c382aebab76ed8e2 (2024)	https://www.semanticscholar.org/paper/e37d7fca6c6bdd8bcac8eeb546c218dd9bb10088 (2002)	The Taiwan Southern Min corpus is used to fine-tune sequence-to-sequence models for Min Nan Chinese, achieving strong performance even when the language is not part of the base Language Model. It is also utilized to apply state-of-the-art discourse segmentation techniques, enabling detailed analysis of discourse functions in Taiwanese Southern Min. This dataset supports research in natural language processing and linguistic analysis, enhancing the understanding and computational handling of this specific language.	https://sites.ualberta.ca/~johnnewm/TSM/Taiwanese_Southern_Min/TSM.html		1	
7322	Nepali Corpus	cited_context	annotated data from Twitter	https://doi.org/10.3115/v1/W14-3907 (2014)	https://www.semanticscholar.org/paper/0ffdee8b9d8eae3bcbf707068ca0aa3ca14cb772 (2012)	The annotated data from Twitter is used to study code-switching phenomena in the NEP-EN (Nepali-English) language pair, specifically within social media content. This dataset provides labeled examples of code-switching, enabling researchers to analyze linguistic patterns and develop models for understanding and processing multilingual social media data.	https://emnlp2014.org/workshops/CodeSwitch/call.html/		1	
7327	Nepali Corpus	citing_context	ENeMeme		https://doi.org/10.1145/3701716.3718389 (2025)	The ENeMeme dataset is used to develop and evaluate models for detecting hate speech in Nepali-English memes. It focuses on multilingual and multimodal content, enabling researchers to address the complexities of hate speech in mixed-language social media posts. This dataset supports the creation of robust models that can accurately identify and mitigate harmful content in diverse linguistic contexts.	https://github.com/therealthapa/crossplatform		1	
7331	Nepali Corpus	citing_context	ILPRL	https://www.semanticscholar.org/paper/933802cd80c2249f887a433825a5eae3e66cd7e3 (2024)	https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de (2019)	The ILPRL dataset is used to evaluate the Named Entity Recognition (NER) performance of XLM-RoBERTa, specifically focusing on F1 scores. It leverages self-supervised masked language modeling with dynamic masking and cross-lingual objectives, enabling researchers to assess the model's effectiveness in multilingual settings. This dataset facilitates the analysis of cross-lingual transfer learning and the optimization of NER tasks in low-resource languages.	https://ilprl.ku.edu.np/		1	
7333	Nepali Corpus	cited_context | citing_context	NAT-CTC		https://doi.org/10.48550/arXiv.2210.05425 (2022)	The NAT-CTC dataset is used to classify Nepali tweets related to COVID-19 topics, focusing on 12,241 tweets in Devanagari script. It employs a classification methodology to analyze these tweets under eight simplified topics. This dataset enables researchers to understand public sentiment and discourse on COVID-19 in the Nepali language, providing insights into specific issues and concerns expressed on social media.; The NAT-CTC dataset is used for classifying Nepali tweets related to COVID-19 into eight simplified topics. It employs a multi-annotator and multi-label classification approach, enabling researchers to analyze and categorize social media content systematically. This methodology helps address research questions related to public sentiment and information dissemination during the pandemic.	https://github.com/naamiinepal/covid-tweet-classification		1	
7367	Nepali Corpus	citing_context	YouTube comments	https://doi.org/10.1145/3647782.3647804 (2024)	https://doi.org/10.1109/ASONAM49781.2020.9381292 (2020)	The YouTube comments dataset is used for aspect-based sentiment analysis, specifically to detect abusive sentiments in Nepali social media texts. Machine learning techniques are employed to analyze the dataset, which is crucial for understanding and mitigating online abuse in Nepali-language content. The dataset's focus on Nepali language and social media context enables researchers to address specific cultural and linguistic nuances in abusive language detection.	https://github.com/oya163/nepali-sentiment-analysis		1	
7341	Nepali Corpus	citing_context	Nepali National Corpus	https://doi.org/10.1007/s10462-021-10093-1 (2021), https://doi.org/10.1109/ICICT64420.2025.11004703 (2025)	https://doi.org/10.3366/E1749503208000166 (2008)	The Nepali National Corpus (NNC), a large-scale monolingual written text dataset containing over 14 million words, is used for developing linguistic resources and NLP activities in the Nepali language. It supports the creation of educational and communication tools through human and automated tagging methods, facilitating the annotation of the corpus. The dataset, systematically scraped from the Ekan-tipur news portal, provides web-based textual content for language analysis, modeling, and the development of a tagset for Nepali, enhancing linguistic and computational research.	https://www.sketchengine.eu/nepali-national-corpus/		1	
7364	Nepali Corpus	cited_context	transcribed audio data for Nepali Language	https://www.semanticscholar.org/paper/39b813feea417f7d9fd7c2cbdf36660d5abb9ca1 (2021)	https://doi.org/10.21437/SLTU.2018-14 (2018)	The transcribed audio data for the Nepali language is used to build Text-to-Speech (TTS) systems, focusing on improving audio-text alignment and pronunciation accuracy. This dataset enables researchers to collect and transcribe Nepali language data, enhancing the development of TTS voices that accurately reflect the nuances of spoken Nepali.	https://www.openslr.org/54/		1	
7340	Nepali Corpus	citing_context	Nepali movie review dataset	https://www.semanticscholar.org/paper/f34f24b0785524a0540f5252632348fd5d58c0dd (2014)		The Nepali movie review dataset is used for sentiment analysis, specifically analyzing 500 text documents from online sources. Researchers employ this dataset to understand the sentiment expressed in Nepali language movie reviews, using methods like text mining and natural language processing. This enables the exploration of public opinion and emotional responses to movies in the Nepali context.	https://www.kaggle.com/datasets/shikharghimire/nepali-language-sentiment-analysis-movie-reviews		1	
7350	Nepali Corpus	cited_context | citing_context	NepCOV19Tweets	https://doi.org/10.1007/s12652-023-04692-9 (2022), https://doi.org/10.1109/ACCESS.2023.3342154 (2023), https://doi.org/10.1155/2022/5681574 (2022)	https://doi.org/10.1155/2021/2158184 (2021)	The NepCOV19Tweets dataset is primarily used for sentiment analysis of Nepali COVID-19-related tweets. Researchers employ both traditional machine learning classifiers and deep learning methods, such as convolutional neural networks and FastText-based feature extraction, to analyze public reactions and opinions during the pandemic. The dataset enables the optimization of hyper-parameters and the evaluation of model effectiveness in understanding public sentiment.; The NepCov19Tweets dataset is used for sentiment analysis of Nepali COVID-19-related tweets, focusing on public reactions and opinions during the pandemic. Researchers employ FastText-based feature extraction and deep learning methods, such as convolutional neural networks, to classify and analyze the sentiment of these tweets, demonstrating the effectiveness of these techniques in understanding public sentiment.	https://www.kaggle.com/datasets/mathew11111/nepcov19tweets		1	
7325	Nepali Corpus	cited_context | citing_context	CombinedNepaliNews	https://doi.org/10.7717/peerj-cs.412 (2021)	https://doi.org/10.18653/v1/N19-1423 (2019)	The CombinedNepaliNews dataset is used to evaluate the performance of proposed methods against existing ones, specifically focusing on the accuracy of language processing tasks in Nepali news articles. This involves comparing methodologies to enhance the precision and reliability of language processing techniques tailored for the Nepali language. The dataset's relevance lies in its application to improve and benchmark natural language processing (NLP) models for Nepali news content.; The CombinedNepaliNews dataset is used to evaluate the performance of proposed methods in Nepali language processing, specifically comparing these methods against existing models like BERT. This dataset facilitates research by providing a benchmark for assessing the effectiveness of new approaches in handling Nepali text data.	https://www.kaggle.com/datasets/lotusacharya/nepalinewsdataset		1	
7344	Nepali Corpus	citing_context	Nepali news articles	https://doi.org/10.48550/arXiv.2409.19566 (2024)	https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The Nepali news articles dataset is used to support multilingual summarization research by creating a comprehensive collection of articles through web scraping from online news portals such as BBC Nepali, Kantipur, and Gorkhapatra. This dataset enables researchers to develop and evaluate summarization techniques specifically tailored for the Nepali language, enhancing the accuracy and relevance of automated summaries in multilingual contexts.	https://www.kaggle.com/datasets/ashokpant/nepali-news-dataset-large		1	
7324	Nepali Corpus	citing_context	CHiPSAL shared task dataset	https://doi.org/10.48550/arXiv.2412.17131 (2024)	https://www.semanticscholar.org/paper/050685e0f34903f5bd6d454a771b50f116074761 (2025)	The CHiPSAL shared task dataset is used to evaluate models on tasks related to processing South Asian languages, with a specific focus on Nepali. It supports research into linguistic challenges and model performance, enabling researchers to assess how well models handle the complexities of Nepali language data.	https://sites.google.com/view/chipsal/shared-tasks		1	
7336	Nepali Corpus	citing_context	Nepali CC-100	https://doi.org/10.1109/ACCESS.2023.3342154 (2023)		The Nepali CC-100 dataset is used to train and benchmark models for the Nepali language, including the roberta-base-ne model with 12 million sentences. It is also utilized to train a BiLSTM model for part-of-speech tagging and named entity recognition, focusing on the linguistic structure and annotation of the Nepali language. This dataset enables researchers to develop and evaluate natural language processing models specifically tailored for Nepali.	https://metatext.io/datasets/cc100-nepali		1	
7348	Nepali Corpus	citing_context	Nepali SQuAD	https://doi.org/10.3126/kjse.v9i1.78368 (2025)	https://doi.org/10.48550/arXiv.2501.05963 (2025)	The Nepali SQuAD dataset is used to adapt machine translation approaches for the Nepali language, specifically focusing on span annotations within a question-answering context. This involves utilizing the dataset's annotated spans to improve the accuracy of machine translation systems in generating correct answers to questions posed in Nepali.	https://huggingface.co/datasets/suban244/squad_v1.1_np		1	
7351	Nepali Corpus	citing_context	NepSA	https://doi.org/10.1109/ACCESS.2023.3342154 (2023)	https://doi.org/10.1109/ASONAM49781.2020.9381292 (2020)	The NepSA dataset is used for performing sentiment analysis on Nepali social media texts, particularly focusing on comments from popular Nepali YouTube channels and videos. Researchers employ this dataset to analyze public sentiment, using methods tailored to the nuances of the Nepali language. This enables detailed insights into audience reactions and opinions on specific content.	https://github.com/merishnaSuwal/nep-off-langdetect		1	
7357	Nepali Corpus	citing_context	OSLR54	https://doi.org/10.48550/arXiv.2402.03050 (2024)	https://doi.org/10.21437/SLTU.2018-14 (2018)	The OSLR54 dataset is primarily used for training and enhancing Text-to-Speech (TTS) models and Automatic Speech Recognition (ASR) systems, particularly for Nepali and other South Asian languages. It employs open-source data and methodologies, focusing on improving the quality, robustness, and usability of speech synthesis applications in under-resourced languages. This dataset supports the development of high-quality TTS voices and contributes to advancing voice synthesis capabilities.	https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet		1	
7343	Nepali Corpus	citing_context	NepaliNER	https://doi.org/10.1109/ACCESS.2023.3342154 (2023)	https://doi.org/10.1109/CIC48465.2019.00031 (2019)	The NepaliNER dataset is used to develop and evaluate Named Entity Recognition (NER) systems for the Nepali language. Researchers focus on identifying and classifying named entities in text, employing this dataset to enhance the accuracy and performance of NER models specifically tailored for Nepali. This dataset enables the advancement of natural language processing techniques for under-resourced languages like Nepali.	https://github.com/dadelani/nepali-ner		1	
7339	Nepali Corpus	cited_context	Nepali Monolingual Written Corpus	https://doi.org/10.1007/s10462-021-10093-1 (2021)	https://doi.org/10.3366/E1749503208000166 (2008)	The Nepali Monolingual Written Corpus (NNC), containing over 14 million words, is used to develop linguistic resources and support research in the Nepali language. It facilitates NLP activities and linguistic analyses, including the creation of a tagset through human and automated tagging methods. The corpus, divided into subsets for various applications, supports studies in language education, communication, and core linguistic features, providing a foundational resource for both educational and computational research.	https://catalogue.elra.info/en-us/repository/browse/ELRA-W0076/		1	
7353	Nepali Corpus	cited_context	NNC spoken corpus	https://doi.org/10.3366/E1749503208000166 (2008)	https://doi.org/10.3115/1118078.1118079 (2001)	The NNC spoken corpus is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or characteristics of the dataset. Therefore, based on the provided evidence, it cannot be accurately described how this dataset is used in research.	https://www.sketchengine.eu/nepali-national-corpus/		2	
7346	Nepali Corpus	citing_context	Nepali news documents dataset	https://doi.org/10.1109/ACCESS.2023.3342154 (2023)	https://doi.org/10.7717/peerj-cs.412 (2021)	The Nepali news documents dataset is used to classify Nepali documents into 17 news categories. Researchers employ a supervised codebook approach for vector representation, enabling the systematic categorization of news articles. This methodology supports the development of classification models tailored to the Nepali language, enhancing the accuracy and relevance of news category prediction.	https://www.kaggle.com/datasets/ashokpant/nepali-news-dataset-large		2	
7375	Nigerian Pidgin Corpus	cited_context	large corpora of articles in Naijá and English		https://doi.org/10.18653/v1/P19-1625 (2019)	The dataset of large corpora of articles in Naijá and English is used to examine code-switching behavior between Naijá and Nigerian Standard English. Researchers focus on analyzing linguistic patterns and usage in articles and comments, employing corpus linguistics methods to identify and study the specific ways in which speakers switch between these languages. This dataset enables detailed insights into the syntactic and semantic aspects of code-switching, contributing to a deeper understanding of multilingual communication in Nigeria.	https://blablablab.si.umich.edu/projects/naija/		1	
7384	Nigerian Pidgin Corpus	citing_context	PidginUNMT	https://doi.org/10.48550/arXiv.2404.18264 (2024)	https://doi.org/10.18653/v1/P19-1625 (2019)	The 'newspaper text' dataset is used to develop and evaluate named entity recognition models for Nigerian Pidgin, analyze language use and content in religious magazines, and study linguistic patterns and sociolinguistic factors in newspaper articles. It enables researchers to focus on specific cultural and linguistic aspects, providing insights into the use of Nigerian Pidgin in written media and its sociocultural context.	https://github.com/keleog/PidginUNMT		1	
7387	Nigerian Pidgin Corpus	citing_context	NigP extension (Data Science Nigeria, 2020)	https://doi.org/10.1111/WENG.12544 (2021)	https://www.semanticscholar.org/paper/82a0f2f4e839280d7041fc63237ef60340870b7d (2020)	The NigP extension dataset (Data Science Nigeria, 2020) is used to enhance the VADER sentiment lexicon for Nigerian Pidgin English, specifically to improve sentiment classification in social media contexts. This involves extending the lexicon to better capture the nuances of Nigerian Pidgin, thereby improving the accuracy of sentiment analysis in this language.	https://github.com/DataScienceNigeria/Research-Papers-by-Data-Science-Nigeria/tree/master/Semantic%20Enrichment%20of%20Nigerian%20Pidgin%20English%20for%20Contextual%20Sentiment%20Classification		1	
7391	Nigerian Pidgin Corpus	cited_context	Pidgin corpus	https://www.semanticscholar.org/paper/880c3a51450b9a4c9f3c0de44352b2ae0f8a3e63 (2019)	https://doi.org/10.3115/v1/D14-1162 (2014)	The Pidgin corpus is used to fine-tune word embeddings using the CBOW method, specifically to capture local context in the Nigerian Pidgin language. This approach enhances the representation of words within their linguistic environment, enabling more accurate and contextually relevant language processing tasks.			2	
7394	Nigerian Pidgin Corpus	citing_context	speech recognition dataset for Naija	https://doi.org/10.18653/v1/2021.conll-1.5 (2021)	https://www.semanticscholar.org/paper/fa01cc42f9d67fd7d50c6e15ba4d9d87cfec3ce3 (2017)	The 'speech recognition dataset for Naija' is used to evaluate sentiment, develop and test speech recognition systems, analyze syntactic structures, and classify named entities in Nigerian Pidgin. Research methodologies include assessing positive, negative, and neutral expressions, improving transcription accuracy, examining dependency relations, and enhancing information extraction. This dataset supports diverse linguistic and computational research, providing a robust resource for understanding and processing Nigerian Pidgin.	https://github.com/hclent/creole-dro		1	
7388	Nigerian Pidgin Corpus	cited_context	NSC	https://doi.org/10.18653/v1/W19-7803 (2019)	https://www.semanticscholar.org/paper/d1eccd9ac0b0dff6cd94f8bc609a812551198742 (2010)	The NSC dataset is used to conduct corpus-based surveys of Naija, focusing on syntactic and lexical aspects of the language. It supports linguistic research, enabling detailed analyses of the structure and vocabulary of Naija, funded by the French research agency ANR. This dataset facilitates in-depth linguistic studies by providing a rich resource for examining the language's grammatical and lexical features.	https://universaldependencies.org/treebanks/pcm_nsc/index.html		1	
7396	Nigerian Pidgin Corpus	citing_context	UD-Pidgin	https://doi.org/10.18653/v1/2025.findings-naacl.85 (2024)	https://doi.org/10.18653/v1/W19-7803 (2019)	The UD-Pidgin dataset is used to create a surface-syntactic treebank for Naija, focusing on dependency parsing and syntactic structure analysis. This involves annotating sentences to capture their grammatical dependencies, enabling researchers to study the syntactic properties of Nigerian Pidgin. The dataset facilitates the development and evaluation of computational models for parsing and understanding the language's syntax.	https://universaldependencies.org/treebanks/pcm_nsc/index.html		1	
7380	Nigerian Pidgin Corpus	citing_context	NaijaHate	https://www.semanticscholar.org/paper/2a9d2e03f927800585d72928ea3d88483191b9c9	https://doi.org/10.48550/arXiv.2403.19260 (2024)	The NaijaHate dataset is used to evaluate hate speech detection on Nigerian Twitter, specifically addressing regional linguistic diversity and ensuring data representativeness. Researchers employ this dataset to develop and test algorithms that can accurately identify hate speech in the context of Nigerian Pidgin, enhancing the understanding of hate speech dynamics in this linguistic community.	https://huggingface.co/datasets/worldbank/NaijaHate		1	
7377	Nigerian Pidgin Corpus	cited_context	masakaner-v1	https://doi.org/10.48550/arXiv.2212.10785 (2022)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The 'masakaner-v1' dataset is used for evaluating Named Entity Recognition (NER) models across multiple African languages, including Nigerian Pidgin (pcm). It focuses on performance metrics and is employed to assess NER model effectiveness in both eastern and western African languages. This dataset enables researchers to compare and improve NER models' accuracy and robustness across diverse linguistic contexts.	https://huggingface.co/datasets/masakhane/masakhaner		1	
7400	Nigerian Pidgin Corpus	cited_context	YOSM	https://doi.org/10.48550/arXiv.2212.10785 (2022)	https://doi.org/10.48550/arXiv.2204.09711 (2022)	The YOSM dataset is primarily used for fine-tuning sentiment analysis models, particularly for Nigerian Pidgin and Yorùbá languages. It enhances the detection of sentiment in domain-specific contexts such as movie reviews and user opinions, improving the model's ability to understand emotional responses in these creole and African languages. The dataset's focus on specific linguistic nuances and domain relevance makes it valuable for advancing natural language processing techniques in under-resourced languages.	https://github.com/IyanuSh/YOSM		1	
7382	Nigerian Pidgin Corpus	cited_context | citing_context	NaijaSenti dataset	https://doi.org/10.48550/arXiv.2304.06845 (2023), https://doi.org/10.18653/v1/2023.semeval-1.205 (2023), https://doi.org/10.1109/ICMI60790.2024.10585876 (2024), https://doi.org/10.48550/arXiv.2404.18264 (2024), https://www.semanticscholar.org/paper/ecd9168526d1a82ac2348c8de52bff6323322da9 (2022)	https://www.semanticscholar.org/paper/ecd9168526d1a82ac2348c8de52bff6323322da9 (2022), https://doi.org/10.18653/v1/2021.mrl-1.11 (2021)	The NaijaSenti dataset is primarily used for multilingual sentiment analysis, focusing on Nigerian Pidgin and other major Nigerian languages. It employs a Twitter corpus to classify sentiments into positive, negative, and neutral categories. Researchers use it to train, test, and evaluate models like mDeBERTaV3 and AfriBERTa, emphasizing cross-lingual transfer, translation, and the nuances of low-resource languages. The dataset facilitates the investigation of model performance and the complexities of sentiment analysis in Nigerian Pidgin, often using a three-way data split for training, validation, and testing.; The NaijaSenti dataset is primarily used for training and evaluating sentiment analysis models, particularly AfriBERTa and mDeBERTa, in Nigerian Pidgin and other African languages. It provides labeled data for social media and multilingual contexts, enhancing cross-lingual capabilities and model performance. The dataset supports research focused on improving sentiment analysis in African languages, including evaluations and fine-tuning of multilingual models.	https://github.com/hausanlp/NaijaSenti		1	
7373	Nigerian Pidgin Corpus	citing_context	eBible corpus	https://doi.org/10.18653/v1/2025.findings-naacl.85 (2024)		The eBible corpus is used for translating religious texts into Nigerian Pidgin, serving as a valuable resource for linguistic and cultural studies in under-resourced languages. This dataset supports research by providing authentic translations that enhance understanding of language use and cultural nuances in Nigerian Pidgin.	https://ebible.org/pcm/		1	
7371	Nigerian Pidgin Corpus	citing_context	CultureBank	https://doi.org/10.48550/arXiv.2503.19642 (2025)	https://doi.org/10.48550/arXiv.2404.15238 (2024)	The CultureBank dataset is used to build culturally aware systems by providing culture-specific data to enhance language technologies. This community-driven, online resource focuses on Nigerian Pidgin and related languages, enabling researchers to develop more contextually relevant and linguistically accurate systems.	https://culturebank.github.io/		1	
7389	Nigerian Pidgin Corpus	cited_context	NSC treebank	https://doi.org/10.18653/v1/W19-7803 (2019)	https://www.semanticscholar.org/paper/13a105a764f8b1f30193901ad8c147a9ace95f8c (2018)	The NSC treebank is used to enhance the study of Naija grammar by focusing on the annotation and analysis of linguistic structures. This dataset supports research aimed at establishing Naija as a recognized language through detailed grammatical analysis. The treebank's annotated data facilitates the examination of syntactic and morphological features, enabling scholars to address specific research questions related to Naija's linguistic properties.	https://universaldependencies.org/treebanks/pcm_nsc/index.html		2	
7383	Nigerian Pidgin Corpus	citing_context	NaijSenti	https://doi.org/10.1109/ITED56637.2022.10051345 (2022)	https://www.semanticscholar.org/paper/ecd9168526d1a82ac2348c8de52bff6323322da9 (2022)	The NaijSenti dataset is used to develop and evaluate multilingual sentiment analysis models, particularly focusing on Nigerian Twitter data. It includes about 30,000 annotated tweets in Nigerian Pidgin, Hausa, Igbo, and Yorùbá. Researchers use this dataset to understand sentiment in these languages, employing it to train and test models that can accurately analyze and classify sentiments expressed in multilingual contexts.	https://github.com/hausanlp/NaijaSenti		2	
7398	Nigerian Pidgin Corpus	citing_context	W ARRI	https://doi.org/10.18653/v1/2025.findings-naacl.85 (2024)	https://doi.org/10.1162/tacl_a_00416 (2021)	The W ARRI dataset, derived from WAPE BBC news data, is primarily used for named entity recognition (NER) in Nigerian Pidgin. It has been incorporated into the MasakhaNER dataset to support NER tasks for African languages. This dataset enables researchers to develop and evaluate NER models specifically tailored for Nigerian Pidgin, enhancing the accuracy of identifying named entities in this language.	https://aclanthology.org/2025.findings-naacl.85.pdf		2	
7413	Northern Pashto Corpus	cited_context	Pashto character dataset	https://doi.org/10.3390/electronics10202508 (2021)	https://doi.org/10.1007/s11042-019-7330-0 (2019)	The Pashto character dataset is used to train and evaluate models, particularly extreme learning machines, for recognizing handwritten Pashto characters. Research focuses on assessing the performance and accuracy of these models, enabling advancements in the recognition of Pashto handwriting.	https://github.com/Dibyasundar/OdiaOCR/tree/master/ELM%20Research%20on%20character		1	
7417	Northern Pashto Corpus	cited_context	POLD dataset	https://doi.org/10.7717/peerj-cs.1617 (2023)	https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de (2019)	The POLD dataset is used to fine-tune XLM-RoBERTa for transfer learning, specifically focusing on multilingual representation learning. It evaluates the effectiveness of pre-trained models in this context, enabling researchers to assess how well these models adapt to new languages and tasks. This dataset supports the development and testing of advanced natural language processing techniques.	https://zenodo.org/records/8195797		1	
7416	Northern Pashto Corpus	cited_context	Poha	https://doi.org/10.1109/ACCESS.2023.3248508 (2023)	https://doi.org/10.3390/s20205884 (2020)	The Poha dataset is used for recognizing Pashto handwritten characters through deep learning techniques. It enhances character recognition accuracy and robustness, focusing on improving the performance of character recognition models. This dataset supports research aimed at advancing the capabilities of machine learning algorithms in processing and understanding handwritten Pashto text.	https://www.mdpi.com/1424-8220/20/20/5884		1	
7409	Northern Pashto Corpus	citing_context	Katib’s Pashto Text Imagebase (KPTI)	https://doi.org/10.7717/peerj-cs.1925 (2024)	https://doi.org/10.1109/ICFHR.2016.0090 (2016)	The Katib’s Pashto Text Imagebase (KPTI) dataset is used to benchmark deep learning models for Pashto text recognition. Researchers focus on evaluating the performance of various neural network architectures, introducing new models, and comparing their effectiveness in recognizing Pashto text. This dataset enables the development and refinement of deep learning techniques tailored to the unique characteristics of the Pashto language.	https://github.com/rahmad77/KPTI		1	
7424	Northern Sotho Corpus	citing_context	NCHLT corpora	https://doi.org/10.4102/LIT.V38I1.1351 (2017)	https://www.semanticscholar.org/paper/1a7715e8d41a228f8fc35f7e3f2580988eea9b24 (2014)	The NCHLT corpora are used to develop text resources for ten South African languages, with a focus on incorporating figurative interpretations into the corpora. This dataset enables researchers to enhance linguistic resources by including idiomatic expressions, thereby improving the accuracy and cultural relevance of language models and tools.	https://sites.google.com/site/nchltspeechcorpus		1	
7431	Northern Sotho Corpus	citing_context	Sepedi Corpus	https://doi.org/10.5788/32-2-1698 (2022)	https://doi.org/10.1016/j.specom.2013.07.001 (2014)	The Sepedi Corpus is used to modify boundary marks within the dataset, ensuring alignment with other parts of the corpus for consistent sentence counting. This process facilitates more accurate linguistic analysis by standardizing the structure of the Sepedi data, enabling researchers to conduct reliable comparisons and analyses across different segments of the corpus.	https://repo.sadilar.org/items/83ad3a3c-e393-43cf-bc54-acaab108aa48		1	
7420	Northern Sotho Corpus	citing_context	Groot Noord-Sotho Woordeboek (GNS)	https://doi.org/10.1080/02572117.2020.1733838 (2020)		The Groot Noord-Sotho Woordeboek (GNS) dataset is used to compare Northern Sotho dictionary entries, specifically focusing on the spelling of loanwords and the consistency in linguistic rules. This involves a detailed analysis of dictionary entries to assess linguistic consistency and the integration of loanwords, enabling researchers to evaluate the standardization of the Northern Sotho language.	https://books.google.de/books/about/Groot_Noord_Sotho_woordeboek.html?id=PKJCzwEACAAJ&redir_esc=y		1	
7433	Northern Thai Corpus	citing_context	Northern Thai Dialect Dataset	https://doi.org/10.48550/arXiv.2504.05898 (2025)	https://doi.org/10.18653/v1/2021.findings-acl.413 (2021)	The Northern Thai Dialect Dataset is used to train and evaluate models for summarization tasks, specifically focusing on the Northern Thai dialect. It supports the development of multilingual abstractive summarization systems, enabling researchers to enhance the performance and accuracy of these models in handling Northern Thai content.	https://github.com/mrpeerat/Thai_local_benchmark		1	
7512	Odia Corpus	citing_context	17K Odia Wikipedia articles	https://www.semanticscholar.org/paper/6c801b22856ad936deaea8f4299a1d994e76a7f7 (2021)	https://doi.org/10.18653/v1/2020.nlposs-1.10 (2020)	The 17K Odia Wikipedia articles dataset is used to train and test language models for the Odia language, focusing on the development and evaluation of natural language processing tools. Specifically, it supports the training of language models and the testing of their classification performance, with an emphasis on model accuracy. This dataset enables researchers to advance NLP capabilities for Indian languages, particularly Odia.	https://www.kaggle.com/datasets/disisbig/odia-wikipedia-articles		1	
7513	Odia Corpus	citing_context	50k sentences corpora	https://www.semanticscholar.org/paper/7f9c8bf26f5b805f83cbd1f46d85ca53f2f683c8 (2017)	https://doi.org/10.1007/978-3-319-08958-4_43	The 50k sentences corpora dataset is used to translate 50,000 sentences from Hindi into 12 major Indian languages, including Odia, with a focus on health and tourism domains. This translation effort employs a corpus-based approach to ensure linguistic accuracy and cultural relevance. The dataset enables researchers to develop and evaluate translation models, enhancing multilingual communication in specific sectors.	https://sanskrit.jnu.ac.in/ilci/index.jsp		1	
7524	Odia Corpus	citing_context	Lingua Libre	https://doi.org/10.1145/3626524 (2023)	https://doi.org/10.1145/3487553.3524931 (2022)	The Lingua Libre dataset is used to build a public domain voice database for the Odia language, focusing on collecting and processing audio data. This dataset supports linguistic research by providing a resource for analyzing spoken Odia, enabling studies on phonetics, phonology, and speech processing. The dataset's public domain status facilitates accessible and collaborative research efforts.	https://lingualibre.org		1	
7519	Odia Corpus	citing_context	IITBBS numeral dataset	https://doi.org/10.14569/ijacsa.2024.0150271 (2024)	https://doi.org/10.1016/j.patrec.2016.05.031 (2016)	The IITBBS numeral dataset is used for numeral classification, focusing on recognizing unconstrained handwritten numerals. Researchers apply BESAC features to enhance recognition accuracy. This dataset enables the development and evaluation of algorithms for numeral recognition, contributing to advancements in handwriting analysis and pattern recognition.	https://ieee-dataport.org/documents/iitbbs-ocr-dataset		1	
7533	Odia Corpus	cited_context	Odia-OPUS	https://doi.org/10.1162/tacl_a_00452 (2021)	https://www.semanticscholar.org/paper/e11edb4201007530c3692814a155b22f78a0d659 (2016)	The Odia-OPUS dataset is used to train machine translation models for the Odia language, specifically aiming to enhance translation quality and coverage across various domains. This dataset enables researchers to develop more accurate and contextually relevant translations, addressing the need for improved linguistic resources in the Odia language.	https://opus.nlpl.eu/		1	
7530	Odia Corpus	cited_context	OdiEnCorp 2.0	https://doi.org/10.1162/tacl_a_00452 (2021)	https://doi.org/10.1109/ICACCP.2019.8882969 (2019)	The ODIA-EN dataset is primarily used to train and evaluate neural machine translation models for the Odia-English language pair. It focuses on improving translation quality and coverage, especially in low-resource settings. The dataset provides multilingual parallel corpora, enhancing the performance of translation systems across various languages, including Odia. This enables researchers to develop more accurate and robust translation models.	https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3211		1	
7531	Odia Corpus	citing_context	Odia master data llama2	https://doi.org/10.1145/3639856.3639890 (2023)		The 'Odia instruction sets' dataset is used to study the structure and usage of Odia language commands in computational tasks. Researchers analyze the syntactic and semantic properties of these instructions to understand how they can be effectively utilized in computational contexts. This dataset enables detailed linguistic analysis and informs the development of Odia-specific computational tools and systems.	https://huggingface.co/datasets/OdiaGenAI/odia_master_data_llama2		1	
7527	Odia Corpus	cited_context	NITR OHCSv1	https://doi.org/10.1007/s11042-020-09457-6 (2020)	https://doi.org/10.1007/978-3-319-71767-8_4 (2018)	The NITR OHCSv1 dataset is used to evaluate feature extraction methods for Odia language character recognition. Researchers focus on assessing the effectiveness of different machine learning models, employing the dataset to compare and analyze various techniques. This enables the development and refinement of algorithms tailored for Odia character recognition, enhancing accuracy and performance in this specific linguistic context.	https://ieeexplore.ieee.org/document/7490020		1	
7520	Odia Corpus	citing_context	ILCI	https://doi.org/10.1145/3637877 (2023)	https://www.semanticscholar.org/paper/9e9741daf545bebf2e3404aa18764f940f76d421 (2021)	The ILCI dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	https://sanskrit.jnu.ac.in/ilci/index.jsp		2	
7523	Odia Corpus	citing_context	ISI numeral dataset	https://doi.org/10.14569/ijacsa.2024.0150271 (2024)	https://doi.org/10.1109/ICAPR.2015.7050694 (2015)	The ISI numeral dataset is used for evaluating the accuracy of handwritten Odia numeral recognition. Research employs feature extraction techniques in the transformed domain to achieve over 90% accuracy. This dataset specifically supports the development and testing of algorithms designed to recognize handwritten numerals in the Odia language, enabling advancements in optical character recognition technology for this script.	https://inria.hal.science/inria-00104486v1		2	
7532	Odia Corpus	citing_context	IITBBS-OCR-Dataset	https://doi.org/10.1109/ICINPRO43533.2018.9096757 (2018)	https://doi.org/10.1109/ICDAR.2005.84 (2005)	The Odia numeral recognition dataset, comprising 5970 samples, is used to train and evaluate models for recognizing handwritten Odia numerals. This dataset supports research in character recognition for Indian scripts, enabling the development and testing of algorithms that improve the accuracy of numeral recognition in the Odia language.	https://ieee-dataport.org/documents/iitbbs-ocr-dataset		2	
8235	Rundi Corpus	citing_context	AfroDigits	https://doi.org/10.48550/arXiv.2303.12582 (2023)	https://doi.org/10.18653/v1/2020.findings-emnlp.195 (2020)	The AfroDigits dataset is used to build a spoken digit dataset for African languages, including Rundi, through a community-based participatory approach. This methodology involves engaging local communities to contribute speech data, addressing the scarcity of African speech corpora. The dataset enables researchers to develop and test speech recognition systems tailored to African languages, enhancing the representation and accuracy of these systems in multilingual contexts.	https://huggingface.co/datasets/chrisjay/crowd-speech-africa		1	
8659	Setswana Corpus	citing_context	United States Peace Corps’ Intro to Spoken Setswana	https://doi.org/10.48550/arXiv.2408.02239 (2024)	https://www.semanticscholar.org/paper/2edbe6cb5a4bed33ee703079241b58146dd1d327 (1987)	The United States Peace Corps’ Intro to Spoken Setswana dataset is used as a foundational reference for spoken Setswana, providing language lessons and cultural context. It is employed to validate translations of human rights concepts, ensuring accuracy and cultural relevance. Additionally, the dataset is used to train and evaluate machine translation models, focusing on practical language skills and cultural nuances. This enables researchers to enhance the precision and applicability of Setswana translations and language models.	https://files.peacecorps.gov/multimedia/audio/languagelessons/botswana/Bw_Setswana_Language_Lessons.pdf		1	
8662	Setswana Corpus	citing_context	WordSim	https://doi.org/10.55492/dhasa.v3i03.3822 (2021)	https://www.semanticscholar.org/paper/fcf816d9e7b804f4201e4cbf5437e62d683c8a8e (2018)	The WordSim dataset is used to evaluate the effectiveness of cross-lingual embeddings for Setswana and Sepedi, focusing on intrinsic evaluations of lexical and semantic similarity. It provides word pairs with human-rated similarities, enabling researchers to assess word embeddings in semantic similarity and relatedness tasks. This dataset supports the development and evaluation of models that capture semantic relationships between words in these languages.	https://zenodo.org/record/5673974		1	
8652	Setswana Corpus	cited_context	SABC News	https://doi.org/10.48550/arXiv.2205.02022 (2022)	https://www.semanticscholar.org/paper/24ac368d08765dfad920ceefb79fba7bfe81d83c (2021)	The SABC News dataset is used to train and evaluate machine translation models, specifically for translating between Setswana and English within the news domain. This dataset enables researchers to focus on improving translation accuracy and fluency in news-specific contexts, leveraging its news domain data to enhance model performance.	https://github.com/masakhane-io/lafand-mt		1	
8653	Setswana Corpus	citing_context	SADiLaR NER dataset	https://doi.org/10.14569/ijacsa.2025.0160249 (2025)	https://doi.org/10.59200/icarti.2023.024 (2023)	The SADiLaR NER dataset is used to evaluate the performance of a CNN model for Setswana Named Entity Recognition, focusing on metrics like F1-Score. It is employed to compare the effectiveness of the CNN model against baseline models, enabling researchers to assess improvements in recognizing named entities in the Setswana language.	https://thesai.org/Publications/ViewPaper?Volume=16&Issue=2&Code=ijacsa&SerialNo=49		1	
8658	Setswana Corpus	citing_context	Setswana Universal Declaration of Human Rights (Nations, 1998)	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)		The 'Setswana Universal Declaration of Human Rights (Nations, 1998)' dataset is primarily used for training and evaluating machine translation models, particularly for low-resource and multilingual settings, including Setswana. It supports tasks such as long-form question answering, topic classification, and parallel text translation. The dataset enhances model performance by providing a rich resource for under-resourced African languages, improving translation quality and linguistic feature analysis. It is also used for multilingual pre-training, OCR, and reformatting to support digital analysis and educational content.	https://www.ohchr.org/en/human-rights/universal-declaration/translations/southern-sothosothosesothosutusesutu		1	
8647	Setswana Corpus	cited_context | citing_context	NCHLT corpus	https://doi.org/10.48550/arXiv.2408.02239 (2024), https://www.semanticscholar.org/paper/33776a7747eb157c2dd70b1283d83f0533569ce9 (2014)	https://doi.org/10.48550/arXiv.2310.09141 (2023), https://www.semanticscholar.org/paper/d77cdbb1c4a7e1707c32a1c6db2b1e7ddf351483 (2013)	The NCHLT dataset is used to provide high-quality Setswana language data for the SetsText project, comprising over eight million tokens from diverse sources such as government documents, books, educational materials, and online content. It serves as a foundational resource, enhancing the quality and breadth of Setswana language datasets, though specific methodological details are not provided.; The NCHLT corpus is used to train and evaluate the Setswana language dataset, focusing on verifying trends and the final selection of corpus metadata. This dataset enables researchers to refine and validate the metadata associated with the Setswana language, ensuring its accuracy and relevance for linguistic studies.	https://huggingface.co/datasets/nwu-ctext/nchlt		1	
a	Setswana Corpus	citing_context	African Wordnet Project (AWN) 	https://www.semanticscholar.org/paper/67b359bb74f77e63fbd828a8ee7c93886e37e776 (2024)	https://doi.org/10.4102/lit.v38i1.1351	The African Wordnet Project (AWN) building wordnets for five African languages: Setswana, isiXhosa, isiZulu, Sesotho sa Leboa 	https://africanwordnet.wordpress.com/		1	
a	Setswana Corpus	citing_context	Vuk’uzenzele	https://www.semanticscholar.org/paper/67b359bb74f77e63fbd828a8ee7c93886e37e776 (2024)	https://doi.org/10.18653/v1/2023.rail-1.3	The dataset contains editions from the South African government magazine Vuk'uzenzele. Data was scraped from PDFs that have been placed in the data/raw folder. The PDFS were obtained from the Vuk'uzenzele website.	https://github.com/dsfsi/vukuzenzele-nlp		1	
a	Setswana Corpus	citing_context	ZA-gov-multilingual	https://www.semanticscholar.org/paper/67b359bb74f77e63fbd828a8ee7c93886e37e776 (2024)	https://doi.org/10.18653/v1/2023.rail-1.3	The dataset contains editions from the South African government magazine Vuk'uzenzele. Data was scraped from PDFs that have been placed in the data/raw folder. The PDFS were obtained from the Vuk'uzenzele website.	https://github.com/dsfsi/gov-za-multilingual		1	
8633	Setswana Corpus	citing_context	Autshumato Machine Translation Evaluation Set	https://www.semanticscholar.org/paper/67b359bb74f77e63fbd828a8ee7c93886e37e776 (2024)	https://doi.org/10.1016/j.dib.2020.105146 (2020)	The comparable evaluation data for use in automatic machine translation evaluations. The evaluation set consists of 500 sentences translated separately by four different professional human translators for each of the 11 official South African languages. This creates a set with four reference translations for each of the 11 languages where each of the texts can be used as an input text as well. This ensures that the evaluation set can be used to evaluate machine translation between any two of the 11 languages.	https://repo.sadilar.org/items/c6b36522-9531-4ec0-9565-17c7524404a0		1	
8648	Setswana Corpus	citing_context	No Language Left Behin (NLLB)	https://doi.org/10.48550/arXiv.2408.02239 (2024)	https://doi.org/10.48550/arXiv.2207.04672 (2022)	The NLLB dataset is used to enhance cross-lingual capabilities in natural language processing. It supports the creation of parallel corpora and multilingual models, facilitating entity recognition, sentence alignment, and machine translation. The dataset's multilingual nature, including potential Setswana content, enables researchers to develop and evaluate cross-lingual transfer learning and multilingual NLP systems.	https://github.com/facebookresearch/fairseq/tree/nllb		1	
8657	Setswana Corpus	cited_context	Setswana-OPUS	https://doi.org/10.48550/arXiv.2310.09141 (2023)	https://www.semanticscholar.org/paper/7a0640db17b461b75df21f737154ad0f6d43e9de (2020)	The Setswana-OPUS dataset is used to gather monolingual input corpora for Setswana, specifically for training and evaluation purposes in research projects starting in 2020. This dataset enables researchers to develop and assess natural language processing models tailored for the Setswana language, focusing on improving their performance and accuracy.	https://opus.nlpl.eu/		1	
8630	Setswana Corpus	citing_context	CaLMQA	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://doi.org/10.48550/arXiv.2309.07445 (2023)	The CaLMQA dataset is primarily used to support research in machine translation, question answering, and information retrieval, particularly for under-resourced languages like Setswana. It enhances the performance of machine translation systems, evaluates question-answering and information retrieval systems, and supports cross-lingual transfer learning. The dataset is also utilized for terminology translation, instruction tuning, and topic classification across multiple languages, including Setswana, providing a rich resource for linguistic tasks and improving the handling of low-resource languages.	https://huggingface.co/datasets/shanearora/CaLMQA		1	
8646	Setswana Corpus	citing_context	MMLU-tsn	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://www.semanticscholar.org/paper/65906e6027246ae9e4ecd18d6e019a24505c842e (2020)	The MMLU-tsn dataset is used to evaluate and develop multitask language understanding in Setswana, focusing on domain-specific vocabulary, complex tasks, and problem-solving skills. It translates benchmarks like MMLU and GSM8K to assess models' performance in linguistic accuracy, numerical reasoning, and logical thinking. The dataset enables researchers to measure models' capabilities in handling complex, contextually appropriate tasks and solving arithmetic problems in Setswana.	https://huggingface.co/datasets/OxxoCodes/mmlu-tsn		1	
8643	Setswana Corpus	citing_context	Marothodi	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://doi.org/10.1145/3394486.3406703 (2020)	The Marothodi dataset is used to train Pula models, specifically focusing on the Setswana language. An altered version of the dataset is combined with another dataset to total approximately 2.3 billion tokens. This combination enhances the training data, enabling more robust and accurate language modeling for Setswana.	https://huggingface.co/datasets/OxxoCodes/Marothodi		1	
8650	Setswana Corpus	citing_context	Puo-Data	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://doi.org/10.18653/v1/P19-1310 (2019)	Puo-Data is used as the largest curated Setswana text collection to train and evaluate PuoBERTa, a language model specifically developed for Setswana. This dataset enables researchers to focus on the development and improvement of natural language processing capabilities for the Setswana language, enhancing the performance and accuracy of language models in this domain.	https://huggingface.co/datasets/dsfsi/PuoData		1	
8641	Setswana Corpus	cited_context	Lwazi ASR corpus	https://www.semanticscholar.org/paper/33776a7747eb157c2dd70b1283d83f0533569ce9 (2014)	https://doi.org/10.21437/Interspeech.2009-760 (2009)	The dataset 'Lwazi' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	https://huggingface.co/datasets/dsfsi/lwazi-asr-corpus-compressed		1	
8632	Setswana Corpus	citing_context	Daily News Dikgang	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://www.semanticscholar.org/paper/1a7715e8d41a228f8fc35f7e3f2580988eea9b24 (2014)	The Daily News Dikgang dataset is extensively used in Setswana language research, primarily for training and testing various natural language processing (NLP) models. It contributes to the development of the NCHLT Setswana RoBERTa model by providing a large, diverse corpus of Setswana text, enhancing the model's robustness and performance. The dataset is also utilized for tasks such as automatic speech recognition, text-to-speech synthesis, news classification, named entity recognition, lemmatization, and part-of-speech tagging, focusing on improving the accuracy, quality, and linguistic diversity of these systems.	https://huggingface.co/datasets/dsfsi/daily-news-dikgang		1	
8629	Setswana Corpus	citing_context	Bloom-lm	https://doi.org/10.18653/v1/2025.naacl-long.338 (2024)	https://doi.org/10.48550/arXiv.2303.03750 (2023)	The Bloom-lm dataset is integrated into the Marothodi corpus to enhance the Setswana language dataset, expanding its size, diversity, and multilingual coverage. It supports various NLP tasks, including speech recognition, language processing, and evaluation. The dataset provides additional annotated text, improving the training and quality of linguistic models for Setswana.	https://huggingface.co/bigscience/bloom		1	
8661	Setswana Corpus	cited_context	Wikipedia Corpus (XML Wikipedia dumps: 11 September 2017)	https://doi.org/10.3390/info11010041 (2020)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The Wikipedia Corpus (XML Wikipedia dumps: 11 September 2017) is used to train word embeddings for multiple languages, including Setswana, in a 300-dimensional space. These embeddings are utilized to initialize word embeddings for various languages, excluding isiNdebele, enhancing natural language processing tasks by providing pre-trained linguistic representations.	https://dumps.wikimedia.org/		1	
8655	Setswana Corpus	cited_context	NCHLT speech corpus of the South African languages	https://www.semanticscholar.org/paper/33776a7747eb157c2dd70b1283d83f0533569ce9 (2014)	https://doi.org/10.21437/ICSLP.1992-277 (1992)	The Setswana dataset is used as a reference for carefully enunciated speech in the GlobalPhone corpus, contrasting with the more natural, conversational speech found in the Fisher corpus. This comparison helps researchers analyze differences in speech patterns and pronunciation clarity, aiding in the development and evaluation of speech recognition systems.	https://sites.google.com/site/nchltspeechcorpus		1	
8656	Setswana Corpus	cited_context	South African Cabinet Speeches	https://doi.org/10.48550/arXiv.2310.09141 (2023)	https://doi.org/10.48550/arXiv.2303.03750 (2023)	The Setswana Cabinet Speeches dataset is used to enhance linguistic resources for Setswana by providing a rich text corpus for language modeling and analysis. It is also utilized to study official government communications in Setswana, focusing on formal language use and policy discourse. This dataset enables researchers to analyze and model the formal and structured nature of Setswana as used in governmental contexts.	https://huggingface.co/datasets/dsfsi/gov-za-monolingual		1	
8660	Setswana Corpus	citing_context	Vuk’zenzele Setswana Corpora	https://doi.org/10.48550/arXiv.2310.09141 (2023)	https://doi.org/10.48550/arXiv.2303.03750 (2023)	The Vuk’zenzele Setswana Corpora is utilized to analyze linguistic patterns in official communications, particularly in political discourse, and to enhance linguistic resources for the Setswana language. This rich text corpus supports language modeling and analysis, providing valuable data for understanding and improving the use of Setswana in formal contexts.	https://huggingface.co/datasets/dsfsi/vukuzenzele-monolingual		1	
8670	Sindhi Corpus	citing_context	lexicon database	https://doi.org/10.1109/INMIC48123.2019.9022770 (2019)		The 'lexicon database' dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive synthesis of how this dataset is actually used in research.	https://www.sindhinlp.com/		1	
8686	Sindhi Corpus	cited_context	Wiki-dumps (2016)	https://doi.org/10.48550/arXiv.2408.15720 (2024)	https://doi.org/10.32350/LLR/11/04 (2015)	The Wiki-dumps (2016) dataset is utilized for enhancing Sindhi language processing capabilities through downstream NLP tasks. It provides 100K tokens and contains 4.1M tokens, which are used to develop language technology tools such as POS tagging and named-entity recognition. This dataset enables researchers to improve the accuracy and efficiency of Sindhi language technology applications.	https://dumps.wikimedia.org/		1	
8665	Sindhi Corpus	citing_context	Qaida	https://doi.org/10.1109/FIT60620.2023.00033 (2023)	https://www.semanticscholar.org/paper/d38e613ae12e9073b4e8feefae360a6ea8d0ad44 (2020)	The dataset '18 thousand ligatures across 256 distinct fonts' is mentioned in the citation context but lacks detailed usage descriptions. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.	https://github.com/AtiqueUrRehman/qaida		1	
8668	Sindhi Corpus	citing_context	Emotion-Pak Corpus	https://doi.org/10.1109/CISCE52179.2021.9445883 (2021)	https://doi.org/10.48084/ETASR.3193 (2019)	The Emotion-Pak Corpus is mentioned in research citations but lacks detailed descriptions of its usage. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling capabilities in any particular research area.	https://zenodo.org/records/3566598		1	
8669	Sindhi Corpus	citing_context	isolated handwritten Sindhi characters	https://doi.org/10.1109/FIT60620.2023.00033 (2023)	https://doi.org/10.17485/ijst/v13i25.914 (2020)	The isolated handwritten Sindhi characters dataset is used to train and evaluate deep residual neural networks for recognizing these characters. This approach achieves 94% accuracy, addressing the specific research question of improving recognition rates for isolated handwritten Sindhi characters. The dataset's focus on individual characters is crucial for this task, enabling precise evaluation of model performance in this domain.	https://indjst.org/articles/deep-learning-based-isolated-handwritten-sindhi-character-recognition		1	
8681	Sindhi Corpus	citing_context	SiNER	https://www.semanticscholar.org/paper/35c6bcc3a86023f95b6be40aa6eddf8df52c027d (2020)	https://doi.org/10.1561/2200000013 (2010)	The SiNER dataset is used to evaluate and analyze Named Entity Recognition (NER) performance in the Sindhi language. It focuses on assessing the effectiveness of a Conditional Random Fields (CRF)-based approach for sequence labeling, specifically tailored for Sindhi language processing. This dataset enables researchers to measure and improve the performance of NER models in the context of Sindhi, contributing to advancements in Sindhi language technology.	https://github.com/AliWazir/SiNER-dataset		1	
8678	Sindhi Corpus	cited_context	sentiment summarization and analysis of Sindhi text	https://doi.org/10.1109/INMIC48123.2019.9022770 (2019)	https://dx.doi.org/10.14569/IJACSA.2017.081038	The Sindhi text corpus is used to develop and train models for the Sindhi language, focusing on text processing, linguistic data annotation, and sentiment analysis. It supports the creation of Unicode-8 based annotated datasets, labeling of Sindhi tweets, and development of large-scale linguistic resources. The dataset enables research in stemming, lemmatization, and sentiment polarity scoring, enhancing the analysis of Sindhi text and social media content.	https://www.sindhinlp.com/		2	
8683	Sindhi Corpus	cited_context	Unicode-8 based linguistic dataset	https://doi.org/10.1109/INMIC48123.2019.9022770 (2019)	https://doi.org/10.1016/j.dib.2018.05.062 (2018)	The Unicode-8 based linguistic dataset is used to develop and annotate large-scale linguistic resources for the Sindhi language, focusing on text corpus creation, stemming, and lemmatization. It is also employed to analyze sentiment in Sindhi text, generating polarity scores for lexicon entries. This dataset enables the training and development of models for text processing and analysis, enhancing the accuracy and efficiency of linguistic research in the Sindhi language.	https://www.sindhinlp.com/		2	
8825	Somali Corpus	citing_context	WCS	https://doi.org/10.1167/16.5.14 (2016)	https://doi.org/10.1073/pnas.0607708103 (2006)	The WCS dataset is used in linguistic research to classify and compare Somali color terms with universal color categories. It provides a standard set of 11 color categories, enabling researchers to analyze the distribution and deployment of Somali color names, such as buluug, in relation to English and other languages. This facilitates comparative linguistic analysis and the examination of universal patterns in color naming.	https://linguistics.berkeley.edu/wcs/data.html		1	
8820	Somali Corpus	citing_context	Masakhane News Dataset (the Somali subset)	https://doi.org/10.48550/arXiv.2503.18117 (2025)	https://doi.org/10.18653/v1/2021.mrl-1.11 (2021)	The Masakhane News Dataset (Somali subset) is used to enhance the quality and coverage of Somali language data in multilingual corpora. It integrates news articles and web-crawled data to increase linguistic diversity and representativeness. The dataset is also utilized to evaluate cross-lingual information retrieval (CLIR) performance, focusing on the effectiveness of Somali language processing in these tasks.	https://github.com/masakhane-io/masakhane-news		1	
8817	Somali Corpus	citing_context	CC100-Somali Dataset	https://doi.org/10.48550/arXiv.2503.18117 (2025)	https://doi.org/10.18653/v1/2020.acl-main.747 (2019)	The CC100-Somali Dataset is used to enhance multilingual language models and corpora, specifically enriching the Somali subset with diverse and representative content. It integrates web-crawled, news, and literary texts to improve the quality and coverage of Somali language data. This dataset supports cross-lingual representation learning, language processing, and evaluation, particularly in cross-lingual information retrieval tasks. Its large scale and varied content make it valuable for building comprehensive and contextually rich Somali language corpora.	https://github.com/facebookresearch/fairseq		1	
8823	Somali Corpus	citing_context	 CIRAL dataset (Somali subset)	https://doi.org/10.48550/arXiv.2503.18117 (2025)	https://doi.org/10.1145/3626772.3657884 (2024)	The Somali Books dataset is used to enhance multilingual language models and corpora, specifically by increasing the diversity and volume of Somali language data. It integrates news articles, web-crawled content, and literary texts to improve the representativeness and linguistic variety of the Somali subset. This dataset is crucial for evaluating cross-lingual information retrieval (CLIR) performance, focusing on the effectiveness of Somali language processing in these tasks.	https://github.com/ciralproject/ciral		1	
8824	Somali Corpus	cited_context	LORELEI language packs	https://doi.org/10.1162/tacl_a_00416 (2021)	https://www.semanticscholar.org/paper/5a0cc05753097721d2c6397d7a7ad0ce4acf4569 (2016)	The SomaliST dataset is used to train and evaluate named entity recognition (NER) models specifically for the Somali language, with a focus on identifying entities in government-related texts. This dataset enables researchers to develop and test NER systems that can accurately recognize and categorize named entities such as organizations, locations, and individuals in Somali-language documents, enhancing the processing and analysis of government-related content.	https://dss.princeton.edu/catalog/resource4916		1	
8836	Southern Sotho Corpus	cited_context	NCHLT corpus	https://www.semanticscholar.org/paper/33776a7747eb157c2dd70b1283d83f0533569ce9 (2014)	https://www.semanticscholar.org/paper/d77cdbb1c4a7e1707c32a1c6db2b1e7ddf351483 (2013)	The NCHLT corpus is mentioned in research citations but lacks detailed descriptions of its usage, methodology, or specific research applications. There is no explicit information on how it is employed in studies or its relevance to particular research questions or characteristics.	https://sites.google.com/site/nchltspeechcorpus/		1	
9631	Sudanese Arabic Corpus	citing_context	Sudanese Arabic sentiment dataset	https://doi.org/10.1177/01655515231188341 (2022)		The Sudanese Arabic sentiment dataset is used to evaluate sentiment analysis models, focusing on three-class classification. Researchers employ this dataset to report accuracy scores, assessing model performance in distinguishing positive, negative, and neutral sentiments. This dataset enables the development and validation of sentiment analysis techniques tailored for Sudanese Arabic, contributing to the broader field of natural language processing.	https://github.com/mustafa20999/Sudanese-Arabic-Sentiment-Datasets		1	
9627	Sudanese Arabic Corpus	citing_context	Sudanese Arabic corpus	https://doi.org/10.1109/ICCCEEE49695.2021.9429560 (2021), https://doi.org/10.1177/01655515231188341 (2022)	https://doi.org/10.1109/ICCCEEE.2018.8515862 (2018)	The Sudanese Arabic corpus is used to train and evaluate sentiment analysis models for the Sudanese Arabic dialect, specifically to minimize bias by employing multiple annotators. It has also been utilized to build a three-class sentiment dataset (SudSenti3) from Twitter, enhancing sentiment analysis in Sudanese Arabic. This dataset supports research in natural language processing, particularly in developing more accurate and culturally sensitive sentiment analysis tools for this dialect.	https://github.com/mustafa20999/Sudanese-Arabic-Sentiment-Datasets		2	
9674	Swahili Corpus	cited_context	OPUS parallel data	https://doi.org/10.18653/v1/2020.nlpcovid19-2.5 (2020)	https://www.semanticscholar.org/paper/25ca4a36df2955b345634b5f8a6b6bb66a774b3c (2012)	The OPUS parallel data dataset is used to train OPUS-MT systems for multiple language pairs, including Swahili. It focuses on parallel text alignment and enhancing translation quality. The dataset's parallel text structure enables researchers to develop and evaluate machine translation models, ensuring accurate and contextually appropriate translations across languages.	https://opus.nlpl.eu/		1	
9658	Swahili Corpus	citing_context	Kencorpus	https://doi.org/10.48550/arXiv.2410.14289 (2024), https://doi.org/10.23919/IST-Africa67297.2025.11060514 (2025)	https://doi.org/10.21248/jlcl.36.2023.243 (2022)	The Kencorpus dataset is used to support natural language processing tasks and linguistic analysis for Swahili, Dholuo, and Luhya languages. It includes both text and speech data, and a two-million-word parallel corpus of English-Swahili, addressing data scarcity issues in these languages. This dataset enables researchers to develop and test algorithms for language processing and conduct detailed linguistic studies.	https://kencorpus.maseno.ac.ke/corpus-datasets/		1	
9663	Swahili Corpus	citing_context	Low -Resource Language Data: Parallel Corpora for Kiswahili and Kidaw'ida, Kalenjin, and Dholuo	https://doi.org/10.23919/IST-Africa67297.2025.11060514 (2025)		The dataset 'Low-Resource Language Data: Parallel Corpora for Kiswahili and Kidaw'ida, Kalenjin, and Dholuo' is used to study parallel corpora in low-resource African languages. Researchers focus on translation and linguistic analysis, employing the dataset to explore the structural and semantic nuances of these languages. This enables the development of more accurate translation models and deeper understanding of linguistic features specific to Kiswahili, Kidaw'ida, Kalenjin, and Dholuo.	https://zenodo.org/records/13355021		1	
9659	Swahili Corpus	cited_context | citing_context	KenSwQuAD	https://doi.org/10.48550/arXiv.2405.11437 (2024), https://doi.org/10.1145/3578553 (2022), https://doi.org/10.21248/jlcl.36.2023.243 (2022)	https://doi.org/10.1145/3578553 (2022)	The KenSwQuAD dataset is primarily used to develop and evaluate question answering systems for the Swahili language, particularly in low-resource settings. It serves as a critical resource for improving NLP models for underrepresented languages. The dataset is derived from annotated primary data collected from the field and the Kencorpus project, supporting linguistic research and the creation of Swahili language processing tools. It enables researchers to focus on corpus-based studies, language processing tasks, and the enhancement of question-answering capabilities in Swahili.; The KenSwQuAD dataset is primarily used for developing and evaluating question-answering systems for Swahili, a low-resource language. It serves as a linguistic resource for corpus-based studies and language processing tasks, focusing on human annotation and data collection from the Kencorpus project. This dataset supports research into improving performance in under-resourced linguistic contexts, enhancing Swahili language processing capabilities.	https://huggingface.co/datasets/lighteval/KenSwQuAD		1	
9685	Swahili Corpus	cited_context	Toxicity-200	https://doi.org/10.18653/v1/2023.emnlp-main.11 (2023)	https://doi.org/10.48550/arXiv.2207.04672 (2022)	The Toxicity-200 dataset is used to filter out offensive content in text data, focusing on identifying markers of toxicity. It ensures the quality and appropriateness of text by employing methods to detect and remove inappropriate content, enhancing the reliability of text datasets in various applications.	https://huggingface.co/datasets/SEACrowd/toxicity_200		1	
9642	Swahili Corpus	cited_context	CLIRMatrix	https://doi.org/10.18653/v1/2022.emnlp-main.597 (2022)	https://doi.org/10.18653/v1/2020.emnlp-main.340 (2020)	The CLIRMatrix dataset is primarily used for cross-lingual information retrieval (CLIR) research, focusing on enhancing and evaluating CLIR systems for multiple African languages, including Swahili. It supports the development and testing of baseline models (e.g., BM25, mDPR, Hybrid) and language-specific analyzers, facilitating the analysis of language distribution and coverage. The dataset includes queries and documents mined from Wikipedia, enabling researchers to address the resource scarcity in less-resourced languages and improve multilingual research capabilities.	https://huggingface.co/datasets/SEACrowd/clir_matrix		1	
9679	Swahili Corpus	cited_context | citing_context	Swahili dataset from AfriSenti-SemEval 2023 Shared Task 12	https://doi.org/10.1109/AIBThings58340.2023.10292494 (2023), https://doi.org/10.1145/3711542.3711596 (2024), https://doi.org/10.1016/j.dib.2020.106517 (2020), https://doi.org/10.18653/v1/2022.naacl-main.23 (2022), https://doi.org/10.1145/3445975 (2021)	https://doi.org/10.18653/v1/2021.mrl-1.11 (2021), https://www.semanticscholar.org/paper/ea6302be0401a59170824131168adeae7e80adcc (2012)	The Swahili dataset from AfriSenti-SemEval 2023 Shared Task 12 is used for various linguistic and computational tasks, including training the XLM-RoBERTa model for sentiment analysis, addressing Swahili verb morphology, and creating corpora of Swahili conversations to study linguistic structures, common typos, and colloquial language use. It is also utilized for multi-label emotion classification, focusing on emotional expression in Swahili. These applications leverage the dataset's rich content and diverse linguistic features to enhance understanding and modeling of the Swahili language.; The Swahili datasets prepared by Shikali et al. are utilized in various linguistic and computational studies. They are used to compare the performance of Conv-LSTM models for part-of-speech tagging, study word representation vectors using a syllabic alphabet, and analyze linguistic structures in Swahili conversations. The dataset also supports research on common Swahili typos and slang usage in Tanzanian SMS communications. Additionally, it is employed for multi-label emotion classification, focusing on emotional expression in Swahili contexts. The dataset's 28K unique words and diverse content enable these specific research applications.	https://afrisenti-semeval.github.io/		1	
9681	Swahili Corpus	citing_context	Swahili Speech Dataset	https://doi.org/10.48550/arXiv.2405.11437 (2024)	https://doi.org/10.1145/3597494 (2023)	The Swahili Speech Dataset is used to develop and improve pre-training methods for spoken digit recognition in Swahili, addressing the scarcity of speech datasets for low-resource languages. This dataset enables researchers to enhance models' performance in recognizing spoken digits, contributing to advancements in speech processing for underrepresented languages.	https://dl.acm.org/doi/full/10.1145/3597494?casa_token=szkKp-t07ZEAAAAA%3AbWpwrqhGu96dwaVezjlnzwPtxYUXUhpAhhQdvNfdFexBcEUF85GRjzLiedYcuhzX5QJ4G5I4Sg8ppw		1	
9633	Swahili Corpus	cited_context	AfriCLIRMatrix	https://doi.org/10.18653/v1/2022.emnlp-main.597 (2022)	https://doi.org/10.1561/1500000019 (2009)	The AfriCLIRMatrix dataset is used as a foundational resource for cross-lingual information retrieval (CLIR) research, providing bilingual and multilingual data. It serves as a starting point for developing and testing CLIR systems, enabling researchers to explore methodologies for improving information retrieval across languages, potentially including Swahili. The dataset's multilingual nature supports the development of more inclusive and effective CLIR solutions.	https://huggingface.co/datasets/castorini/africlirmatrix		1	
9636	Swahili Corpus	cited_context	AFROMT	https://doi.org/10.18653/v1/2021.emnlp-main.99 (2021)	https://doi.org/10.18653/v1/2021.emnlp-main.99 (2021)	The AFROMT dataset is used to construct monolingual corpora for African languages, including Swahili, primarily for training machine translation models. It provides large-scale, web-crawled data, enabling researchers to derive additional monolingual datasets. This supports the development and improvement of machine translation systems for under-resourced languages.	https://github.com/machelreid/afromt		1	
9675	Swahili Corpus	citing_context	PolitiKweli	https://doi.org/10.5815/ijitcs.2025.01.05 (2025)		The PolitiKweli dataset is used to analyze political discourse in Swahili, particularly for detecting misinformation and propaganda in social media. It supports the development and evaluation of models to classify false information in political contexts. Additionally, it is utilized to study code-switched Swahili-English posts from Twitter during elections, focusing on linguistic patterns and social media usage. This dataset enables researchers to address the spread of misinformation and understand linguistic behaviors in political communication.	https://github.com/jayneamol/kweli		1	
9652	Swahili Corpus	cited_context | citing_context	Free Spoken Digit Dataset (FSDD(11))	https://doi.org/10.1145/3597494 (2023)	https://doi.org/10.1007/978-981-15-4828-4_18 (2019)	The Free Spoken Digit Dataset (FSDD(11)) is used to train and evaluate spoken digit recognition models, with a focus on digit classification accuracy. It is employed in both multilingual contexts and specifically for the Swahili language. The dataset's spoken digits enable researchers to assess model performance in recognizing spoken numbers, facilitating advancements in speech recognition technology.	https://github.com/Jakobovski/free-spoken-digit-dataset		1	
9645	Swahili Corpus	citing_context	Crossmodal-3600	https://doi.org/10.48550/arXiv.2406.15359 (2024)	https://doi.org/10.48550/arXiv.2205.12522 (2022)	The Crossmodal-3600 dataset is used to evaluate massively multilingual multimodal models, with a focus on Swahili and other languages. It supports cross-modal tasks and is designed for comprehensive model evaluation. Researchers use this dataset to assess the performance of models in handling diverse linguistic and modal data, enabling the development of more robust and versatile AI systems.	https://google.github.io/crossmodal-3600/		1	
9671	Swahili Corpus	citing_context	Common Swahili Stop-words	https://doi.org/10.1145/3711542.3711596 (2024)	https://doi.org/10.1016/j.dib.2020.106517 (2020)	The dataset of stop-words is used to enhance text preprocessing for Swahili, specifically by removing common stop-words, correcting common typos, and identifying and normalizing slang terms. This improves the accuracy of text analysis, enabling more reliable and meaningful research outcomes in Swahili language processing.	https://data.mendeley.com/datasets/mmf4hnsm2n/1		1	
9691	Swahili Corpus	citing_context	WURA corpus	https://doi.org/10.48550/arXiv.2504.06536 (2025)	https://doi.org/10.18653/v1/2023.emnlp-main.11 (2023)	The WURA corpus is used to sample 10 billion tokens for training multilingual models, focusing on sixteen African languages and four high-resource languages. It supports the development of these models by providing a large, diverse linguistic dataset, enabling researchers to enhance model performance and cross-lingual capabilities in African languages.	https://github.com/castorini/AfriTeVa-keji		1	
9831	Sylheti Corpus	citing_context	ONUBAD	https://doi.org/10.48550/arXiv.2505.12273 (2025)	https://doi.org/10.1016/j.dib.2025.111276 (2025)	The ONUBAD dataset is used to translate Sylheti and other Bangla regional dialects into Standard Bangla and English. It consists of an expert-translated parallel corpus with 1,540 words, 130 clauses, and 980 sentences per dialect. This dataset enables researchers to focus on translation accuracy and linguistic nuances, facilitating the development of translation models and tools for these dialects.	https://data.mendeley.com/datasets/6ft99kf89b/2		1	
10006	Tatar Corpus	citing_context	Corpus of Written Tatar	https://doi.org/10.18653/V1/2021.CALCS-1.18 (2021)		The Corpus of Written Tatar is used to prepare and evaluate 700 shuffled sentences, both in Cyrillic and Latin scripts, for performance assessment. These sentences, totaling 8,466 words with duplicates and 5,261 without, are manually transcribed and verified by a native speaker. The dataset is also utilized to develop and train the FinTat 2 transliteration tool, specifically focusing on written Tatar language data. This enables researchers to enhance transliteration accuracy and evaluate system performance.	https://www.corpus.tatar/en		1	
10010	Tatar Corpus	citing_context	Tatar Language Corpus	https://doi.org/10.1007/978-3-031-24337-0_28 (2019)	https://doi.org/10.3115/v1/D14-1162 (2014)	The Tatar Language Corpus is used to train word embedding models, specifically focusing on the Tatar language. These models are trained with 300-dimensional vectors and a window size of 5, enabling researchers to capture semantic and syntactic relationships within the Tatar language. This methodology supports research in natural language processing and linguistic analysis, enhancing the representation of Tatar words in computational models.	https://github.com/tat-nlp/SART		1	
10007	Tatar Corpus	cited_context	Common Voice Corpus 10.0	https://doi.org/10.3390/info14020074 (2023)	https://doi.org/10.1007/978-3-030-83527-9_41 (2021)	The CVC dataset is used to fine-tune a Tatar speech recognition system, leveraging 129 hours of annotated data. It is specifically employed for training and testing, enabling researchers to achieve a Word Error Rate (WER) of 5.37%. This dataset facilitates the development and evaluation of speech recognition models tailored for the Tatar language.	https://huggingface.co/datasets/mozilla-foundation/common_voice_10_0		1	
10014	Tatar Corpus	cited_context | citing_context	TATAR-TTS	https://doi.org/10.1109/IECON55916.2024.10905876 (2024), https://doi.org/10.48550/arXiv.2305.15749 (2023)	https://doi.org/10.1109/ICAIIC60209.2024.10463261 (2024), https://doi.org/10.1109/ICASSP40776.2020.9053512 (2019)	The TatarTTS dataset is used to develop and evaluate text-to-speech (TTS) synthesis systems for the Tatar language. Researchers focus on enhancing the naturalness and intelligibility of synthesized speech, employing methodologies that assess the quality and clarity of the generated audio. This dataset enables the creation of more effective and realistic TTS systems for Tatar, addressing specific linguistic challenges and improving user experience.; The TATAR-TTS dataset is used to train text-to-speech (TTS) models specifically for the Tatar language. It is integrated into the LJ Speech training recipe within the ESPnet-TTS toolkit, enabling researchers to develop and refine TTS systems that accurately synthesize Tatar speech. This dataset facilitates the creation of natural-sounding voice outputs, enhancing the accessibility and usability of Tatar language technologies.	https://huggingface.co/datasets/issai/TatarTTS		1	
10009	Tatar Corpus	cited_context | citing_context	SART	https://doi.org/10.1109/ICAIIC60209.2024.10463261 (2024)	https://doi.org/10.1007/978-3-031-24337-0_28 (2019)	The SART dataset is used to evaluate word embeddings for the Tatar language, focusing on tasks such as text similarity, analogies, and text relatedness. It provides benchmark datasets for classical NLP tasks, enabling researchers to assess the performance of word embeddings in these specific linguistic contexts.; The SART dataset is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how this dataset is actually used in research.	https://github.com/tat-nlp/SART		1	
10012	Tatar Corpus	cited_context | citing_context	TurkicTTS	https://doi.org/10.48550/arXiv.2305.15749 (2023), https://doi.org/10.3390/info14020074 (2023)	https://doi.org/10.48550/arXiv.2305.15749 (2023), https://doi.org/10.3390/info14020074 (2023)	supports ten Turkic languages, including Azerbaijani, Bashkir, Kazakh, Kyrgyz, Sakha, Tatar, Turkish, Turkmen, Uyghur, and Uzbek	https://github.com/IS2AI/TurkicTTS		1	
10015	Tatar Corpus	citing_context	Turkic Unified Multilingual Language Understanding (TUMLU)	https://doi.org/10.48550/arXiv.2502.11020 (2025)	https://doi.org/10.48550/arXiv.2502.11020 (2025)	The dataset consists of 4-choice questions at middle- and high-school levels. It consists of 38139 questions across 8 languages and 11 subjects	https://github.com/ceferisbarov/TUMLU		1	
10204	Tigrigna Corpus	citing_context	Nagaoka Tigrinya Corpus	https://doi.org/10.1109/ICAIBD49809.2020.9137443 (2020)	https://www.semanticscholar.org/paper/4f39041e2d53ee4b0c171ed836fe7136395f2ad1 (2016)	The Nagaoka Tigrinya Corpus is used to develop a part-of-speech tagged corpus for Tigrinya, specifically focusing on categorizing news articles. This enhances linguistic analysis and improves tagging accuracy, leveraging the dataset's structured content to refine natural language processing techniques for the Tigrinya language.	http://eng.jnlp.org/yemane/ntigcorpus		1	
10559	Turkmen Corpus	citing_context	KazNERD	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://www.semanticscholar.org/paper/cf4c9d626facb20c271ca5cbd04af5c0eb06f813 (2021)	The Turkmen-ERD dataset is used for Named Entity Recognition (NER) in the Turkmen language, supporting the development and improvement of NER models. This dataset enables researchers to train and evaluate NER systems, enhancing their performance in identifying and classifying named entities within Turkmen text. The dataset's relevance lies in its application to Turkmen language processing, contributing to the broader field of natural language processing (NLP) for under-resourced languages.	https://github.com/IS2AI/KazNERD		1	
10561	Turkmen Corpus	citing_context	Turkmen Wikipedia	https://doi.org/10.48550/arXiv.2407.05006 (2024)	https://doi.org/10.48550/arXiv.2404.04487 (2024)	The Turkmen Wikipedia dataset is used to develop and evaluate Turkmen language processing tools, focusing on the quality and coverage of Turkmen content within a large-scale, multilingual environment. This dataset enables researchers to assess and improve the performance of these tools in handling Turkmen language data, ensuring they are effective in real-world applications.	https://github.com/tmLang-NLP/datasets		1	
10910	Uyghur Corpus	citing_context	Uyghur morphological collaborative word corpus	https://doi.org/10.1007/978-3-030-84186-7_19 (2021)		The Uyghur morphological collaborative word corpus is primarily used for morphological segmentation and analysis, focusing on character-level and word-level annotations. It contains 20,000 to 200,000 Uyghur words and 3500 to 10,000 sentence sets, enhancing linguistic research and Uyghur language processing. The dataset supports research in morphological analysis, stemming, and linguistic structure, providing high-quality annotated data for these tasks.	http://thuuymorph.thunlp.org/		1	
10909	Uyghur Corpus	cited_context	COPA	https://doi.org/10.18653/v1/2024.eacl-long.100 (2024)		The Uyghur language dataset is used to train and evaluate models for Uyghur language processing. This involves developing and testing algorithms that can effectively handle tasks such as text classification, sentiment analysis, and machine translation specific to the Uyghur language. The dataset's relevance lies in its ability to provide a robust resource for enhancing the performance and accuracy of these models in Uyghur language applications.	https://github.com/drwiner/COPA		1	
10908	Uyghur Corpus	citing_context	Thuyg-20	https://doi.org/10.1109/ICARCE55724.2022.10046434 (2022)		The dataset 'ThUB-20' is mentioned in the citation context but lacks detailed descriptions of its usage in research. There is no explicit information regarding its application, methodology, research questions, or specific characteristics. Therefore, it cannot be accurately described as being used for any particular research area or purpose based on the provided evidence.	https://www.openslr.org/22/		1	
10904	Uyghur Corpus	citing_context	real scene Uyghur dataset	https://doi.org/10.3390/s23208610 (2023)	https://doi.org/10.1109/TPAMI.2018.2848939 (2019)	The real scene Uyghur dataset is used to test and analyze deep learning-based text recognition models, particularly focusing on the performance of ABINet in recognizing Uyghur text in real-world scenes. This dataset enables researchers to evaluate model accuracy and robustness in complex, real-life environments, providing insights into the effectiveness of deep learning techniques for Uyghur text recognition.	https://github.com/kongfanjie/SUST-and-RUST-datasets-for-Uyghur-STR		1	
10907	Uyghur Corpus	citing_context	synthetic scene Uyghur image dataset	https://doi.org/10.3390/s23208610 (2023)	https://doi.org/10.1109/CVPR.2016.254 (2016)	The 'synthetic scene Uyghur image dataset' is mentioned in research citations but lacks detailed descriptions of its usage. There is no explicit information on specific methodologies, research questions, or applications. The dataset's role and relevance in actual research remain unclear based on the provided evidence.	https://github.com/kongfanjie/SUST-and-RUST-datasets-for-Uyghur-STR		2	
11182	Western Punjabi Corpus	citing_context	Western Punjabi language dataset	https://doi.org/10.1109/ACCESS.2025.3527710 (2025)	https://doi.org/10.1287/mnsc.1070.0810 (2004)	The Western Punjabi language dataset is used for sentiment analysis and consumer behavior research, specifically by collecting and categorizing user reviews across various domains including politics, mobile phones, movies, and theatre. This dataset enables researchers to analyze sentiments and understand consumer opinions through a structured approach, providing insights into how users perceive and interact with different topics and products.	https://github.com/toqeerehsan/Shahmukhi-POS-Tagging		1	
11181	Western Punjabi Corpus	citing_context	SMHaroof	https://doi.org/10.3724/2096-7004.di.2025.0035 (2025)	https://doi.org/10.18653/v1/W19-3101 (2019)	The Western Punjabi corpus is used to develop a lexicon of 13,600 words in the Mahji dialect, derived from a 0.9 million word corpus. This dataset is automatically extracted from Wikipedia and manually pre-processed, enabling researchers to create a comprehensive lexical resource for the Western Punjabi language. The corpus supports linguistic research by providing a large, structured dataset for analyzing and documenting the Mahji dialect.	https://ieee-dataport.org/documents/shahmukhi-database-smdb-smharoof-v1		1	
11190	Xhosa Corpus	citing_context	Xhosa RDF data set	https://www.semanticscholar.org/paper/88c4e18a4a10303809457e6131ca408fc08b25aa (2018)	https://doi.org/10.1145/2566486.2568002 (2014)	The Xhosa RDF data set is used to conduct syntactic and semantic data quality tests, ensuring the integrity and correctness of the RDF data. This involves validating the data's structure and meaning, which is crucial for maintaining reliable and accurate information in RDF formats. The dataset enables researchers to assess and improve the quality of RDF data, supporting robust data management and analysis.	https://github.com/MMoOn-Project/OpenBantu/blob/master/xho/inventory/ob_xho.ttl		1	
11189	Xhosa Corpus	citing_context	WordNet RDF data set	https://www.semanticscholar.org/paper/88c4e18a4a10303809457e6131ca408fc08b25aa (2018)	https://www.semanticscholar.org/paper/4f7ba001255a08f2fd9387fb1a747ad09ffe1c0e (2014)	The WordNet RDF data set is used to facilitate the creation of linked data by providing English translations for Xhosa lexical entries. This linkage enhances the semantic richness of the data, enabling researchers to enrich Xhosa language resources through external connections with English terms. The dataset's RDF format supports structured linking, making it a valuable tool for linguistic and semantic research.	https://github.com/jrvosse/wordnet-3.0-rdf		1	
11217	Yoruba Corpus	citing_context	Wiki	https://www.semanticscholar.org/paper/4ca059f9aa3c16dffb2e7239205082aa47072dcf (2020)	https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c (2018)	The Wiki dataset is primarily used to train and generate word embeddings for the Yoruba language, focusing on the quality and representation of Yoruba language vectors. Researchers use both diacritized and undiacritized (normalized) versions of the Wikipedia dump to explore the effects of diacritics and normalization on word embeddings. This methodology enhances comparability across languages and provides a broader linguistic context, enabling detailed analysis of Yoruba language modeling and vector representation.	https://huggingface.co/datasets/ajesujoba/yoruba_text_c3		1	
11214	Yoruba Corpus	citing_context	OpenSLR 86	https://doi.org/10.1109/NIGERCON62786.2024.10927351 (2024)	https://doi.org/10.21437/interspeech.2020-1096 (2020)	The OpenSLR 86 dataset is used to evaluate word embedding models specifically for the Yoruba language. Researchers employ the next word prediction task to analyze phonetic and morphological features, assessing the effectiveness of these models in capturing linguistic nuances. This dataset enables detailed examination of language-specific challenges and enhances understanding of Yoruba's unique characteristics in computational linguistics.	https://www.openslr.org/86/		1	
11210	Yoruba Corpus	cited_context	MENYO-20k	https://doi.org/10.48550/arXiv.2303.17972 (2023)	https://www.semanticscholar.org/paper/24ac368d08765dfad920ceefb79fba7bfe81d83c (2021)	The MENYO-20k dataset is primarily used to train and evaluate Yoruba-English neural machine translation models. Research focuses on the impact of diacritics and domain-specific content on translation quality, particularly in movie transcripts. The dataset serves as a benchmark for low-resource language pairs, enabling researchers to assess and improve the performance of Yoruba-English translation models across various domains.	https://github.com/uds-lsv/menyo-20k_MT		1	
11204	Yoruba Corpus	citing_context	FLEURS test set	https://doi.org/10.48550/arXiv.2505.20564 (2025)	https://doi.org/10.1109/SLT54892.2023.10023141 (2022)	The FLEURS test set is used to evaluate the performance of Automatic Speech Recognition (ASR) models, particularly focusing on their effectiveness with the Yoruba language. This dataset enables researchers to assess model accuracy and robustness in recognizing spoken Yoruba, contributing to the development and improvement of ASR systems for under-resourced languages.	https://huggingface.co/datasets/google/fleurs		1	
11195	Yoruba Corpus	citing_context	Afriberta-Corpus	https://doi.org/10.48550/arXiv.2412.03334 (2024)	https://doi.org/10.18653/v1/2021.mrl-1.11 (2021)	The Afriberta-Corpus is used to explore the viability of pretrained multilingual language models for low-resource languages, particularly Yoruba. It provides a large-scale multilingual and monolingual corpus, enabling researchers to train and evaluate models on domain-specific aspects of Yoruba, such as news and web data. This dataset supports broader multilingual research and development, covering 167 languages, and facilitates the analysis of domain-relevant aspects of the Yoruba language.	https://huggingface.co/datasets/castorini/afriberta-corpus		1	
11220	Yoruba Corpus	citing_context	Yorùbá Text C3	https://doi.org/10.48550/arXiv.2412.03334 (2024)	https://www.semanticscholar.org/paper/4e96e6d2abc54bc55d5489a9e5a666f273b69410 (2019)	The Yorùbá Text C3 dataset is used to enrich parallel text resources, enhancing the diversity and coverage of training data for machine translation. It serves as a foundational resource to improve the quality and accuracy of translations involving the Yorùbá language. The dataset's parallel text structure is crucial for training models to better handle the linguistic nuances of Yorùbá.	https://huggingface.co/datasets/ajesujoba/yoruba_text_c3		1	
11192	Yoruba Corpus	cited_context	SLR86	https://doi.org/10.48550/arXiv.2207.03546 (2022)	https://doi.org/10.21437/interspeech.2020-1096 (2020)	The 4 hour multi-speaker dataset is used to train and evaluate speech recognition models for Yoruba, with a focus on tone realization and pronunciation accuracy. It is also employed to develop and test open-source Yoruba speech recognition systems, emphasizing multi-speaker variability and robustness. This dataset enables researchers to improve the performance and reliability of Yoruba speech recognition technologies.	http://openslr.org/86/		1	
11193	Yoruba Corpus	cited_context | citing_context	75 hour corpus	https://doi.org/10.48550/arXiv.2207.03546 (2022)	https://www.semanticscholar.org/paper/d90b9e0c1456caea44ab7b749e7d4ea12eb9dfce (2012)	The 75 hour corpus is used to enhance the diversity of speakers in Yoruba speech recognition, supporting the development and evaluation of multi-speaker models. It focuses on improving tone realization and acoustic modeling, enabling robustness testing and the creation of more accurate Yoruba speech recognition systems.; The 75 hour corpus is used to train and evaluate speech recognition models for Yoruba, with a focus on tone realization and pronunciation accuracy. It supports the development and testing of open-source Yoruba speech recognition systems, emphasizing multi-speaker variability and robustness. This dataset enables researchers to improve the performance and reliability of Yoruba speech recognition technologies.	https://github.com/Niger-Volta-LTI/yoruba-voice		1	
11199	Yoruba Corpus	citing_context	AfroDigits dataset	https://doi.org/10.48550/arXiv.2303.12582 (2023)	https://doi.org/10.1109/5.726791 (1998)	The AfroDigits dataset is used for speech processing, particularly for training and evaluating models focused on the Yoruba language. It serves as a 'Hello World' dataset, providing a foundational resource for initial model development. Additionally, the dataset supports the creation of a spoken digit dataset for African languages, including Yoruba, through a community-driven, participatory approach. This enables researchers to develop and test speech recognition systems tailored to Yoruba and other African languages.	https://huggingface.co/datasets/chrisjay/crowd-speech-africa		2	
11222	Yoruba Corpus	cited_context	Yoruba language dataset	https://doi.org/10.48550/arXiv.2302.08956 (2023)	https://www.semanticscholar.org/paper/f5808755486ad4fe58d832e752e624382c5a2d5a (2020)	The Yoruba language dataset is utilized in sentiment analysis research for African languages. While the specific methodologies, research questions, and detailed applications are not elaborated upon, the dataset's relevance lies in its contribution to the broader field of natural language processing, particularly in enhancing the understanding and analysis of sentiments expressed in African languages.	https://huggingface.co/datasets/acflp/YANKARI		2	
11219	Yoruba Corpus	citing_context	Yorùbá speech recognition corpus	https://doi.org/10.1145/3690384 (2024)	https://www.semanticscholar.org/paper/d90b9e0c1456caea44ab7b749e7d4ea12eb9dfce (2012)	The Yorùbá speech recognition corpus is used to train and evaluate tone recognition systems, specifically addressing language-specific ambiguities in Yoruba. Researchers apply a syllabification algorithm to transcriptions and extract tones from each syllable, focusing on phonetic features to improve the accuracy of tone recognition in the Yorùbá language.	https://github.com/Niger-Volta-LTI/yoruba-voice		2	
11223	Yoruba Corpus	cited_context | citing_context	Yoruba Text-to-Speech System corpus	https://doi.org/10.21437/interspeech.2020-1096 (2020)	https://www.semanticscholar.org/paper/2b7326af648ec38685effd2a6dc2679d4e5fe97b (2017)	The Yoruba Text-to-Speech System corpus is used for fundamental frequency analysis in Yoruba, specifically focusing on tone and phonetic aspects. Researchers employ the corpus for training and evaluating models to understand and replicate the tonal patterns and phonetic characteristics of the Yoruba language. This dataset enables detailed acoustic analysis and enhances the development of accurate text-to-speech systems for Yoruba.; The Yoruba Text-to-Speech System corpus is used for fundamental frequency analyses to study Yoruba tone realization in TTS systems and gender recognition. Research focuses on the phonetic and acoustic properties of the Yoruba language, leveraging the dataset's detailed audio recordings and linguistic annotations to enhance the accuracy and naturalness of synthesized speech.	https://github.com/Niger-Volta-LTI/yoruba-audio		2	
11265	Zarma Corpus	citing_context	Feriji Dataset	https://doi.org/10.48550/arXiv.2410.15539 (2024)	https://doi.org/10.48550/arXiv.2406.05888 (2024)	The Feriji Dataset is used to address the scarcity of annotated data in the Zarma language by generating synthetic datasets through custom corruption scripts. It evaluates system performance on linguistic errors by manually corrupting sentences from a French-Zarma parallel corpus and assesses spelling correction accuracy by introducing typographical errors into 3,539 sentences. This enables research on error detection and correction in Zarma, enhancing the development of language processing systems for under-resourced languages.	https://huggingface.co/datasets/27Group/Feriji		1	